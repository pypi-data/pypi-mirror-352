// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: xla/backends/gpu/runtime/thunk.proto

#ifndef GOOGLE_PROTOBUF_INCLUDED_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto
#define GOOGLE_PROTOBUF_INCLUDED_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto

#include <limits>
#include <string>

#include <google/protobuf/port_def.inc>
#if PROTOBUF_VERSION < 3021000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers. Please update
#error your headers.
#endif
#if 3021009 < PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers. Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/port_undef.inc>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata_lite.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/unknown_field_set.h>
#include "xla/service/buffer_assignment.pb.h"
#include "xla/stream_executor/gpu/gpu_blas_lt.pb.h"
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>
#define PROTOBUF_INTERNAL_EXPORT_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto PROTOBUF_EXPORT
PROTOBUF_NAMESPACE_OPEN
namespace internal {
class AnyMetadata;
}  // namespace internal
PROTOBUF_NAMESPACE_CLOSE

// Internal implementation detail -- do not use these members.
struct PROTOBUF_EXPORT TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto {
  static const uint32_t offsets[];
};
PROTOBUF_EXPORT extern const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
namespace xla {
namespace gpu {
class ConditionalThunkProto;
struct ConditionalThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern ConditionalThunkProtoDefaultTypeInternal _ConditionalThunkProto_default_instance_;
class CopyThunkProto;
struct CopyThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern CopyThunkProtoDefaultTypeInternal _CopyThunkProto_default_instance_;
class DeviceToHostCopyThunkProto;
struct DeviceToHostCopyThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern DeviceToHostCopyThunkProtoDefaultTypeInternal _DeviceToHostCopyThunkProto_default_instance_;
class Dim3DProto;
struct Dim3DProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern Dim3DProtoDefaultTypeInternal _Dim3DProto_default_instance_;
class GemmThunkProto;
struct GemmThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern GemmThunkProtoDefaultTypeInternal _GemmThunkProto_default_instance_;
class HostToDeviceCopyThunkProto;
struct HostToDeviceCopyThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern HostToDeviceCopyThunkProtoDefaultTypeInternal _HostToDeviceCopyThunkProto_default_instance_;
class KernelThunkProto;
struct KernelThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern KernelThunkProtoDefaultTypeInternal _KernelThunkProto_default_instance_;
class SequentialThunkProto;
struct SequentialThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern SequentialThunkProtoDefaultTypeInternal _SequentialThunkProto_default_instance_;
class ThunkInfoProto;
struct ThunkInfoProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern ThunkInfoProtoDefaultTypeInternal _ThunkInfoProto_default_instance_;
class ThunkProto;
struct ThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern ThunkProtoDefaultTypeInternal _ThunkProto_default_instance_;
class WhileThunkProto;
struct WhileThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern WhileThunkProtoDefaultTypeInternal _WhileThunkProto_default_instance_;
}  // namespace gpu
}  // namespace xla
PROTOBUF_NAMESPACE_OPEN
template<> PROTOBUF_EXPORT ::xla::gpu::ConditionalThunkProto* Arena::CreateMaybeMessage<::xla::gpu::ConditionalThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::CopyThunkProto* Arena::CreateMaybeMessage<::xla::gpu::CopyThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::DeviceToHostCopyThunkProto* Arena::CreateMaybeMessage<::xla::gpu::DeviceToHostCopyThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::Dim3DProto* Arena::CreateMaybeMessage<::xla::gpu::Dim3DProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::GemmThunkProto* Arena::CreateMaybeMessage<::xla::gpu::GemmThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::HostToDeviceCopyThunkProto* Arena::CreateMaybeMessage<::xla::gpu::HostToDeviceCopyThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::KernelThunkProto* Arena::CreateMaybeMessage<::xla::gpu::KernelThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::SequentialThunkProto* Arena::CreateMaybeMessage<::xla::gpu::SequentialThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::ThunkInfoProto* Arena::CreateMaybeMessage<::xla::gpu::ThunkInfoProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::ThunkProto* Arena::CreateMaybeMessage<::xla::gpu::ThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::WhileThunkProto* Arena::CreateMaybeMessage<::xla::gpu::WhileThunkProto>(Arena*);
PROTOBUF_NAMESPACE_CLOSE
namespace xla {
namespace gpu {

// ===================================================================

class PROTOBUF_EXPORT ThunkInfoProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.ThunkInfoProto) */ {
 public:
  inline ThunkInfoProto() : ThunkInfoProto(nullptr) {}
  ~ThunkInfoProto() override;
  explicit PROTOBUF_CONSTEXPR ThunkInfoProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ThunkInfoProto(const ThunkInfoProto& from);
  ThunkInfoProto(ThunkInfoProto&& from) noexcept
    : ThunkInfoProto() {
    *this = ::std::move(from);
  }

  inline ThunkInfoProto& operator=(const ThunkInfoProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline ThunkInfoProto& operator=(ThunkInfoProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ThunkInfoProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const ThunkInfoProto* internal_default_instance() {
    return reinterpret_cast<const ThunkInfoProto*>(
               &_ThunkInfoProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  friend void swap(ThunkInfoProto& a, ThunkInfoProto& b) {
    a.Swap(&b);
  }
  inline void Swap(ThunkInfoProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ThunkInfoProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ThunkInfoProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ThunkInfoProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ThunkInfoProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const ThunkInfoProto& from) {
    ThunkInfoProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ThunkInfoProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.ThunkInfoProto";
  }
  protected:
  explicit ThunkInfoProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kProfileAnnotationFieldNumber = 1,
    kExecutionStreamIdFieldNumber = 2,
  };
  // string profile_annotation = 1;
  void clear_profile_annotation();
  const std::string& profile_annotation() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_profile_annotation(ArgT0&& arg0, ArgT... args);
  std::string* mutable_profile_annotation();
  PROTOBUF_NODISCARD std::string* release_profile_annotation();
  void set_allocated_profile_annotation(std::string* profile_annotation);
  private:
  const std::string& _internal_profile_annotation() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_profile_annotation(const std::string& value);
  std::string* _internal_mutable_profile_annotation();
  public:

  // int64 execution_stream_id = 2;
  void clear_execution_stream_id();
  int64_t execution_stream_id() const;
  void set_execution_stream_id(int64_t value);
  private:
  int64_t _internal_execution_stream_id() const;
  void _internal_set_execution_stream_id(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.ThunkInfoProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr profile_annotation_;
    int64_t execution_stream_id_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT CopyThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.CopyThunkProto) */ {
 public:
  inline CopyThunkProto() : CopyThunkProto(nullptr) {}
  ~CopyThunkProto() override;
  explicit PROTOBUF_CONSTEXPR CopyThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CopyThunkProto(const CopyThunkProto& from);
  CopyThunkProto(CopyThunkProto&& from) noexcept
    : CopyThunkProto() {
    *this = ::std::move(from);
  }

  inline CopyThunkProto& operator=(const CopyThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline CopyThunkProto& operator=(CopyThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const CopyThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const CopyThunkProto* internal_default_instance() {
    return reinterpret_cast<const CopyThunkProto*>(
               &_CopyThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  friend void swap(CopyThunkProto& a, CopyThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(CopyThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CopyThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CopyThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CopyThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const CopyThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const CopyThunkProto& from) {
    CopyThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(CopyThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.CopyThunkProto";
  }
  protected:
  explicit CopyThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSourceBufferFieldNumber = 1,
    kDestinationBufferFieldNumber = 2,
    kMemSizeFieldNumber = 3,
  };
  // .xla.buffer_assignment.BufferAllocationSliceProto source_buffer = 1;
  bool has_source_buffer() const;
  private:
  bool _internal_has_source_buffer() const;
  public:
  void clear_source_buffer();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& source_buffer() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_source_buffer();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_source_buffer();
  void set_allocated_source_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* source_buffer);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_source_buffer() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_source_buffer();
  public:
  void unsafe_arena_set_allocated_source_buffer(
      ::xla::buffer_assignment::BufferAllocationSliceProto* source_buffer);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_source_buffer();

  // .xla.buffer_assignment.BufferAllocationSliceProto destination_buffer = 2;
  bool has_destination_buffer() const;
  private:
  bool _internal_has_destination_buffer() const;
  public:
  void clear_destination_buffer();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& destination_buffer() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_destination_buffer();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_destination_buffer();
  void set_allocated_destination_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* destination_buffer);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_destination_buffer() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_destination_buffer();
  public:
  void unsafe_arena_set_allocated_destination_buffer(
      ::xla::buffer_assignment::BufferAllocationSliceProto* destination_buffer);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_destination_buffer();

  // int64 mem_size = 3;
  void clear_mem_size();
  int64_t mem_size() const;
  void set_mem_size(int64_t value);
  private:
  int64_t _internal_mem_size() const;
  void _internal_set_mem_size(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.CopyThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::xla::buffer_assignment::BufferAllocationSliceProto* source_buffer_;
    ::xla::buffer_assignment::BufferAllocationSliceProto* destination_buffer_;
    int64_t mem_size_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT DeviceToHostCopyThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.DeviceToHostCopyThunkProto) */ {
 public:
  inline DeviceToHostCopyThunkProto() : DeviceToHostCopyThunkProto(nullptr) {}
  ~DeviceToHostCopyThunkProto() override;
  explicit PROTOBUF_CONSTEXPR DeviceToHostCopyThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  DeviceToHostCopyThunkProto(const DeviceToHostCopyThunkProto& from);
  DeviceToHostCopyThunkProto(DeviceToHostCopyThunkProto&& from) noexcept
    : DeviceToHostCopyThunkProto() {
    *this = ::std::move(from);
  }

  inline DeviceToHostCopyThunkProto& operator=(const DeviceToHostCopyThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline DeviceToHostCopyThunkProto& operator=(DeviceToHostCopyThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const DeviceToHostCopyThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const DeviceToHostCopyThunkProto* internal_default_instance() {
    return reinterpret_cast<const DeviceToHostCopyThunkProto*>(
               &_DeviceToHostCopyThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  friend void swap(DeviceToHostCopyThunkProto& a, DeviceToHostCopyThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(DeviceToHostCopyThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(DeviceToHostCopyThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  DeviceToHostCopyThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<DeviceToHostCopyThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const DeviceToHostCopyThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const DeviceToHostCopyThunkProto& from) {
    DeviceToHostCopyThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(DeviceToHostCopyThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.DeviceToHostCopyThunkProto";
  }
  protected:
  explicit DeviceToHostCopyThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kCopyThunkFieldNumber = 1,
  };
  // .xla.gpu.CopyThunkProto copy_thunk = 1;
  bool has_copy_thunk() const;
  private:
  bool _internal_has_copy_thunk() const;
  public:
  void clear_copy_thunk();
  const ::xla::gpu::CopyThunkProto& copy_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::CopyThunkProto* release_copy_thunk();
  ::xla::gpu::CopyThunkProto* mutable_copy_thunk();
  void set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk);
  private:
  const ::xla::gpu::CopyThunkProto& _internal_copy_thunk() const;
  ::xla::gpu::CopyThunkProto* _internal_mutable_copy_thunk();
  public:
  void unsafe_arena_set_allocated_copy_thunk(
      ::xla::gpu::CopyThunkProto* copy_thunk);
  ::xla::gpu::CopyThunkProto* unsafe_arena_release_copy_thunk();

  // @@protoc_insertion_point(class_scope:xla.gpu.DeviceToHostCopyThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::xla::gpu::CopyThunkProto* copy_thunk_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT HostToDeviceCopyThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.HostToDeviceCopyThunkProto) */ {
 public:
  inline HostToDeviceCopyThunkProto() : HostToDeviceCopyThunkProto(nullptr) {}
  ~HostToDeviceCopyThunkProto() override;
  explicit PROTOBUF_CONSTEXPR HostToDeviceCopyThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  HostToDeviceCopyThunkProto(const HostToDeviceCopyThunkProto& from);
  HostToDeviceCopyThunkProto(HostToDeviceCopyThunkProto&& from) noexcept
    : HostToDeviceCopyThunkProto() {
    *this = ::std::move(from);
  }

  inline HostToDeviceCopyThunkProto& operator=(const HostToDeviceCopyThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline HostToDeviceCopyThunkProto& operator=(HostToDeviceCopyThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const HostToDeviceCopyThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const HostToDeviceCopyThunkProto* internal_default_instance() {
    return reinterpret_cast<const HostToDeviceCopyThunkProto*>(
               &_HostToDeviceCopyThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    3;

  friend void swap(HostToDeviceCopyThunkProto& a, HostToDeviceCopyThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(HostToDeviceCopyThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(HostToDeviceCopyThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  HostToDeviceCopyThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<HostToDeviceCopyThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const HostToDeviceCopyThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const HostToDeviceCopyThunkProto& from) {
    HostToDeviceCopyThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(HostToDeviceCopyThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.HostToDeviceCopyThunkProto";
  }
  protected:
  explicit HostToDeviceCopyThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kCopyThunkFieldNumber = 1,
  };
  // .xla.gpu.CopyThunkProto copy_thunk = 1;
  bool has_copy_thunk() const;
  private:
  bool _internal_has_copy_thunk() const;
  public:
  void clear_copy_thunk();
  const ::xla::gpu::CopyThunkProto& copy_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::CopyThunkProto* release_copy_thunk();
  ::xla::gpu::CopyThunkProto* mutable_copy_thunk();
  void set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk);
  private:
  const ::xla::gpu::CopyThunkProto& _internal_copy_thunk() const;
  ::xla::gpu::CopyThunkProto* _internal_mutable_copy_thunk();
  public:
  void unsafe_arena_set_allocated_copy_thunk(
      ::xla::gpu::CopyThunkProto* copy_thunk);
  ::xla::gpu::CopyThunkProto* unsafe_arena_release_copy_thunk();

  // @@protoc_insertion_point(class_scope:xla.gpu.HostToDeviceCopyThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::xla::gpu::CopyThunkProto* copy_thunk_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT ConditionalThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.ConditionalThunkProto) */ {
 public:
  inline ConditionalThunkProto() : ConditionalThunkProto(nullptr) {}
  ~ConditionalThunkProto() override;
  explicit PROTOBUF_CONSTEXPR ConditionalThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ConditionalThunkProto(const ConditionalThunkProto& from);
  ConditionalThunkProto(ConditionalThunkProto&& from) noexcept
    : ConditionalThunkProto() {
    *this = ::std::move(from);
  }

  inline ConditionalThunkProto& operator=(const ConditionalThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline ConditionalThunkProto& operator=(ConditionalThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ConditionalThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const ConditionalThunkProto* internal_default_instance() {
    return reinterpret_cast<const ConditionalThunkProto*>(
               &_ConditionalThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    4;

  friend void swap(ConditionalThunkProto& a, ConditionalThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(ConditionalThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ConditionalThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ConditionalThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ConditionalThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ConditionalThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const ConditionalThunkProto& from) {
    ConditionalThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ConditionalThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.ConditionalThunkProto";
  }
  protected:
  explicit ConditionalThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kBranchThunksFieldNumber = 2,
    kBranchIndexBufferFieldNumber = 1,
    kBranchIndexIsBoolFieldNumber = 3,
  };
  // repeated .xla.gpu.SequentialThunkProto branch_thunks = 2;
  int branch_thunks_size() const;
  private:
  int _internal_branch_thunks_size() const;
  public:
  void clear_branch_thunks();
  ::xla::gpu::SequentialThunkProto* mutable_branch_thunks(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::SequentialThunkProto >*
      mutable_branch_thunks();
  private:
  const ::xla::gpu::SequentialThunkProto& _internal_branch_thunks(int index) const;
  ::xla::gpu::SequentialThunkProto* _internal_add_branch_thunks();
  public:
  const ::xla::gpu::SequentialThunkProto& branch_thunks(int index) const;
  ::xla::gpu::SequentialThunkProto* add_branch_thunks();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::SequentialThunkProto >&
      branch_thunks() const;

  // .xla.buffer_assignment.BufferAllocationSliceProto branch_index_buffer = 1;
  bool has_branch_index_buffer() const;
  private:
  bool _internal_has_branch_index_buffer() const;
  public:
  void clear_branch_index_buffer();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& branch_index_buffer() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_branch_index_buffer();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_branch_index_buffer();
  void set_allocated_branch_index_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* branch_index_buffer);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_branch_index_buffer() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_branch_index_buffer();
  public:
  void unsafe_arena_set_allocated_branch_index_buffer(
      ::xla::buffer_assignment::BufferAllocationSliceProto* branch_index_buffer);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_branch_index_buffer();

  // bool branch_index_is_bool = 3;
  void clear_branch_index_is_bool();
  bool branch_index_is_bool() const;
  void set_branch_index_is_bool(bool value);
  private:
  bool _internal_branch_index_is_bool() const;
  void _internal_set_branch_index_is_bool(bool value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.ConditionalThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::SequentialThunkProto > branch_thunks_;
    ::xla::buffer_assignment::BufferAllocationSliceProto* branch_index_buffer_;
    bool branch_index_is_bool_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT WhileThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.WhileThunkProto) */ {
 public:
  inline WhileThunkProto() : WhileThunkProto(nullptr) {}
  ~WhileThunkProto() override;
  explicit PROTOBUF_CONSTEXPR WhileThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  WhileThunkProto(const WhileThunkProto& from);
  WhileThunkProto(WhileThunkProto&& from) noexcept
    : WhileThunkProto() {
    *this = ::std::move(from);
  }

  inline WhileThunkProto& operator=(const WhileThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline WhileThunkProto& operator=(WhileThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const WhileThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const WhileThunkProto* internal_default_instance() {
    return reinterpret_cast<const WhileThunkProto*>(
               &_WhileThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    5;

  friend void swap(WhileThunkProto& a, WhileThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(WhileThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(WhileThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  WhileThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<WhileThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const WhileThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const WhileThunkProto& from) {
    WhileThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(WhileThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.WhileThunkProto";
  }
  protected:
  explicit WhileThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kConditionResultBufferIndexFieldNumber = 1,
    kConditionThunkSequenceFieldNumber = 2,
    kBodyThunkSequenceFieldNumber = 3,
    kTripCountFieldNumber = 4,
  };
  // .xla.buffer_assignment.BufferAllocationSliceProto condition_result_buffer_index = 1;
  bool has_condition_result_buffer_index() const;
  private:
  bool _internal_has_condition_result_buffer_index() const;
  public:
  void clear_condition_result_buffer_index();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& condition_result_buffer_index() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_condition_result_buffer_index();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_condition_result_buffer_index();
  void set_allocated_condition_result_buffer_index(::xla::buffer_assignment::BufferAllocationSliceProto* condition_result_buffer_index);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_condition_result_buffer_index() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_condition_result_buffer_index();
  public:
  void unsafe_arena_set_allocated_condition_result_buffer_index(
      ::xla::buffer_assignment::BufferAllocationSliceProto* condition_result_buffer_index);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_condition_result_buffer_index();

  // .xla.gpu.SequentialThunkProto condition_thunk_sequence = 2;
  bool has_condition_thunk_sequence() const;
  private:
  bool _internal_has_condition_thunk_sequence() const;
  public:
  void clear_condition_thunk_sequence();
  const ::xla::gpu::SequentialThunkProto& condition_thunk_sequence() const;
  PROTOBUF_NODISCARD ::xla::gpu::SequentialThunkProto* release_condition_thunk_sequence();
  ::xla::gpu::SequentialThunkProto* mutable_condition_thunk_sequence();
  void set_allocated_condition_thunk_sequence(::xla::gpu::SequentialThunkProto* condition_thunk_sequence);
  private:
  const ::xla::gpu::SequentialThunkProto& _internal_condition_thunk_sequence() const;
  ::xla::gpu::SequentialThunkProto* _internal_mutable_condition_thunk_sequence();
  public:
  void unsafe_arena_set_allocated_condition_thunk_sequence(
      ::xla::gpu::SequentialThunkProto* condition_thunk_sequence);
  ::xla::gpu::SequentialThunkProto* unsafe_arena_release_condition_thunk_sequence();

  // .xla.gpu.SequentialThunkProto body_thunk_sequence = 3;
  bool has_body_thunk_sequence() const;
  private:
  bool _internal_has_body_thunk_sequence() const;
  public:
  void clear_body_thunk_sequence();
  const ::xla::gpu::SequentialThunkProto& body_thunk_sequence() const;
  PROTOBUF_NODISCARD ::xla::gpu::SequentialThunkProto* release_body_thunk_sequence();
  ::xla::gpu::SequentialThunkProto* mutable_body_thunk_sequence();
  void set_allocated_body_thunk_sequence(::xla::gpu::SequentialThunkProto* body_thunk_sequence);
  private:
  const ::xla::gpu::SequentialThunkProto& _internal_body_thunk_sequence() const;
  ::xla::gpu::SequentialThunkProto* _internal_mutable_body_thunk_sequence();
  public:
  void unsafe_arena_set_allocated_body_thunk_sequence(
      ::xla::gpu::SequentialThunkProto* body_thunk_sequence);
  ::xla::gpu::SequentialThunkProto* unsafe_arena_release_body_thunk_sequence();

  // optional int64 trip_count = 4;
  bool has_trip_count() const;
  private:
  bool _internal_has_trip_count() const;
  public:
  void clear_trip_count();
  int64_t trip_count() const;
  void set_trip_count(int64_t value);
  private:
  int64_t _internal_trip_count() const;
  void _internal_set_trip_count(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.WhileThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::internal::HasBits<1> _has_bits_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
    ::xla::buffer_assignment::BufferAllocationSliceProto* condition_result_buffer_index_;
    ::xla::gpu::SequentialThunkProto* condition_thunk_sequence_;
    ::xla::gpu::SequentialThunkProto* body_thunk_sequence_;
    int64_t trip_count_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT Dim3DProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.Dim3DProto) */ {
 public:
  inline Dim3DProto() : Dim3DProto(nullptr) {}
  ~Dim3DProto() override;
  explicit PROTOBUF_CONSTEXPR Dim3DProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  Dim3DProto(const Dim3DProto& from);
  Dim3DProto(Dim3DProto&& from) noexcept
    : Dim3DProto() {
    *this = ::std::move(from);
  }

  inline Dim3DProto& operator=(const Dim3DProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline Dim3DProto& operator=(Dim3DProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const Dim3DProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const Dim3DProto* internal_default_instance() {
    return reinterpret_cast<const Dim3DProto*>(
               &_Dim3DProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    6;

  friend void swap(Dim3DProto& a, Dim3DProto& b) {
    a.Swap(&b);
  }
  inline void Swap(Dim3DProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(Dim3DProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  Dim3DProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<Dim3DProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const Dim3DProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const Dim3DProto& from) {
    Dim3DProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(Dim3DProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.Dim3DProto";
  }
  protected:
  explicit Dim3DProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kXFieldNumber = 1,
    kYFieldNumber = 2,
    kZFieldNumber = 3,
  };
  // int64 x = 1;
  void clear_x();
  int64_t x() const;
  void set_x(int64_t value);
  private:
  int64_t _internal_x() const;
  void _internal_set_x(int64_t value);
  public:

  // int64 y = 2;
  void clear_y();
  int64_t y() const;
  void set_y(int64_t value);
  private:
  int64_t _internal_y() const;
  void _internal_set_y(int64_t value);
  public:

  // int64 z = 3;
  void clear_z();
  int64_t z() const;
  void set_z(int64_t value);
  private:
  int64_t _internal_z() const;
  void _internal_set_z(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.Dim3DProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    int64_t x_;
    int64_t y_;
    int64_t z_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT KernelThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.KernelThunkProto) */ {
 public:
  inline KernelThunkProto() : KernelThunkProto(nullptr) {}
  ~KernelThunkProto() override;
  explicit PROTOBUF_CONSTEXPR KernelThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  KernelThunkProto(const KernelThunkProto& from);
  KernelThunkProto(KernelThunkProto&& from) noexcept
    : KernelThunkProto() {
    *this = ::std::move(from);
  }

  inline KernelThunkProto& operator=(const KernelThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline KernelThunkProto& operator=(KernelThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const KernelThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const KernelThunkProto* internal_default_instance() {
    return reinterpret_cast<const KernelThunkProto*>(
               &_KernelThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    7;

  friend void swap(KernelThunkProto& a, KernelThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(KernelThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(KernelThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  KernelThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<KernelThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const KernelThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const KernelThunkProto& from) {
    KernelThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(KernelThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.KernelThunkProto";
  }
  protected:
  explicit KernelThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kArgsFieldNumber = 1,
    kWrittenFieldNumber = 2,
    kKernelNameFieldNumber = 3,
    kLaunchBlockCountsFieldNumber = 4,
    kLaunchThreadCountsPerBlockFieldNumber = 5,
    kClusterDimFieldNumber = 6,
    kShmemBytesFieldNumber = 7,
  };
  // repeated .xla.buffer_assignment.BufferAllocationSliceProto args = 1;
  int args_size() const;
  private:
  int _internal_args_size() const;
  public:
  void clear_args();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_args(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::buffer_assignment::BufferAllocationSliceProto >*
      mutable_args();
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_args(int index) const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_add_args();
  public:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& args(int index) const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* add_args();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::buffer_assignment::BufferAllocationSliceProto >&
      args() const;

  // repeated bool written = 2;
  int written_size() const;
  private:
  int _internal_written_size() const;
  public:
  void clear_written();
  private:
  bool _internal_written(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      _internal_written() const;
  void _internal_add_written(bool value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      _internal_mutable_written();
  public:
  bool written(int index) const;
  void set_written(int index, bool value);
  void add_written(bool value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
      written() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
      mutable_written();

  // string kernel_name = 3;
  void clear_kernel_name();
  const std::string& kernel_name() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_kernel_name(ArgT0&& arg0, ArgT... args);
  std::string* mutable_kernel_name();
  PROTOBUF_NODISCARD std::string* release_kernel_name();
  void set_allocated_kernel_name(std::string* kernel_name);
  private:
  const std::string& _internal_kernel_name() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_kernel_name(const std::string& value);
  std::string* _internal_mutable_kernel_name();
  public:

  // .xla.gpu.Dim3DProto launch_block_counts = 4;
  bool has_launch_block_counts() const;
  private:
  bool _internal_has_launch_block_counts() const;
  public:
  void clear_launch_block_counts();
  const ::xla::gpu::Dim3DProto& launch_block_counts() const;
  PROTOBUF_NODISCARD ::xla::gpu::Dim3DProto* release_launch_block_counts();
  ::xla::gpu::Dim3DProto* mutable_launch_block_counts();
  void set_allocated_launch_block_counts(::xla::gpu::Dim3DProto* launch_block_counts);
  private:
  const ::xla::gpu::Dim3DProto& _internal_launch_block_counts() const;
  ::xla::gpu::Dim3DProto* _internal_mutable_launch_block_counts();
  public:
  void unsafe_arena_set_allocated_launch_block_counts(
      ::xla::gpu::Dim3DProto* launch_block_counts);
  ::xla::gpu::Dim3DProto* unsafe_arena_release_launch_block_counts();

  // .xla.gpu.Dim3DProto launch_thread_counts_per_block = 5;
  bool has_launch_thread_counts_per_block() const;
  private:
  bool _internal_has_launch_thread_counts_per_block() const;
  public:
  void clear_launch_thread_counts_per_block();
  const ::xla::gpu::Dim3DProto& launch_thread_counts_per_block() const;
  PROTOBUF_NODISCARD ::xla::gpu::Dim3DProto* release_launch_thread_counts_per_block();
  ::xla::gpu::Dim3DProto* mutable_launch_thread_counts_per_block();
  void set_allocated_launch_thread_counts_per_block(::xla::gpu::Dim3DProto* launch_thread_counts_per_block);
  private:
  const ::xla::gpu::Dim3DProto& _internal_launch_thread_counts_per_block() const;
  ::xla::gpu::Dim3DProto* _internal_mutable_launch_thread_counts_per_block();
  public:
  void unsafe_arena_set_allocated_launch_thread_counts_per_block(
      ::xla::gpu::Dim3DProto* launch_thread_counts_per_block);
  ::xla::gpu::Dim3DProto* unsafe_arena_release_launch_thread_counts_per_block();

  // optional .xla.gpu.Dim3DProto cluster_dim = 6;
  bool has_cluster_dim() const;
  private:
  bool _internal_has_cluster_dim() const;
  public:
  void clear_cluster_dim();
  const ::xla::gpu::Dim3DProto& cluster_dim() const;
  PROTOBUF_NODISCARD ::xla::gpu::Dim3DProto* release_cluster_dim();
  ::xla::gpu::Dim3DProto* mutable_cluster_dim();
  void set_allocated_cluster_dim(::xla::gpu::Dim3DProto* cluster_dim);
  private:
  const ::xla::gpu::Dim3DProto& _internal_cluster_dim() const;
  ::xla::gpu::Dim3DProto* _internal_mutable_cluster_dim();
  public:
  void unsafe_arena_set_allocated_cluster_dim(
      ::xla::gpu::Dim3DProto* cluster_dim);
  ::xla::gpu::Dim3DProto* unsafe_arena_release_cluster_dim();

  // int64 shmem_bytes = 7;
  void clear_shmem_bytes();
  int64_t shmem_bytes() const;
  void set_shmem_bytes(int64_t value);
  private:
  int64_t _internal_shmem_bytes() const;
  void _internal_set_shmem_bytes(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.KernelThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::internal::HasBits<1> _has_bits_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
    ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::buffer_assignment::BufferAllocationSliceProto > args_;
    ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool > written_;
    ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr kernel_name_;
    ::xla::gpu::Dim3DProto* launch_block_counts_;
    ::xla::gpu::Dim3DProto* launch_thread_counts_per_block_;
    ::xla::gpu::Dim3DProto* cluster_dim_;
    int64_t shmem_bytes_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT GemmThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.GemmThunkProto) */ {
 public:
  inline GemmThunkProto() : GemmThunkProto(nullptr) {}
  ~GemmThunkProto() override;
  explicit PROTOBUF_CONSTEXPR GemmThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GemmThunkProto(const GemmThunkProto& from);
  GemmThunkProto(GemmThunkProto&& from) noexcept
    : GemmThunkProto() {
    *this = ::std::move(from);
  }

  inline GemmThunkProto& operator=(const GemmThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline GemmThunkProto& operator=(GemmThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const GemmThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const GemmThunkProto* internal_default_instance() {
    return reinterpret_cast<const GemmThunkProto*>(
               &_GemmThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    8;

  friend void swap(GemmThunkProto& a, GemmThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(GemmThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GemmThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  GemmThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<GemmThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const GemmThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const GemmThunkProto& from) {
    GemmThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(GemmThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.GemmThunkProto";
  }
  protected:
  explicit GemmThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kGemmConfigFieldNumber = 1,
    kLhsBufferFieldNumber = 2,
    kRhsBufferFieldNumber = 3,
    kOutputBufferFieldNumber = 4,
    kWorkspaceFieldNumber = 5,
    kDeterministicFieldNumber = 6,
  };
  // .xla.GemmConfigProto gemm_config = 1;
  bool has_gemm_config() const;
  private:
  bool _internal_has_gemm_config() const;
  public:
  void clear_gemm_config();
  const ::xla::GemmConfigProto& gemm_config() const;
  PROTOBUF_NODISCARD ::xla::GemmConfigProto* release_gemm_config();
  ::xla::GemmConfigProto* mutable_gemm_config();
  void set_allocated_gemm_config(::xla::GemmConfigProto* gemm_config);
  private:
  const ::xla::GemmConfigProto& _internal_gemm_config() const;
  ::xla::GemmConfigProto* _internal_mutable_gemm_config();
  public:
  void unsafe_arena_set_allocated_gemm_config(
      ::xla::GemmConfigProto* gemm_config);
  ::xla::GemmConfigProto* unsafe_arena_release_gemm_config();

  // .xla.buffer_assignment.BufferAllocationSliceProto lhs_buffer = 2;
  bool has_lhs_buffer() const;
  private:
  bool _internal_has_lhs_buffer() const;
  public:
  void clear_lhs_buffer();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& lhs_buffer() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_lhs_buffer();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_lhs_buffer();
  void set_allocated_lhs_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* lhs_buffer);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_lhs_buffer() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_lhs_buffer();
  public:
  void unsafe_arena_set_allocated_lhs_buffer(
      ::xla::buffer_assignment::BufferAllocationSliceProto* lhs_buffer);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_lhs_buffer();

  // .xla.buffer_assignment.BufferAllocationSliceProto rhs_buffer = 3;
  bool has_rhs_buffer() const;
  private:
  bool _internal_has_rhs_buffer() const;
  public:
  void clear_rhs_buffer();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& rhs_buffer() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_rhs_buffer();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_rhs_buffer();
  void set_allocated_rhs_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* rhs_buffer);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_rhs_buffer() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_rhs_buffer();
  public:
  void unsafe_arena_set_allocated_rhs_buffer(
      ::xla::buffer_assignment::BufferAllocationSliceProto* rhs_buffer);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_rhs_buffer();

  // .xla.buffer_assignment.BufferAllocationSliceProto output_buffer = 4;
  bool has_output_buffer() const;
  private:
  bool _internal_has_output_buffer() const;
  public:
  void clear_output_buffer();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& output_buffer() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_output_buffer();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_output_buffer();
  void set_allocated_output_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* output_buffer);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_output_buffer() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_output_buffer();
  public:
  void unsafe_arena_set_allocated_output_buffer(
      ::xla::buffer_assignment::BufferAllocationSliceProto* output_buffer);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_output_buffer();

  // optional .xla.buffer_assignment.BufferAllocationSliceProto workspace = 5;
  bool has_workspace() const;
  private:
  bool _internal_has_workspace() const;
  public:
  void clear_workspace();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& workspace() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_workspace();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_workspace();
  void set_allocated_workspace(::xla::buffer_assignment::BufferAllocationSliceProto* workspace);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_workspace() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_workspace();
  public:
  void unsafe_arena_set_allocated_workspace(
      ::xla::buffer_assignment::BufferAllocationSliceProto* workspace);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_workspace();

  // bool deterministic = 6;
  void clear_deterministic();
  bool deterministic() const;
  void set_deterministic(bool value);
  private:
  bool _internal_deterministic() const;
  void _internal_set_deterministic(bool value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.GemmThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::internal::HasBits<1> _has_bits_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
    ::xla::GemmConfigProto* gemm_config_;
    ::xla::buffer_assignment::BufferAllocationSliceProto* lhs_buffer_;
    ::xla::buffer_assignment::BufferAllocationSliceProto* rhs_buffer_;
    ::xla::buffer_assignment::BufferAllocationSliceProto* output_buffer_;
    ::xla::buffer_assignment::BufferAllocationSliceProto* workspace_;
    bool deterministic_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT ThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.ThunkProto) */ {
 public:
  inline ThunkProto() : ThunkProto(nullptr) {}
  ~ThunkProto() override;
  explicit PROTOBUF_CONSTEXPR ThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ThunkProto(const ThunkProto& from);
  ThunkProto(ThunkProto&& from) noexcept
    : ThunkProto() {
    *this = ::std::move(from);
  }

  inline ThunkProto& operator=(const ThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline ThunkProto& operator=(ThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ThunkProto& default_instance() {
    return *internal_default_instance();
  }
  enum ImplCase {
    kSequentialThunk = 2,
    kCopyThunk = 3,
    kDeviceToHostCopyThunk = 4,
    kHostToDeviceCopyThunk = 5,
    kConditionalThunk = 6,
    kWhileThunk = 7,
    kKernelThunk = 8,
    kGemmThunk = 9,
    IMPL_NOT_SET = 0,
  };

  static inline const ThunkProto* internal_default_instance() {
    return reinterpret_cast<const ThunkProto*>(
               &_ThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    9;

  friend void swap(ThunkProto& a, ThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(ThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const ThunkProto& from) {
    ThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.ThunkProto";
  }
  protected:
  explicit ThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kThunkInfoFieldNumber = 1,
    kSequentialThunkFieldNumber = 2,
    kCopyThunkFieldNumber = 3,
    kDeviceToHostCopyThunkFieldNumber = 4,
    kHostToDeviceCopyThunkFieldNumber = 5,
    kConditionalThunkFieldNumber = 6,
    kWhileThunkFieldNumber = 7,
    kKernelThunkFieldNumber = 8,
    kGemmThunkFieldNumber = 9,
  };
  // .xla.gpu.ThunkInfoProto thunk_info = 1;
  bool has_thunk_info() const;
  private:
  bool _internal_has_thunk_info() const;
  public:
  void clear_thunk_info();
  const ::xla::gpu::ThunkInfoProto& thunk_info() const;
  PROTOBUF_NODISCARD ::xla::gpu::ThunkInfoProto* release_thunk_info();
  ::xla::gpu::ThunkInfoProto* mutable_thunk_info();
  void set_allocated_thunk_info(::xla::gpu::ThunkInfoProto* thunk_info);
  private:
  const ::xla::gpu::ThunkInfoProto& _internal_thunk_info() const;
  ::xla::gpu::ThunkInfoProto* _internal_mutable_thunk_info();
  public:
  void unsafe_arena_set_allocated_thunk_info(
      ::xla::gpu::ThunkInfoProto* thunk_info);
  ::xla::gpu::ThunkInfoProto* unsafe_arena_release_thunk_info();

  // .xla.gpu.SequentialThunkProto sequential_thunk = 2;
  bool has_sequential_thunk() const;
  private:
  bool _internal_has_sequential_thunk() const;
  public:
  void clear_sequential_thunk();
  const ::xla::gpu::SequentialThunkProto& sequential_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::SequentialThunkProto* release_sequential_thunk();
  ::xla::gpu::SequentialThunkProto* mutable_sequential_thunk();
  void set_allocated_sequential_thunk(::xla::gpu::SequentialThunkProto* sequential_thunk);
  private:
  const ::xla::gpu::SequentialThunkProto& _internal_sequential_thunk() const;
  ::xla::gpu::SequentialThunkProto* _internal_mutable_sequential_thunk();
  public:
  void unsafe_arena_set_allocated_sequential_thunk(
      ::xla::gpu::SequentialThunkProto* sequential_thunk);
  ::xla::gpu::SequentialThunkProto* unsafe_arena_release_sequential_thunk();

  // .xla.gpu.CopyThunkProto copy_thunk = 3;
  bool has_copy_thunk() const;
  private:
  bool _internal_has_copy_thunk() const;
  public:
  void clear_copy_thunk();
  const ::xla::gpu::CopyThunkProto& copy_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::CopyThunkProto* release_copy_thunk();
  ::xla::gpu::CopyThunkProto* mutable_copy_thunk();
  void set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk);
  private:
  const ::xla::gpu::CopyThunkProto& _internal_copy_thunk() const;
  ::xla::gpu::CopyThunkProto* _internal_mutable_copy_thunk();
  public:
  void unsafe_arena_set_allocated_copy_thunk(
      ::xla::gpu::CopyThunkProto* copy_thunk);
  ::xla::gpu::CopyThunkProto* unsafe_arena_release_copy_thunk();

  // .xla.gpu.DeviceToHostCopyThunkProto device_to_host_copy_thunk = 4;
  bool has_device_to_host_copy_thunk() const;
  private:
  bool _internal_has_device_to_host_copy_thunk() const;
  public:
  void clear_device_to_host_copy_thunk();
  const ::xla::gpu::DeviceToHostCopyThunkProto& device_to_host_copy_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::DeviceToHostCopyThunkProto* release_device_to_host_copy_thunk();
  ::xla::gpu::DeviceToHostCopyThunkProto* mutable_device_to_host_copy_thunk();
  void set_allocated_device_to_host_copy_thunk(::xla::gpu::DeviceToHostCopyThunkProto* device_to_host_copy_thunk);
  private:
  const ::xla::gpu::DeviceToHostCopyThunkProto& _internal_device_to_host_copy_thunk() const;
  ::xla::gpu::DeviceToHostCopyThunkProto* _internal_mutable_device_to_host_copy_thunk();
  public:
  void unsafe_arena_set_allocated_device_to_host_copy_thunk(
      ::xla::gpu::DeviceToHostCopyThunkProto* device_to_host_copy_thunk);
  ::xla::gpu::DeviceToHostCopyThunkProto* unsafe_arena_release_device_to_host_copy_thunk();

  // .xla.gpu.HostToDeviceCopyThunkProto host_to_device_copy_thunk = 5;
  bool has_host_to_device_copy_thunk() const;
  private:
  bool _internal_has_host_to_device_copy_thunk() const;
  public:
  void clear_host_to_device_copy_thunk();
  const ::xla::gpu::HostToDeviceCopyThunkProto& host_to_device_copy_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::HostToDeviceCopyThunkProto* release_host_to_device_copy_thunk();
  ::xla::gpu::HostToDeviceCopyThunkProto* mutable_host_to_device_copy_thunk();
  void set_allocated_host_to_device_copy_thunk(::xla::gpu::HostToDeviceCopyThunkProto* host_to_device_copy_thunk);
  private:
  const ::xla::gpu::HostToDeviceCopyThunkProto& _internal_host_to_device_copy_thunk() const;
  ::xla::gpu::HostToDeviceCopyThunkProto* _internal_mutable_host_to_device_copy_thunk();
  public:
  void unsafe_arena_set_allocated_host_to_device_copy_thunk(
      ::xla::gpu::HostToDeviceCopyThunkProto* host_to_device_copy_thunk);
  ::xla::gpu::HostToDeviceCopyThunkProto* unsafe_arena_release_host_to_device_copy_thunk();

  // .xla.gpu.ConditionalThunkProto conditional_thunk = 6;
  bool has_conditional_thunk() const;
  private:
  bool _internal_has_conditional_thunk() const;
  public:
  void clear_conditional_thunk();
  const ::xla::gpu::ConditionalThunkProto& conditional_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::ConditionalThunkProto* release_conditional_thunk();
  ::xla::gpu::ConditionalThunkProto* mutable_conditional_thunk();
  void set_allocated_conditional_thunk(::xla::gpu::ConditionalThunkProto* conditional_thunk);
  private:
  const ::xla::gpu::ConditionalThunkProto& _internal_conditional_thunk() const;
  ::xla::gpu::ConditionalThunkProto* _internal_mutable_conditional_thunk();
  public:
  void unsafe_arena_set_allocated_conditional_thunk(
      ::xla::gpu::ConditionalThunkProto* conditional_thunk);
  ::xla::gpu::ConditionalThunkProto* unsafe_arena_release_conditional_thunk();

  // .xla.gpu.WhileThunkProto while_thunk = 7;
  bool has_while_thunk() const;
  private:
  bool _internal_has_while_thunk() const;
  public:
  void clear_while_thunk();
  const ::xla::gpu::WhileThunkProto& while_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::WhileThunkProto* release_while_thunk();
  ::xla::gpu::WhileThunkProto* mutable_while_thunk();
  void set_allocated_while_thunk(::xla::gpu::WhileThunkProto* while_thunk);
  private:
  const ::xla::gpu::WhileThunkProto& _internal_while_thunk() const;
  ::xla::gpu::WhileThunkProto* _internal_mutable_while_thunk();
  public:
  void unsafe_arena_set_allocated_while_thunk(
      ::xla::gpu::WhileThunkProto* while_thunk);
  ::xla::gpu::WhileThunkProto* unsafe_arena_release_while_thunk();

  // .xla.gpu.KernelThunkProto kernel_thunk = 8;
  bool has_kernel_thunk() const;
  private:
  bool _internal_has_kernel_thunk() const;
  public:
  void clear_kernel_thunk();
  const ::xla::gpu::KernelThunkProto& kernel_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::KernelThunkProto* release_kernel_thunk();
  ::xla::gpu::KernelThunkProto* mutable_kernel_thunk();
  void set_allocated_kernel_thunk(::xla::gpu::KernelThunkProto* kernel_thunk);
  private:
  const ::xla::gpu::KernelThunkProto& _internal_kernel_thunk() const;
  ::xla::gpu::KernelThunkProto* _internal_mutable_kernel_thunk();
  public:
  void unsafe_arena_set_allocated_kernel_thunk(
      ::xla::gpu::KernelThunkProto* kernel_thunk);
  ::xla::gpu::KernelThunkProto* unsafe_arena_release_kernel_thunk();

  // .xla.gpu.GemmThunkProto gemm_thunk = 9;
  bool has_gemm_thunk() const;
  private:
  bool _internal_has_gemm_thunk() const;
  public:
  void clear_gemm_thunk();
  const ::xla::gpu::GemmThunkProto& gemm_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::GemmThunkProto* release_gemm_thunk();
  ::xla::gpu::GemmThunkProto* mutable_gemm_thunk();
  void set_allocated_gemm_thunk(::xla::gpu::GemmThunkProto* gemm_thunk);
  private:
  const ::xla::gpu::GemmThunkProto& _internal_gemm_thunk() const;
  ::xla::gpu::GemmThunkProto* _internal_mutable_gemm_thunk();
  public:
  void unsafe_arena_set_allocated_gemm_thunk(
      ::xla::gpu::GemmThunkProto* gemm_thunk);
  ::xla::gpu::GemmThunkProto* unsafe_arena_release_gemm_thunk();

  void clear_impl();
  ImplCase impl_case() const;
  // @@protoc_insertion_point(class_scope:xla.gpu.ThunkProto)
 private:
  class _Internal;
  void set_has_sequential_thunk();
  void set_has_copy_thunk();
  void set_has_device_to_host_copy_thunk();
  void set_has_host_to_device_copy_thunk();
  void set_has_conditional_thunk();
  void set_has_while_thunk();
  void set_has_kernel_thunk();
  void set_has_gemm_thunk();

  inline bool has_impl() const;
  inline void clear_has_impl();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::xla::gpu::ThunkInfoProto* thunk_info_;
    union ImplUnion {
      constexpr ImplUnion() : _constinit_{} {}
        ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
      ::xla::gpu::SequentialThunkProto* sequential_thunk_;
      ::xla::gpu::CopyThunkProto* copy_thunk_;
      ::xla::gpu::DeviceToHostCopyThunkProto* device_to_host_copy_thunk_;
      ::xla::gpu::HostToDeviceCopyThunkProto* host_to_device_copy_thunk_;
      ::xla::gpu::ConditionalThunkProto* conditional_thunk_;
      ::xla::gpu::WhileThunkProto* while_thunk_;
      ::xla::gpu::KernelThunkProto* kernel_thunk_;
      ::xla::gpu::GemmThunkProto* gemm_thunk_;
    } impl_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
    uint32_t _oneof_case_[1];

  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT SequentialThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.SequentialThunkProto) */ {
 public:
  inline SequentialThunkProto() : SequentialThunkProto(nullptr) {}
  ~SequentialThunkProto() override;
  explicit PROTOBUF_CONSTEXPR SequentialThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SequentialThunkProto(const SequentialThunkProto& from);
  SequentialThunkProto(SequentialThunkProto&& from) noexcept
    : SequentialThunkProto() {
    *this = ::std::move(from);
  }

  inline SequentialThunkProto& operator=(const SequentialThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline SequentialThunkProto& operator=(SequentialThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const SequentialThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const SequentialThunkProto* internal_default_instance() {
    return reinterpret_cast<const SequentialThunkProto*>(
               &_SequentialThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    10;

  friend void swap(SequentialThunkProto& a, SequentialThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(SequentialThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SequentialThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SequentialThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SequentialThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const SequentialThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const SequentialThunkProto& from) {
    SequentialThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(SequentialThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.SequentialThunkProto";
  }
  protected:
  explicit SequentialThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kThunksFieldNumber = 1,
  };
  // repeated .xla.gpu.ThunkProto thunks = 1;
  int thunks_size() const;
  private:
  int _internal_thunks_size() const;
  public:
  void clear_thunks();
  ::xla::gpu::ThunkProto* mutable_thunks(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >*
      mutable_thunks();
  private:
  const ::xla::gpu::ThunkProto& _internal_thunks(int index) const;
  ::xla::gpu::ThunkProto* _internal_add_thunks();
  public:
  const ::xla::gpu::ThunkProto& thunks(int index) const;
  ::xla::gpu::ThunkProto* add_thunks();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >&
      thunks() const;

  // @@protoc_insertion_point(class_scope:xla.gpu.SequentialThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto > thunks_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// ThunkInfoProto

// string profile_annotation = 1;
inline void ThunkInfoProto::clear_profile_annotation() {
  _impl_.profile_annotation_.ClearToEmpty();
}
inline const std::string& ThunkInfoProto::profile_annotation() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkInfoProto.profile_annotation)
  return _internal_profile_annotation();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void ThunkInfoProto::set_profile_annotation(ArgT0&& arg0, ArgT... args) {
 
 _impl_.profile_annotation_.Set(static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:xla.gpu.ThunkInfoProto.profile_annotation)
}
inline std::string* ThunkInfoProto::mutable_profile_annotation() {
  std::string* _s = _internal_mutable_profile_annotation();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkInfoProto.profile_annotation)
  return _s;
}
inline const std::string& ThunkInfoProto::_internal_profile_annotation() const {
  return _impl_.profile_annotation_.Get();
}
inline void ThunkInfoProto::_internal_set_profile_annotation(const std::string& value) {
  
  _impl_.profile_annotation_.Set(value, GetArenaForAllocation());
}
inline std::string* ThunkInfoProto::_internal_mutable_profile_annotation() {
  
  return _impl_.profile_annotation_.Mutable(GetArenaForAllocation());
}
inline std::string* ThunkInfoProto::release_profile_annotation() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkInfoProto.profile_annotation)
  return _impl_.profile_annotation_.Release();
}
inline void ThunkInfoProto::set_allocated_profile_annotation(std::string* profile_annotation) {
  if (profile_annotation != nullptr) {
    
  } else {
    
  }
  _impl_.profile_annotation_.SetAllocated(profile_annotation, GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (_impl_.profile_annotation_.IsDefault()) {
    _impl_.profile_annotation_.Set("", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.ThunkInfoProto.profile_annotation)
}

// int64 execution_stream_id = 2;
inline void ThunkInfoProto::clear_execution_stream_id() {
  _impl_.execution_stream_id_ = int64_t{0};
}
inline int64_t ThunkInfoProto::_internal_execution_stream_id() const {
  return _impl_.execution_stream_id_;
}
inline int64_t ThunkInfoProto::execution_stream_id() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkInfoProto.execution_stream_id)
  return _internal_execution_stream_id();
}
inline void ThunkInfoProto::_internal_set_execution_stream_id(int64_t value) {
  
  _impl_.execution_stream_id_ = value;
}
inline void ThunkInfoProto::set_execution_stream_id(int64_t value) {
  _internal_set_execution_stream_id(value);
  // @@protoc_insertion_point(field_set:xla.gpu.ThunkInfoProto.execution_stream_id)
}

// -------------------------------------------------------------------

// CopyThunkProto

// .xla.buffer_assignment.BufferAllocationSliceProto source_buffer = 1;
inline bool CopyThunkProto::_internal_has_source_buffer() const {
  return this != internal_default_instance() && _impl_.source_buffer_ != nullptr;
}
inline bool CopyThunkProto::has_source_buffer() const {
  return _internal_has_source_buffer();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& CopyThunkProto::_internal_source_buffer() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.source_buffer_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& CopyThunkProto::source_buffer() const {
  // @@protoc_insertion_point(field_get:xla.gpu.CopyThunkProto.source_buffer)
  return _internal_source_buffer();
}
inline void CopyThunkProto::unsafe_arena_set_allocated_source_buffer(
    ::xla::buffer_assignment::BufferAllocationSliceProto* source_buffer) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.source_buffer_);
  }
  _impl_.source_buffer_ = source_buffer;
  if (source_buffer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.CopyThunkProto.source_buffer)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::release_source_buffer() {
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.source_buffer_;
  _impl_.source_buffer_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::unsafe_arena_release_source_buffer() {
  // @@protoc_insertion_point(field_release:xla.gpu.CopyThunkProto.source_buffer)
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.source_buffer_;
  _impl_.source_buffer_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::_internal_mutable_source_buffer() {
  
  if (_impl_.source_buffer_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.source_buffer_ = p;
  }
  return _impl_.source_buffer_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::mutable_source_buffer() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_source_buffer();
  // @@protoc_insertion_point(field_mutable:xla.gpu.CopyThunkProto.source_buffer)
  return _msg;
}
inline void CopyThunkProto::set_allocated_source_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* source_buffer) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.source_buffer_);
  }
  if (source_buffer) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(source_buffer));
    if (message_arena != submessage_arena) {
      source_buffer = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, source_buffer, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.source_buffer_ = source_buffer;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.CopyThunkProto.source_buffer)
}

// .xla.buffer_assignment.BufferAllocationSliceProto destination_buffer = 2;
inline bool CopyThunkProto::_internal_has_destination_buffer() const {
  return this != internal_default_instance() && _impl_.destination_buffer_ != nullptr;
}
inline bool CopyThunkProto::has_destination_buffer() const {
  return _internal_has_destination_buffer();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& CopyThunkProto::_internal_destination_buffer() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.destination_buffer_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& CopyThunkProto::destination_buffer() const {
  // @@protoc_insertion_point(field_get:xla.gpu.CopyThunkProto.destination_buffer)
  return _internal_destination_buffer();
}
inline void CopyThunkProto::unsafe_arena_set_allocated_destination_buffer(
    ::xla::buffer_assignment::BufferAllocationSliceProto* destination_buffer) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.destination_buffer_);
  }
  _impl_.destination_buffer_ = destination_buffer;
  if (destination_buffer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.CopyThunkProto.destination_buffer)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::release_destination_buffer() {
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.destination_buffer_;
  _impl_.destination_buffer_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::unsafe_arena_release_destination_buffer() {
  // @@protoc_insertion_point(field_release:xla.gpu.CopyThunkProto.destination_buffer)
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.destination_buffer_;
  _impl_.destination_buffer_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::_internal_mutable_destination_buffer() {
  
  if (_impl_.destination_buffer_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.destination_buffer_ = p;
  }
  return _impl_.destination_buffer_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::mutable_destination_buffer() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_destination_buffer();
  // @@protoc_insertion_point(field_mutable:xla.gpu.CopyThunkProto.destination_buffer)
  return _msg;
}
inline void CopyThunkProto::set_allocated_destination_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* destination_buffer) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.destination_buffer_);
  }
  if (destination_buffer) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(destination_buffer));
    if (message_arena != submessage_arena) {
      destination_buffer = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, destination_buffer, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.destination_buffer_ = destination_buffer;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.CopyThunkProto.destination_buffer)
}

// int64 mem_size = 3;
inline void CopyThunkProto::clear_mem_size() {
  _impl_.mem_size_ = int64_t{0};
}
inline int64_t CopyThunkProto::_internal_mem_size() const {
  return _impl_.mem_size_;
}
inline int64_t CopyThunkProto::mem_size() const {
  // @@protoc_insertion_point(field_get:xla.gpu.CopyThunkProto.mem_size)
  return _internal_mem_size();
}
inline void CopyThunkProto::_internal_set_mem_size(int64_t value) {
  
  _impl_.mem_size_ = value;
}
inline void CopyThunkProto::set_mem_size(int64_t value) {
  _internal_set_mem_size(value);
  // @@protoc_insertion_point(field_set:xla.gpu.CopyThunkProto.mem_size)
}

// -------------------------------------------------------------------

// DeviceToHostCopyThunkProto

// .xla.gpu.CopyThunkProto copy_thunk = 1;
inline bool DeviceToHostCopyThunkProto::_internal_has_copy_thunk() const {
  return this != internal_default_instance() && _impl_.copy_thunk_ != nullptr;
}
inline bool DeviceToHostCopyThunkProto::has_copy_thunk() const {
  return _internal_has_copy_thunk();
}
inline void DeviceToHostCopyThunkProto::clear_copy_thunk() {
  if (GetArenaForAllocation() == nullptr && _impl_.copy_thunk_ != nullptr) {
    delete _impl_.copy_thunk_;
  }
  _impl_.copy_thunk_ = nullptr;
}
inline const ::xla::gpu::CopyThunkProto& DeviceToHostCopyThunkProto::_internal_copy_thunk() const {
  const ::xla::gpu::CopyThunkProto* p = _impl_.copy_thunk_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::CopyThunkProto&>(
      ::xla::gpu::_CopyThunkProto_default_instance_);
}
inline const ::xla::gpu::CopyThunkProto& DeviceToHostCopyThunkProto::copy_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.DeviceToHostCopyThunkProto.copy_thunk)
  return _internal_copy_thunk();
}
inline void DeviceToHostCopyThunkProto::unsafe_arena_set_allocated_copy_thunk(
    ::xla::gpu::CopyThunkProto* copy_thunk) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.copy_thunk_);
  }
  _impl_.copy_thunk_ = copy_thunk;
  if (copy_thunk) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.DeviceToHostCopyThunkProto.copy_thunk)
}
inline ::xla::gpu::CopyThunkProto* DeviceToHostCopyThunkProto::release_copy_thunk() {
  
  ::xla::gpu::CopyThunkProto* temp = _impl_.copy_thunk_;
  _impl_.copy_thunk_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::CopyThunkProto* DeviceToHostCopyThunkProto::unsafe_arena_release_copy_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.DeviceToHostCopyThunkProto.copy_thunk)
  
  ::xla::gpu::CopyThunkProto* temp = _impl_.copy_thunk_;
  _impl_.copy_thunk_ = nullptr;
  return temp;
}
inline ::xla::gpu::CopyThunkProto* DeviceToHostCopyThunkProto::_internal_mutable_copy_thunk() {
  
  if (_impl_.copy_thunk_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::CopyThunkProto>(GetArenaForAllocation());
    _impl_.copy_thunk_ = p;
  }
  return _impl_.copy_thunk_;
}
inline ::xla::gpu::CopyThunkProto* DeviceToHostCopyThunkProto::mutable_copy_thunk() {
  ::xla::gpu::CopyThunkProto* _msg = _internal_mutable_copy_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.DeviceToHostCopyThunkProto.copy_thunk)
  return _msg;
}
inline void DeviceToHostCopyThunkProto::set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.copy_thunk_;
  }
  if (copy_thunk) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(copy_thunk);
    if (message_arena != submessage_arena) {
      copy_thunk = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, copy_thunk, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.copy_thunk_ = copy_thunk;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.DeviceToHostCopyThunkProto.copy_thunk)
}

// -------------------------------------------------------------------

// HostToDeviceCopyThunkProto

// .xla.gpu.CopyThunkProto copy_thunk = 1;
inline bool HostToDeviceCopyThunkProto::_internal_has_copy_thunk() const {
  return this != internal_default_instance() && _impl_.copy_thunk_ != nullptr;
}
inline bool HostToDeviceCopyThunkProto::has_copy_thunk() const {
  return _internal_has_copy_thunk();
}
inline void HostToDeviceCopyThunkProto::clear_copy_thunk() {
  if (GetArenaForAllocation() == nullptr && _impl_.copy_thunk_ != nullptr) {
    delete _impl_.copy_thunk_;
  }
  _impl_.copy_thunk_ = nullptr;
}
inline const ::xla::gpu::CopyThunkProto& HostToDeviceCopyThunkProto::_internal_copy_thunk() const {
  const ::xla::gpu::CopyThunkProto* p = _impl_.copy_thunk_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::CopyThunkProto&>(
      ::xla::gpu::_CopyThunkProto_default_instance_);
}
inline const ::xla::gpu::CopyThunkProto& HostToDeviceCopyThunkProto::copy_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.HostToDeviceCopyThunkProto.copy_thunk)
  return _internal_copy_thunk();
}
inline void HostToDeviceCopyThunkProto::unsafe_arena_set_allocated_copy_thunk(
    ::xla::gpu::CopyThunkProto* copy_thunk) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.copy_thunk_);
  }
  _impl_.copy_thunk_ = copy_thunk;
  if (copy_thunk) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.HostToDeviceCopyThunkProto.copy_thunk)
}
inline ::xla::gpu::CopyThunkProto* HostToDeviceCopyThunkProto::release_copy_thunk() {
  
  ::xla::gpu::CopyThunkProto* temp = _impl_.copy_thunk_;
  _impl_.copy_thunk_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::CopyThunkProto* HostToDeviceCopyThunkProto::unsafe_arena_release_copy_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.HostToDeviceCopyThunkProto.copy_thunk)
  
  ::xla::gpu::CopyThunkProto* temp = _impl_.copy_thunk_;
  _impl_.copy_thunk_ = nullptr;
  return temp;
}
inline ::xla::gpu::CopyThunkProto* HostToDeviceCopyThunkProto::_internal_mutable_copy_thunk() {
  
  if (_impl_.copy_thunk_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::CopyThunkProto>(GetArenaForAllocation());
    _impl_.copy_thunk_ = p;
  }
  return _impl_.copy_thunk_;
}
inline ::xla::gpu::CopyThunkProto* HostToDeviceCopyThunkProto::mutable_copy_thunk() {
  ::xla::gpu::CopyThunkProto* _msg = _internal_mutable_copy_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.HostToDeviceCopyThunkProto.copy_thunk)
  return _msg;
}
inline void HostToDeviceCopyThunkProto::set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.copy_thunk_;
  }
  if (copy_thunk) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(copy_thunk);
    if (message_arena != submessage_arena) {
      copy_thunk = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, copy_thunk, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.copy_thunk_ = copy_thunk;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.HostToDeviceCopyThunkProto.copy_thunk)
}

// -------------------------------------------------------------------

// ConditionalThunkProto

// .xla.buffer_assignment.BufferAllocationSliceProto branch_index_buffer = 1;
inline bool ConditionalThunkProto::_internal_has_branch_index_buffer() const {
  return this != internal_default_instance() && _impl_.branch_index_buffer_ != nullptr;
}
inline bool ConditionalThunkProto::has_branch_index_buffer() const {
  return _internal_has_branch_index_buffer();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& ConditionalThunkProto::_internal_branch_index_buffer() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.branch_index_buffer_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& ConditionalThunkProto::branch_index_buffer() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ConditionalThunkProto.branch_index_buffer)
  return _internal_branch_index_buffer();
}
inline void ConditionalThunkProto::unsafe_arena_set_allocated_branch_index_buffer(
    ::xla::buffer_assignment::BufferAllocationSliceProto* branch_index_buffer) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.branch_index_buffer_);
  }
  _impl_.branch_index_buffer_ = branch_index_buffer;
  if (branch_index_buffer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ConditionalThunkProto.branch_index_buffer)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* ConditionalThunkProto::release_branch_index_buffer() {
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.branch_index_buffer_;
  _impl_.branch_index_buffer_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* ConditionalThunkProto::unsafe_arena_release_branch_index_buffer() {
  // @@protoc_insertion_point(field_release:xla.gpu.ConditionalThunkProto.branch_index_buffer)
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.branch_index_buffer_;
  _impl_.branch_index_buffer_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* ConditionalThunkProto::_internal_mutable_branch_index_buffer() {
  
  if (_impl_.branch_index_buffer_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.branch_index_buffer_ = p;
  }
  return _impl_.branch_index_buffer_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* ConditionalThunkProto::mutable_branch_index_buffer() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_branch_index_buffer();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ConditionalThunkProto.branch_index_buffer)
  return _msg;
}
inline void ConditionalThunkProto::set_allocated_branch_index_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* branch_index_buffer) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.branch_index_buffer_);
  }
  if (branch_index_buffer) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(branch_index_buffer));
    if (message_arena != submessage_arena) {
      branch_index_buffer = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, branch_index_buffer, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.branch_index_buffer_ = branch_index_buffer;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.ConditionalThunkProto.branch_index_buffer)
}

// repeated .xla.gpu.SequentialThunkProto branch_thunks = 2;
inline int ConditionalThunkProto::_internal_branch_thunks_size() const {
  return _impl_.branch_thunks_.size();
}
inline int ConditionalThunkProto::branch_thunks_size() const {
  return _internal_branch_thunks_size();
}
inline void ConditionalThunkProto::clear_branch_thunks() {
  _impl_.branch_thunks_.Clear();
}
inline ::xla::gpu::SequentialThunkProto* ConditionalThunkProto::mutable_branch_thunks(int index) {
  // @@protoc_insertion_point(field_mutable:xla.gpu.ConditionalThunkProto.branch_thunks)
  return _impl_.branch_thunks_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::SequentialThunkProto >*
ConditionalThunkProto::mutable_branch_thunks() {
  // @@protoc_insertion_point(field_mutable_list:xla.gpu.ConditionalThunkProto.branch_thunks)
  return &_impl_.branch_thunks_;
}
inline const ::xla::gpu::SequentialThunkProto& ConditionalThunkProto::_internal_branch_thunks(int index) const {
  return _impl_.branch_thunks_.Get(index);
}
inline const ::xla::gpu::SequentialThunkProto& ConditionalThunkProto::branch_thunks(int index) const {
  // @@protoc_insertion_point(field_get:xla.gpu.ConditionalThunkProto.branch_thunks)
  return _internal_branch_thunks(index);
}
inline ::xla::gpu::SequentialThunkProto* ConditionalThunkProto::_internal_add_branch_thunks() {
  return _impl_.branch_thunks_.Add();
}
inline ::xla::gpu::SequentialThunkProto* ConditionalThunkProto::add_branch_thunks() {
  ::xla::gpu::SequentialThunkProto* _add = _internal_add_branch_thunks();
  // @@protoc_insertion_point(field_add:xla.gpu.ConditionalThunkProto.branch_thunks)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::SequentialThunkProto >&
ConditionalThunkProto::branch_thunks() const {
  // @@protoc_insertion_point(field_list:xla.gpu.ConditionalThunkProto.branch_thunks)
  return _impl_.branch_thunks_;
}

// bool branch_index_is_bool = 3;
inline void ConditionalThunkProto::clear_branch_index_is_bool() {
  _impl_.branch_index_is_bool_ = false;
}
inline bool ConditionalThunkProto::_internal_branch_index_is_bool() const {
  return _impl_.branch_index_is_bool_;
}
inline bool ConditionalThunkProto::branch_index_is_bool() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ConditionalThunkProto.branch_index_is_bool)
  return _internal_branch_index_is_bool();
}
inline void ConditionalThunkProto::_internal_set_branch_index_is_bool(bool value) {
  
  _impl_.branch_index_is_bool_ = value;
}
inline void ConditionalThunkProto::set_branch_index_is_bool(bool value) {
  _internal_set_branch_index_is_bool(value);
  // @@protoc_insertion_point(field_set:xla.gpu.ConditionalThunkProto.branch_index_is_bool)
}

// -------------------------------------------------------------------

// WhileThunkProto

// .xla.buffer_assignment.BufferAllocationSliceProto condition_result_buffer_index = 1;
inline bool WhileThunkProto::_internal_has_condition_result_buffer_index() const {
  return this != internal_default_instance() && _impl_.condition_result_buffer_index_ != nullptr;
}
inline bool WhileThunkProto::has_condition_result_buffer_index() const {
  return _internal_has_condition_result_buffer_index();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& WhileThunkProto::_internal_condition_result_buffer_index() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.condition_result_buffer_index_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& WhileThunkProto::condition_result_buffer_index() const {
  // @@protoc_insertion_point(field_get:xla.gpu.WhileThunkProto.condition_result_buffer_index)
  return _internal_condition_result_buffer_index();
}
inline void WhileThunkProto::unsafe_arena_set_allocated_condition_result_buffer_index(
    ::xla::buffer_assignment::BufferAllocationSliceProto* condition_result_buffer_index) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.condition_result_buffer_index_);
  }
  _impl_.condition_result_buffer_index_ = condition_result_buffer_index;
  if (condition_result_buffer_index) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.WhileThunkProto.condition_result_buffer_index)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* WhileThunkProto::release_condition_result_buffer_index() {
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.condition_result_buffer_index_;
  _impl_.condition_result_buffer_index_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* WhileThunkProto::unsafe_arena_release_condition_result_buffer_index() {
  // @@protoc_insertion_point(field_release:xla.gpu.WhileThunkProto.condition_result_buffer_index)
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.condition_result_buffer_index_;
  _impl_.condition_result_buffer_index_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* WhileThunkProto::_internal_mutable_condition_result_buffer_index() {
  
  if (_impl_.condition_result_buffer_index_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.condition_result_buffer_index_ = p;
  }
  return _impl_.condition_result_buffer_index_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* WhileThunkProto::mutable_condition_result_buffer_index() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_condition_result_buffer_index();
  // @@protoc_insertion_point(field_mutable:xla.gpu.WhileThunkProto.condition_result_buffer_index)
  return _msg;
}
inline void WhileThunkProto::set_allocated_condition_result_buffer_index(::xla::buffer_assignment::BufferAllocationSliceProto* condition_result_buffer_index) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.condition_result_buffer_index_);
  }
  if (condition_result_buffer_index) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(condition_result_buffer_index));
    if (message_arena != submessage_arena) {
      condition_result_buffer_index = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, condition_result_buffer_index, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.condition_result_buffer_index_ = condition_result_buffer_index;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.WhileThunkProto.condition_result_buffer_index)
}

// .xla.gpu.SequentialThunkProto condition_thunk_sequence = 2;
inline bool WhileThunkProto::_internal_has_condition_thunk_sequence() const {
  return this != internal_default_instance() && _impl_.condition_thunk_sequence_ != nullptr;
}
inline bool WhileThunkProto::has_condition_thunk_sequence() const {
  return _internal_has_condition_thunk_sequence();
}
inline void WhileThunkProto::clear_condition_thunk_sequence() {
  if (GetArenaForAllocation() == nullptr && _impl_.condition_thunk_sequence_ != nullptr) {
    delete _impl_.condition_thunk_sequence_;
  }
  _impl_.condition_thunk_sequence_ = nullptr;
}
inline const ::xla::gpu::SequentialThunkProto& WhileThunkProto::_internal_condition_thunk_sequence() const {
  const ::xla::gpu::SequentialThunkProto* p = _impl_.condition_thunk_sequence_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::SequentialThunkProto&>(
      ::xla::gpu::_SequentialThunkProto_default_instance_);
}
inline const ::xla::gpu::SequentialThunkProto& WhileThunkProto::condition_thunk_sequence() const {
  // @@protoc_insertion_point(field_get:xla.gpu.WhileThunkProto.condition_thunk_sequence)
  return _internal_condition_thunk_sequence();
}
inline void WhileThunkProto::unsafe_arena_set_allocated_condition_thunk_sequence(
    ::xla::gpu::SequentialThunkProto* condition_thunk_sequence) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.condition_thunk_sequence_);
  }
  _impl_.condition_thunk_sequence_ = condition_thunk_sequence;
  if (condition_thunk_sequence) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.WhileThunkProto.condition_thunk_sequence)
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::release_condition_thunk_sequence() {
  
  ::xla::gpu::SequentialThunkProto* temp = _impl_.condition_thunk_sequence_;
  _impl_.condition_thunk_sequence_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::unsafe_arena_release_condition_thunk_sequence() {
  // @@protoc_insertion_point(field_release:xla.gpu.WhileThunkProto.condition_thunk_sequence)
  
  ::xla::gpu::SequentialThunkProto* temp = _impl_.condition_thunk_sequence_;
  _impl_.condition_thunk_sequence_ = nullptr;
  return temp;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::_internal_mutable_condition_thunk_sequence() {
  
  if (_impl_.condition_thunk_sequence_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::SequentialThunkProto>(GetArenaForAllocation());
    _impl_.condition_thunk_sequence_ = p;
  }
  return _impl_.condition_thunk_sequence_;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::mutable_condition_thunk_sequence() {
  ::xla::gpu::SequentialThunkProto* _msg = _internal_mutable_condition_thunk_sequence();
  // @@protoc_insertion_point(field_mutable:xla.gpu.WhileThunkProto.condition_thunk_sequence)
  return _msg;
}
inline void WhileThunkProto::set_allocated_condition_thunk_sequence(::xla::gpu::SequentialThunkProto* condition_thunk_sequence) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.condition_thunk_sequence_;
  }
  if (condition_thunk_sequence) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(condition_thunk_sequence);
    if (message_arena != submessage_arena) {
      condition_thunk_sequence = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, condition_thunk_sequence, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.condition_thunk_sequence_ = condition_thunk_sequence;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.WhileThunkProto.condition_thunk_sequence)
}

// .xla.gpu.SequentialThunkProto body_thunk_sequence = 3;
inline bool WhileThunkProto::_internal_has_body_thunk_sequence() const {
  return this != internal_default_instance() && _impl_.body_thunk_sequence_ != nullptr;
}
inline bool WhileThunkProto::has_body_thunk_sequence() const {
  return _internal_has_body_thunk_sequence();
}
inline void WhileThunkProto::clear_body_thunk_sequence() {
  if (GetArenaForAllocation() == nullptr && _impl_.body_thunk_sequence_ != nullptr) {
    delete _impl_.body_thunk_sequence_;
  }
  _impl_.body_thunk_sequence_ = nullptr;
}
inline const ::xla::gpu::SequentialThunkProto& WhileThunkProto::_internal_body_thunk_sequence() const {
  const ::xla::gpu::SequentialThunkProto* p = _impl_.body_thunk_sequence_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::SequentialThunkProto&>(
      ::xla::gpu::_SequentialThunkProto_default_instance_);
}
inline const ::xla::gpu::SequentialThunkProto& WhileThunkProto::body_thunk_sequence() const {
  // @@protoc_insertion_point(field_get:xla.gpu.WhileThunkProto.body_thunk_sequence)
  return _internal_body_thunk_sequence();
}
inline void WhileThunkProto::unsafe_arena_set_allocated_body_thunk_sequence(
    ::xla::gpu::SequentialThunkProto* body_thunk_sequence) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.body_thunk_sequence_);
  }
  _impl_.body_thunk_sequence_ = body_thunk_sequence;
  if (body_thunk_sequence) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.WhileThunkProto.body_thunk_sequence)
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::release_body_thunk_sequence() {
  
  ::xla::gpu::SequentialThunkProto* temp = _impl_.body_thunk_sequence_;
  _impl_.body_thunk_sequence_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::unsafe_arena_release_body_thunk_sequence() {
  // @@protoc_insertion_point(field_release:xla.gpu.WhileThunkProto.body_thunk_sequence)
  
  ::xla::gpu::SequentialThunkProto* temp = _impl_.body_thunk_sequence_;
  _impl_.body_thunk_sequence_ = nullptr;
  return temp;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::_internal_mutable_body_thunk_sequence() {
  
  if (_impl_.body_thunk_sequence_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::SequentialThunkProto>(GetArenaForAllocation());
    _impl_.body_thunk_sequence_ = p;
  }
  return _impl_.body_thunk_sequence_;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::mutable_body_thunk_sequence() {
  ::xla::gpu::SequentialThunkProto* _msg = _internal_mutable_body_thunk_sequence();
  // @@protoc_insertion_point(field_mutable:xla.gpu.WhileThunkProto.body_thunk_sequence)
  return _msg;
}
inline void WhileThunkProto::set_allocated_body_thunk_sequence(::xla::gpu::SequentialThunkProto* body_thunk_sequence) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.body_thunk_sequence_;
  }
  if (body_thunk_sequence) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(body_thunk_sequence);
    if (message_arena != submessage_arena) {
      body_thunk_sequence = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, body_thunk_sequence, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.body_thunk_sequence_ = body_thunk_sequence;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.WhileThunkProto.body_thunk_sequence)
}

// optional int64 trip_count = 4;
inline bool WhileThunkProto::_internal_has_trip_count() const {
  bool value = (_impl_._has_bits_[0] & 0x00000001u) != 0;
  return value;
}
inline bool WhileThunkProto::has_trip_count() const {
  return _internal_has_trip_count();
}
inline void WhileThunkProto::clear_trip_count() {
  _impl_.trip_count_ = int64_t{0};
  _impl_._has_bits_[0] &= ~0x00000001u;
}
inline int64_t WhileThunkProto::_internal_trip_count() const {
  return _impl_.trip_count_;
}
inline int64_t WhileThunkProto::trip_count() const {
  // @@protoc_insertion_point(field_get:xla.gpu.WhileThunkProto.trip_count)
  return _internal_trip_count();
}
inline void WhileThunkProto::_internal_set_trip_count(int64_t value) {
  _impl_._has_bits_[0] |= 0x00000001u;
  _impl_.trip_count_ = value;
}
inline void WhileThunkProto::set_trip_count(int64_t value) {
  _internal_set_trip_count(value);
  // @@protoc_insertion_point(field_set:xla.gpu.WhileThunkProto.trip_count)
}

// -------------------------------------------------------------------

// Dim3DProto

// int64 x = 1;
inline void Dim3DProto::clear_x() {
  _impl_.x_ = int64_t{0};
}
inline int64_t Dim3DProto::_internal_x() const {
  return _impl_.x_;
}
inline int64_t Dim3DProto::x() const {
  // @@protoc_insertion_point(field_get:xla.gpu.Dim3DProto.x)
  return _internal_x();
}
inline void Dim3DProto::_internal_set_x(int64_t value) {
  
  _impl_.x_ = value;
}
inline void Dim3DProto::set_x(int64_t value) {
  _internal_set_x(value);
  // @@protoc_insertion_point(field_set:xla.gpu.Dim3DProto.x)
}

// int64 y = 2;
inline void Dim3DProto::clear_y() {
  _impl_.y_ = int64_t{0};
}
inline int64_t Dim3DProto::_internal_y() const {
  return _impl_.y_;
}
inline int64_t Dim3DProto::y() const {
  // @@protoc_insertion_point(field_get:xla.gpu.Dim3DProto.y)
  return _internal_y();
}
inline void Dim3DProto::_internal_set_y(int64_t value) {
  
  _impl_.y_ = value;
}
inline void Dim3DProto::set_y(int64_t value) {
  _internal_set_y(value);
  // @@protoc_insertion_point(field_set:xla.gpu.Dim3DProto.y)
}

// int64 z = 3;
inline void Dim3DProto::clear_z() {
  _impl_.z_ = int64_t{0};
}
inline int64_t Dim3DProto::_internal_z() const {
  return _impl_.z_;
}
inline int64_t Dim3DProto::z() const {
  // @@protoc_insertion_point(field_get:xla.gpu.Dim3DProto.z)
  return _internal_z();
}
inline void Dim3DProto::_internal_set_z(int64_t value) {
  
  _impl_.z_ = value;
}
inline void Dim3DProto::set_z(int64_t value) {
  _internal_set_z(value);
  // @@protoc_insertion_point(field_set:xla.gpu.Dim3DProto.z)
}

// -------------------------------------------------------------------

// KernelThunkProto

// repeated .xla.buffer_assignment.BufferAllocationSliceProto args = 1;
inline int KernelThunkProto::_internal_args_size() const {
  return _impl_.args_.size();
}
inline int KernelThunkProto::args_size() const {
  return _internal_args_size();
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* KernelThunkProto::mutable_args(int index) {
  // @@protoc_insertion_point(field_mutable:xla.gpu.KernelThunkProto.args)
  return _impl_.args_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::buffer_assignment::BufferAllocationSliceProto >*
KernelThunkProto::mutable_args() {
  // @@protoc_insertion_point(field_mutable_list:xla.gpu.KernelThunkProto.args)
  return &_impl_.args_;
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& KernelThunkProto::_internal_args(int index) const {
  return _impl_.args_.Get(index);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& KernelThunkProto::args(int index) const {
  // @@protoc_insertion_point(field_get:xla.gpu.KernelThunkProto.args)
  return _internal_args(index);
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* KernelThunkProto::_internal_add_args() {
  return _impl_.args_.Add();
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* KernelThunkProto::add_args() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _add = _internal_add_args();
  // @@protoc_insertion_point(field_add:xla.gpu.KernelThunkProto.args)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::buffer_assignment::BufferAllocationSliceProto >&
KernelThunkProto::args() const {
  // @@protoc_insertion_point(field_list:xla.gpu.KernelThunkProto.args)
  return _impl_.args_;
}

// repeated bool written = 2;
inline int KernelThunkProto::_internal_written_size() const {
  return _impl_.written_.size();
}
inline int KernelThunkProto::written_size() const {
  return _internal_written_size();
}
inline void KernelThunkProto::clear_written() {
  _impl_.written_.Clear();
}
inline bool KernelThunkProto::_internal_written(int index) const {
  return _impl_.written_.Get(index);
}
inline bool KernelThunkProto::written(int index) const {
  // @@protoc_insertion_point(field_get:xla.gpu.KernelThunkProto.written)
  return _internal_written(index);
}
inline void KernelThunkProto::set_written(int index, bool value) {
  _impl_.written_.Set(index, value);
  // @@protoc_insertion_point(field_set:xla.gpu.KernelThunkProto.written)
}
inline void KernelThunkProto::_internal_add_written(bool value) {
  _impl_.written_.Add(value);
}
inline void KernelThunkProto::add_written(bool value) {
  _internal_add_written(value);
  // @@protoc_insertion_point(field_add:xla.gpu.KernelThunkProto.written)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
KernelThunkProto::_internal_written() const {
  return _impl_.written_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >&
KernelThunkProto::written() const {
  // @@protoc_insertion_point(field_list:xla.gpu.KernelThunkProto.written)
  return _internal_written();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
KernelThunkProto::_internal_mutable_written() {
  return &_impl_.written_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< bool >*
KernelThunkProto::mutable_written() {
  // @@protoc_insertion_point(field_mutable_list:xla.gpu.KernelThunkProto.written)
  return _internal_mutable_written();
}

// string kernel_name = 3;
inline void KernelThunkProto::clear_kernel_name() {
  _impl_.kernel_name_.ClearToEmpty();
}
inline const std::string& KernelThunkProto::kernel_name() const {
  // @@protoc_insertion_point(field_get:xla.gpu.KernelThunkProto.kernel_name)
  return _internal_kernel_name();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void KernelThunkProto::set_kernel_name(ArgT0&& arg0, ArgT... args) {
 
 _impl_.kernel_name_.Set(static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:xla.gpu.KernelThunkProto.kernel_name)
}
inline std::string* KernelThunkProto::mutable_kernel_name() {
  std::string* _s = _internal_mutable_kernel_name();
  // @@protoc_insertion_point(field_mutable:xla.gpu.KernelThunkProto.kernel_name)
  return _s;
}
inline const std::string& KernelThunkProto::_internal_kernel_name() const {
  return _impl_.kernel_name_.Get();
}
inline void KernelThunkProto::_internal_set_kernel_name(const std::string& value) {
  
  _impl_.kernel_name_.Set(value, GetArenaForAllocation());
}
inline std::string* KernelThunkProto::_internal_mutable_kernel_name() {
  
  return _impl_.kernel_name_.Mutable(GetArenaForAllocation());
}
inline std::string* KernelThunkProto::release_kernel_name() {
  // @@protoc_insertion_point(field_release:xla.gpu.KernelThunkProto.kernel_name)
  return _impl_.kernel_name_.Release();
}
inline void KernelThunkProto::set_allocated_kernel_name(std::string* kernel_name) {
  if (kernel_name != nullptr) {
    
  } else {
    
  }
  _impl_.kernel_name_.SetAllocated(kernel_name, GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (_impl_.kernel_name_.IsDefault()) {
    _impl_.kernel_name_.Set("", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.KernelThunkProto.kernel_name)
}

// .xla.gpu.Dim3DProto launch_block_counts = 4;
inline bool KernelThunkProto::_internal_has_launch_block_counts() const {
  return this != internal_default_instance() && _impl_.launch_block_counts_ != nullptr;
}
inline bool KernelThunkProto::has_launch_block_counts() const {
  return _internal_has_launch_block_counts();
}
inline void KernelThunkProto::clear_launch_block_counts() {
  if (GetArenaForAllocation() == nullptr && _impl_.launch_block_counts_ != nullptr) {
    delete _impl_.launch_block_counts_;
  }
  _impl_.launch_block_counts_ = nullptr;
}
inline const ::xla::gpu::Dim3DProto& KernelThunkProto::_internal_launch_block_counts() const {
  const ::xla::gpu::Dim3DProto* p = _impl_.launch_block_counts_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::Dim3DProto&>(
      ::xla::gpu::_Dim3DProto_default_instance_);
}
inline const ::xla::gpu::Dim3DProto& KernelThunkProto::launch_block_counts() const {
  // @@protoc_insertion_point(field_get:xla.gpu.KernelThunkProto.launch_block_counts)
  return _internal_launch_block_counts();
}
inline void KernelThunkProto::unsafe_arena_set_allocated_launch_block_counts(
    ::xla::gpu::Dim3DProto* launch_block_counts) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.launch_block_counts_);
  }
  _impl_.launch_block_counts_ = launch_block_counts;
  if (launch_block_counts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.KernelThunkProto.launch_block_counts)
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::release_launch_block_counts() {
  
  ::xla::gpu::Dim3DProto* temp = _impl_.launch_block_counts_;
  _impl_.launch_block_counts_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::unsafe_arena_release_launch_block_counts() {
  // @@protoc_insertion_point(field_release:xla.gpu.KernelThunkProto.launch_block_counts)
  
  ::xla::gpu::Dim3DProto* temp = _impl_.launch_block_counts_;
  _impl_.launch_block_counts_ = nullptr;
  return temp;
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::_internal_mutable_launch_block_counts() {
  
  if (_impl_.launch_block_counts_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::Dim3DProto>(GetArenaForAllocation());
    _impl_.launch_block_counts_ = p;
  }
  return _impl_.launch_block_counts_;
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::mutable_launch_block_counts() {
  ::xla::gpu::Dim3DProto* _msg = _internal_mutable_launch_block_counts();
  // @@protoc_insertion_point(field_mutable:xla.gpu.KernelThunkProto.launch_block_counts)
  return _msg;
}
inline void KernelThunkProto::set_allocated_launch_block_counts(::xla::gpu::Dim3DProto* launch_block_counts) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.launch_block_counts_;
  }
  if (launch_block_counts) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(launch_block_counts);
    if (message_arena != submessage_arena) {
      launch_block_counts = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, launch_block_counts, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.launch_block_counts_ = launch_block_counts;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.KernelThunkProto.launch_block_counts)
}

// .xla.gpu.Dim3DProto launch_thread_counts_per_block = 5;
inline bool KernelThunkProto::_internal_has_launch_thread_counts_per_block() const {
  return this != internal_default_instance() && _impl_.launch_thread_counts_per_block_ != nullptr;
}
inline bool KernelThunkProto::has_launch_thread_counts_per_block() const {
  return _internal_has_launch_thread_counts_per_block();
}
inline void KernelThunkProto::clear_launch_thread_counts_per_block() {
  if (GetArenaForAllocation() == nullptr && _impl_.launch_thread_counts_per_block_ != nullptr) {
    delete _impl_.launch_thread_counts_per_block_;
  }
  _impl_.launch_thread_counts_per_block_ = nullptr;
}
inline const ::xla::gpu::Dim3DProto& KernelThunkProto::_internal_launch_thread_counts_per_block() const {
  const ::xla::gpu::Dim3DProto* p = _impl_.launch_thread_counts_per_block_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::Dim3DProto&>(
      ::xla::gpu::_Dim3DProto_default_instance_);
}
inline const ::xla::gpu::Dim3DProto& KernelThunkProto::launch_thread_counts_per_block() const {
  // @@protoc_insertion_point(field_get:xla.gpu.KernelThunkProto.launch_thread_counts_per_block)
  return _internal_launch_thread_counts_per_block();
}
inline void KernelThunkProto::unsafe_arena_set_allocated_launch_thread_counts_per_block(
    ::xla::gpu::Dim3DProto* launch_thread_counts_per_block) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.launch_thread_counts_per_block_);
  }
  _impl_.launch_thread_counts_per_block_ = launch_thread_counts_per_block;
  if (launch_thread_counts_per_block) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.KernelThunkProto.launch_thread_counts_per_block)
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::release_launch_thread_counts_per_block() {
  
  ::xla::gpu::Dim3DProto* temp = _impl_.launch_thread_counts_per_block_;
  _impl_.launch_thread_counts_per_block_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::unsafe_arena_release_launch_thread_counts_per_block() {
  // @@protoc_insertion_point(field_release:xla.gpu.KernelThunkProto.launch_thread_counts_per_block)
  
  ::xla::gpu::Dim3DProto* temp = _impl_.launch_thread_counts_per_block_;
  _impl_.launch_thread_counts_per_block_ = nullptr;
  return temp;
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::_internal_mutable_launch_thread_counts_per_block() {
  
  if (_impl_.launch_thread_counts_per_block_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::Dim3DProto>(GetArenaForAllocation());
    _impl_.launch_thread_counts_per_block_ = p;
  }
  return _impl_.launch_thread_counts_per_block_;
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::mutable_launch_thread_counts_per_block() {
  ::xla::gpu::Dim3DProto* _msg = _internal_mutable_launch_thread_counts_per_block();
  // @@protoc_insertion_point(field_mutable:xla.gpu.KernelThunkProto.launch_thread_counts_per_block)
  return _msg;
}
inline void KernelThunkProto::set_allocated_launch_thread_counts_per_block(::xla::gpu::Dim3DProto* launch_thread_counts_per_block) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.launch_thread_counts_per_block_;
  }
  if (launch_thread_counts_per_block) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(launch_thread_counts_per_block);
    if (message_arena != submessage_arena) {
      launch_thread_counts_per_block = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, launch_thread_counts_per_block, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.launch_thread_counts_per_block_ = launch_thread_counts_per_block;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.KernelThunkProto.launch_thread_counts_per_block)
}

// optional .xla.gpu.Dim3DProto cluster_dim = 6;
inline bool KernelThunkProto::_internal_has_cluster_dim() const {
  bool value = (_impl_._has_bits_[0] & 0x00000001u) != 0;
  PROTOBUF_ASSUME(!value || _impl_.cluster_dim_ != nullptr);
  return value;
}
inline bool KernelThunkProto::has_cluster_dim() const {
  return _internal_has_cluster_dim();
}
inline void KernelThunkProto::clear_cluster_dim() {
  if (_impl_.cluster_dim_ != nullptr) _impl_.cluster_dim_->Clear();
  _impl_._has_bits_[0] &= ~0x00000001u;
}
inline const ::xla::gpu::Dim3DProto& KernelThunkProto::_internal_cluster_dim() const {
  const ::xla::gpu::Dim3DProto* p = _impl_.cluster_dim_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::Dim3DProto&>(
      ::xla::gpu::_Dim3DProto_default_instance_);
}
inline const ::xla::gpu::Dim3DProto& KernelThunkProto::cluster_dim() const {
  // @@protoc_insertion_point(field_get:xla.gpu.KernelThunkProto.cluster_dim)
  return _internal_cluster_dim();
}
inline void KernelThunkProto::unsafe_arena_set_allocated_cluster_dim(
    ::xla::gpu::Dim3DProto* cluster_dim) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.cluster_dim_);
  }
  _impl_.cluster_dim_ = cluster_dim;
  if (cluster_dim) {
    _impl_._has_bits_[0] |= 0x00000001u;
  } else {
    _impl_._has_bits_[0] &= ~0x00000001u;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.KernelThunkProto.cluster_dim)
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::release_cluster_dim() {
  _impl_._has_bits_[0] &= ~0x00000001u;
  ::xla::gpu::Dim3DProto* temp = _impl_.cluster_dim_;
  _impl_.cluster_dim_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::unsafe_arena_release_cluster_dim() {
  // @@protoc_insertion_point(field_release:xla.gpu.KernelThunkProto.cluster_dim)
  _impl_._has_bits_[0] &= ~0x00000001u;
  ::xla::gpu::Dim3DProto* temp = _impl_.cluster_dim_;
  _impl_.cluster_dim_ = nullptr;
  return temp;
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::_internal_mutable_cluster_dim() {
  _impl_._has_bits_[0] |= 0x00000001u;
  if (_impl_.cluster_dim_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::Dim3DProto>(GetArenaForAllocation());
    _impl_.cluster_dim_ = p;
  }
  return _impl_.cluster_dim_;
}
inline ::xla::gpu::Dim3DProto* KernelThunkProto::mutable_cluster_dim() {
  ::xla::gpu::Dim3DProto* _msg = _internal_mutable_cluster_dim();
  // @@protoc_insertion_point(field_mutable:xla.gpu.KernelThunkProto.cluster_dim)
  return _msg;
}
inline void KernelThunkProto::set_allocated_cluster_dim(::xla::gpu::Dim3DProto* cluster_dim) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.cluster_dim_;
  }
  if (cluster_dim) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(cluster_dim);
    if (message_arena != submessage_arena) {
      cluster_dim = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, cluster_dim, submessage_arena);
    }
    _impl_._has_bits_[0] |= 0x00000001u;
  } else {
    _impl_._has_bits_[0] &= ~0x00000001u;
  }
  _impl_.cluster_dim_ = cluster_dim;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.KernelThunkProto.cluster_dim)
}

// int64 shmem_bytes = 7;
inline void KernelThunkProto::clear_shmem_bytes() {
  _impl_.shmem_bytes_ = int64_t{0};
}
inline int64_t KernelThunkProto::_internal_shmem_bytes() const {
  return _impl_.shmem_bytes_;
}
inline int64_t KernelThunkProto::shmem_bytes() const {
  // @@protoc_insertion_point(field_get:xla.gpu.KernelThunkProto.shmem_bytes)
  return _internal_shmem_bytes();
}
inline void KernelThunkProto::_internal_set_shmem_bytes(int64_t value) {
  
  _impl_.shmem_bytes_ = value;
}
inline void KernelThunkProto::set_shmem_bytes(int64_t value) {
  _internal_set_shmem_bytes(value);
  // @@protoc_insertion_point(field_set:xla.gpu.KernelThunkProto.shmem_bytes)
}

// -------------------------------------------------------------------

// GemmThunkProto

// .xla.GemmConfigProto gemm_config = 1;
inline bool GemmThunkProto::_internal_has_gemm_config() const {
  return this != internal_default_instance() && _impl_.gemm_config_ != nullptr;
}
inline bool GemmThunkProto::has_gemm_config() const {
  return _internal_has_gemm_config();
}
inline const ::xla::GemmConfigProto& GemmThunkProto::_internal_gemm_config() const {
  const ::xla::GemmConfigProto* p = _impl_.gemm_config_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::GemmConfigProto&>(
      ::xla::_GemmConfigProto_default_instance_);
}
inline const ::xla::GemmConfigProto& GemmThunkProto::gemm_config() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmThunkProto.gemm_config)
  return _internal_gemm_config();
}
inline void GemmThunkProto::unsafe_arena_set_allocated_gemm_config(
    ::xla::GemmConfigProto* gemm_config) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.gemm_config_);
  }
  _impl_.gemm_config_ = gemm_config;
  if (gemm_config) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.GemmThunkProto.gemm_config)
}
inline ::xla::GemmConfigProto* GemmThunkProto::release_gemm_config() {
  
  ::xla::GemmConfigProto* temp = _impl_.gemm_config_;
  _impl_.gemm_config_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::GemmConfigProto* GemmThunkProto::unsafe_arena_release_gemm_config() {
  // @@protoc_insertion_point(field_release:xla.gpu.GemmThunkProto.gemm_config)
  
  ::xla::GemmConfigProto* temp = _impl_.gemm_config_;
  _impl_.gemm_config_ = nullptr;
  return temp;
}
inline ::xla::GemmConfigProto* GemmThunkProto::_internal_mutable_gemm_config() {
  
  if (_impl_.gemm_config_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::GemmConfigProto>(GetArenaForAllocation());
    _impl_.gemm_config_ = p;
  }
  return _impl_.gemm_config_;
}
inline ::xla::GemmConfigProto* GemmThunkProto::mutable_gemm_config() {
  ::xla::GemmConfigProto* _msg = _internal_mutable_gemm_config();
  // @@protoc_insertion_point(field_mutable:xla.gpu.GemmThunkProto.gemm_config)
  return _msg;
}
inline void GemmThunkProto::set_allocated_gemm_config(::xla::GemmConfigProto* gemm_config) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.gemm_config_);
  }
  if (gemm_config) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(gemm_config));
    if (message_arena != submessage_arena) {
      gemm_config = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, gemm_config, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.gemm_config_ = gemm_config;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.GemmThunkProto.gemm_config)
}

// .xla.buffer_assignment.BufferAllocationSliceProto lhs_buffer = 2;
inline bool GemmThunkProto::_internal_has_lhs_buffer() const {
  return this != internal_default_instance() && _impl_.lhs_buffer_ != nullptr;
}
inline bool GemmThunkProto::has_lhs_buffer() const {
  return _internal_has_lhs_buffer();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& GemmThunkProto::_internal_lhs_buffer() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.lhs_buffer_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& GemmThunkProto::lhs_buffer() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmThunkProto.lhs_buffer)
  return _internal_lhs_buffer();
}
inline void GemmThunkProto::unsafe_arena_set_allocated_lhs_buffer(
    ::xla::buffer_assignment::BufferAllocationSliceProto* lhs_buffer) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.lhs_buffer_);
  }
  _impl_.lhs_buffer_ = lhs_buffer;
  if (lhs_buffer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.GemmThunkProto.lhs_buffer)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::release_lhs_buffer() {
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.lhs_buffer_;
  _impl_.lhs_buffer_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::unsafe_arena_release_lhs_buffer() {
  // @@protoc_insertion_point(field_release:xla.gpu.GemmThunkProto.lhs_buffer)
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.lhs_buffer_;
  _impl_.lhs_buffer_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::_internal_mutable_lhs_buffer() {
  
  if (_impl_.lhs_buffer_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.lhs_buffer_ = p;
  }
  return _impl_.lhs_buffer_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::mutable_lhs_buffer() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_lhs_buffer();
  // @@protoc_insertion_point(field_mutable:xla.gpu.GemmThunkProto.lhs_buffer)
  return _msg;
}
inline void GemmThunkProto::set_allocated_lhs_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* lhs_buffer) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.lhs_buffer_);
  }
  if (lhs_buffer) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(lhs_buffer));
    if (message_arena != submessage_arena) {
      lhs_buffer = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, lhs_buffer, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.lhs_buffer_ = lhs_buffer;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.GemmThunkProto.lhs_buffer)
}

// .xla.buffer_assignment.BufferAllocationSliceProto rhs_buffer = 3;
inline bool GemmThunkProto::_internal_has_rhs_buffer() const {
  return this != internal_default_instance() && _impl_.rhs_buffer_ != nullptr;
}
inline bool GemmThunkProto::has_rhs_buffer() const {
  return _internal_has_rhs_buffer();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& GemmThunkProto::_internal_rhs_buffer() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.rhs_buffer_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& GemmThunkProto::rhs_buffer() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmThunkProto.rhs_buffer)
  return _internal_rhs_buffer();
}
inline void GemmThunkProto::unsafe_arena_set_allocated_rhs_buffer(
    ::xla::buffer_assignment::BufferAllocationSliceProto* rhs_buffer) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.rhs_buffer_);
  }
  _impl_.rhs_buffer_ = rhs_buffer;
  if (rhs_buffer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.GemmThunkProto.rhs_buffer)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::release_rhs_buffer() {
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.rhs_buffer_;
  _impl_.rhs_buffer_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::unsafe_arena_release_rhs_buffer() {
  // @@protoc_insertion_point(field_release:xla.gpu.GemmThunkProto.rhs_buffer)
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.rhs_buffer_;
  _impl_.rhs_buffer_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::_internal_mutable_rhs_buffer() {
  
  if (_impl_.rhs_buffer_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.rhs_buffer_ = p;
  }
  return _impl_.rhs_buffer_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::mutable_rhs_buffer() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_rhs_buffer();
  // @@protoc_insertion_point(field_mutable:xla.gpu.GemmThunkProto.rhs_buffer)
  return _msg;
}
inline void GemmThunkProto::set_allocated_rhs_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* rhs_buffer) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.rhs_buffer_);
  }
  if (rhs_buffer) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(rhs_buffer));
    if (message_arena != submessage_arena) {
      rhs_buffer = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, rhs_buffer, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.rhs_buffer_ = rhs_buffer;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.GemmThunkProto.rhs_buffer)
}

// .xla.buffer_assignment.BufferAllocationSliceProto output_buffer = 4;
inline bool GemmThunkProto::_internal_has_output_buffer() const {
  return this != internal_default_instance() && _impl_.output_buffer_ != nullptr;
}
inline bool GemmThunkProto::has_output_buffer() const {
  return _internal_has_output_buffer();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& GemmThunkProto::_internal_output_buffer() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.output_buffer_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& GemmThunkProto::output_buffer() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmThunkProto.output_buffer)
  return _internal_output_buffer();
}
inline void GemmThunkProto::unsafe_arena_set_allocated_output_buffer(
    ::xla::buffer_assignment::BufferAllocationSliceProto* output_buffer) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.output_buffer_);
  }
  _impl_.output_buffer_ = output_buffer;
  if (output_buffer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.GemmThunkProto.output_buffer)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::release_output_buffer() {
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.output_buffer_;
  _impl_.output_buffer_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::unsafe_arena_release_output_buffer() {
  // @@protoc_insertion_point(field_release:xla.gpu.GemmThunkProto.output_buffer)
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.output_buffer_;
  _impl_.output_buffer_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::_internal_mutable_output_buffer() {
  
  if (_impl_.output_buffer_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.output_buffer_ = p;
  }
  return _impl_.output_buffer_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::mutable_output_buffer() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_output_buffer();
  // @@protoc_insertion_point(field_mutable:xla.gpu.GemmThunkProto.output_buffer)
  return _msg;
}
inline void GemmThunkProto::set_allocated_output_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* output_buffer) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.output_buffer_);
  }
  if (output_buffer) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(output_buffer));
    if (message_arena != submessage_arena) {
      output_buffer = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, output_buffer, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.output_buffer_ = output_buffer;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.GemmThunkProto.output_buffer)
}

// optional .xla.buffer_assignment.BufferAllocationSliceProto workspace = 5;
inline bool GemmThunkProto::_internal_has_workspace() const {
  bool value = (_impl_._has_bits_[0] & 0x00000001u) != 0;
  PROTOBUF_ASSUME(!value || _impl_.workspace_ != nullptr);
  return value;
}
inline bool GemmThunkProto::has_workspace() const {
  return _internal_has_workspace();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& GemmThunkProto::_internal_workspace() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.workspace_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& GemmThunkProto::workspace() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmThunkProto.workspace)
  return _internal_workspace();
}
inline void GemmThunkProto::unsafe_arena_set_allocated_workspace(
    ::xla::buffer_assignment::BufferAllocationSliceProto* workspace) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.workspace_);
  }
  _impl_.workspace_ = workspace;
  if (workspace) {
    _impl_._has_bits_[0] |= 0x00000001u;
  } else {
    _impl_._has_bits_[0] &= ~0x00000001u;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.GemmThunkProto.workspace)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::release_workspace() {
  _impl_._has_bits_[0] &= ~0x00000001u;
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.workspace_;
  _impl_.workspace_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::unsafe_arena_release_workspace() {
  // @@protoc_insertion_point(field_release:xla.gpu.GemmThunkProto.workspace)
  _impl_._has_bits_[0] &= ~0x00000001u;
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.workspace_;
  _impl_.workspace_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::_internal_mutable_workspace() {
  _impl_._has_bits_[0] |= 0x00000001u;
  if (_impl_.workspace_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.workspace_ = p;
  }
  return _impl_.workspace_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* GemmThunkProto::mutable_workspace() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_workspace();
  // @@protoc_insertion_point(field_mutable:xla.gpu.GemmThunkProto.workspace)
  return _msg;
}
inline void GemmThunkProto::set_allocated_workspace(::xla::buffer_assignment::BufferAllocationSliceProto* workspace) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.workspace_);
  }
  if (workspace) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(workspace));
    if (message_arena != submessage_arena) {
      workspace = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, workspace, submessage_arena);
    }
    _impl_._has_bits_[0] |= 0x00000001u;
  } else {
    _impl_._has_bits_[0] &= ~0x00000001u;
  }
  _impl_.workspace_ = workspace;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.GemmThunkProto.workspace)
}

// bool deterministic = 6;
inline void GemmThunkProto::clear_deterministic() {
  _impl_.deterministic_ = false;
}
inline bool GemmThunkProto::_internal_deterministic() const {
  return _impl_.deterministic_;
}
inline bool GemmThunkProto::deterministic() const {
  // @@protoc_insertion_point(field_get:xla.gpu.GemmThunkProto.deterministic)
  return _internal_deterministic();
}
inline void GemmThunkProto::_internal_set_deterministic(bool value) {
  
  _impl_.deterministic_ = value;
}
inline void GemmThunkProto::set_deterministic(bool value) {
  _internal_set_deterministic(value);
  // @@protoc_insertion_point(field_set:xla.gpu.GemmThunkProto.deterministic)
}

// -------------------------------------------------------------------

// ThunkProto

// .xla.gpu.ThunkInfoProto thunk_info = 1;
inline bool ThunkProto::_internal_has_thunk_info() const {
  return this != internal_default_instance() && _impl_.thunk_info_ != nullptr;
}
inline bool ThunkProto::has_thunk_info() const {
  return _internal_has_thunk_info();
}
inline void ThunkProto::clear_thunk_info() {
  if (GetArenaForAllocation() == nullptr && _impl_.thunk_info_ != nullptr) {
    delete _impl_.thunk_info_;
  }
  _impl_.thunk_info_ = nullptr;
}
inline const ::xla::gpu::ThunkInfoProto& ThunkProto::_internal_thunk_info() const {
  const ::xla::gpu::ThunkInfoProto* p = _impl_.thunk_info_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::ThunkInfoProto&>(
      ::xla::gpu::_ThunkInfoProto_default_instance_);
}
inline const ::xla::gpu::ThunkInfoProto& ThunkProto::thunk_info() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.thunk_info)
  return _internal_thunk_info();
}
inline void ThunkProto::unsafe_arena_set_allocated_thunk_info(
    ::xla::gpu::ThunkInfoProto* thunk_info) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.thunk_info_);
  }
  _impl_.thunk_info_ = thunk_info;
  if (thunk_info) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.thunk_info)
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::release_thunk_info() {
  
  ::xla::gpu::ThunkInfoProto* temp = _impl_.thunk_info_;
  _impl_.thunk_info_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::unsafe_arena_release_thunk_info() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.thunk_info)
  
  ::xla::gpu::ThunkInfoProto* temp = _impl_.thunk_info_;
  _impl_.thunk_info_ = nullptr;
  return temp;
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::_internal_mutable_thunk_info() {
  
  if (_impl_.thunk_info_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::ThunkInfoProto>(GetArenaForAllocation());
    _impl_.thunk_info_ = p;
  }
  return _impl_.thunk_info_;
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::mutable_thunk_info() {
  ::xla::gpu::ThunkInfoProto* _msg = _internal_mutable_thunk_info();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.thunk_info)
  return _msg;
}
inline void ThunkProto::set_allocated_thunk_info(::xla::gpu::ThunkInfoProto* thunk_info) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.thunk_info_;
  }
  if (thunk_info) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(thunk_info);
    if (message_arena != submessage_arena) {
      thunk_info = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, thunk_info, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.thunk_info_ = thunk_info;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.ThunkProto.thunk_info)
}

// .xla.gpu.SequentialThunkProto sequential_thunk = 2;
inline bool ThunkProto::_internal_has_sequential_thunk() const {
  return impl_case() == kSequentialThunk;
}
inline bool ThunkProto::has_sequential_thunk() const {
  return _internal_has_sequential_thunk();
}
inline void ThunkProto::set_has_sequential_thunk() {
  _impl_._oneof_case_[0] = kSequentialThunk;
}
inline void ThunkProto::clear_sequential_thunk() {
  if (_internal_has_sequential_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.sequential_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::release_sequential_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.sequential_thunk)
  if (_internal_has_sequential_thunk()) {
    clear_has_impl();
    ::xla::gpu::SequentialThunkProto* temp = _impl_.impl_.sequential_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.sequential_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::SequentialThunkProto& ThunkProto::_internal_sequential_thunk() const {
  return _internal_has_sequential_thunk()
      ? *_impl_.impl_.sequential_thunk_
      : reinterpret_cast< ::xla::gpu::SequentialThunkProto&>(::xla::gpu::_SequentialThunkProto_default_instance_);
}
inline const ::xla::gpu::SequentialThunkProto& ThunkProto::sequential_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.sequential_thunk)
  return _internal_sequential_thunk();
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::unsafe_arena_release_sequential_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.sequential_thunk)
  if (_internal_has_sequential_thunk()) {
    clear_has_impl();
    ::xla::gpu::SequentialThunkProto* temp = _impl_.impl_.sequential_thunk_;
    _impl_.impl_.sequential_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_sequential_thunk(::xla::gpu::SequentialThunkProto* sequential_thunk) {
  clear_impl();
  if (sequential_thunk) {
    set_has_sequential_thunk();
    _impl_.impl_.sequential_thunk_ = sequential_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.sequential_thunk)
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::_internal_mutable_sequential_thunk() {
  if (!_internal_has_sequential_thunk()) {
    clear_impl();
    set_has_sequential_thunk();
    _impl_.impl_.sequential_thunk_ = CreateMaybeMessage< ::xla::gpu::SequentialThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.sequential_thunk_;
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::mutable_sequential_thunk() {
  ::xla::gpu::SequentialThunkProto* _msg = _internal_mutable_sequential_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.sequential_thunk)
  return _msg;
}

// .xla.gpu.CopyThunkProto copy_thunk = 3;
inline bool ThunkProto::_internal_has_copy_thunk() const {
  return impl_case() == kCopyThunk;
}
inline bool ThunkProto::has_copy_thunk() const {
  return _internal_has_copy_thunk();
}
inline void ThunkProto::set_has_copy_thunk() {
  _impl_._oneof_case_[0] = kCopyThunk;
}
inline void ThunkProto::clear_copy_thunk() {
  if (_internal_has_copy_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.copy_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::CopyThunkProto* ThunkProto::release_copy_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.copy_thunk)
  if (_internal_has_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::CopyThunkProto* temp = _impl_.impl_.copy_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::CopyThunkProto& ThunkProto::_internal_copy_thunk() const {
  return _internal_has_copy_thunk()
      ? *_impl_.impl_.copy_thunk_
      : reinterpret_cast< ::xla::gpu::CopyThunkProto&>(::xla::gpu::_CopyThunkProto_default_instance_);
}
inline const ::xla::gpu::CopyThunkProto& ThunkProto::copy_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.copy_thunk)
  return _internal_copy_thunk();
}
inline ::xla::gpu::CopyThunkProto* ThunkProto::unsafe_arena_release_copy_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.copy_thunk)
  if (_internal_has_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::CopyThunkProto* temp = _impl_.impl_.copy_thunk_;
    _impl_.impl_.copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk) {
  clear_impl();
  if (copy_thunk) {
    set_has_copy_thunk();
    _impl_.impl_.copy_thunk_ = copy_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.copy_thunk)
}
inline ::xla::gpu::CopyThunkProto* ThunkProto::_internal_mutable_copy_thunk() {
  if (!_internal_has_copy_thunk()) {
    clear_impl();
    set_has_copy_thunk();
    _impl_.impl_.copy_thunk_ = CreateMaybeMessage< ::xla::gpu::CopyThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.copy_thunk_;
}
inline ::xla::gpu::CopyThunkProto* ThunkProto::mutable_copy_thunk() {
  ::xla::gpu::CopyThunkProto* _msg = _internal_mutable_copy_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.copy_thunk)
  return _msg;
}

// .xla.gpu.DeviceToHostCopyThunkProto device_to_host_copy_thunk = 4;
inline bool ThunkProto::_internal_has_device_to_host_copy_thunk() const {
  return impl_case() == kDeviceToHostCopyThunk;
}
inline bool ThunkProto::has_device_to_host_copy_thunk() const {
  return _internal_has_device_to_host_copy_thunk();
}
inline void ThunkProto::set_has_device_to_host_copy_thunk() {
  _impl_._oneof_case_[0] = kDeviceToHostCopyThunk;
}
inline void ThunkProto::clear_device_to_host_copy_thunk() {
  if (_internal_has_device_to_host_copy_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.device_to_host_copy_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::DeviceToHostCopyThunkProto* ThunkProto::release_device_to_host_copy_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.device_to_host_copy_thunk)
  if (_internal_has_device_to_host_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::DeviceToHostCopyThunkProto* temp = _impl_.impl_.device_to_host_copy_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.device_to_host_copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::DeviceToHostCopyThunkProto& ThunkProto::_internal_device_to_host_copy_thunk() const {
  return _internal_has_device_to_host_copy_thunk()
      ? *_impl_.impl_.device_to_host_copy_thunk_
      : reinterpret_cast< ::xla::gpu::DeviceToHostCopyThunkProto&>(::xla::gpu::_DeviceToHostCopyThunkProto_default_instance_);
}
inline const ::xla::gpu::DeviceToHostCopyThunkProto& ThunkProto::device_to_host_copy_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.device_to_host_copy_thunk)
  return _internal_device_to_host_copy_thunk();
}
inline ::xla::gpu::DeviceToHostCopyThunkProto* ThunkProto::unsafe_arena_release_device_to_host_copy_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.device_to_host_copy_thunk)
  if (_internal_has_device_to_host_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::DeviceToHostCopyThunkProto* temp = _impl_.impl_.device_to_host_copy_thunk_;
    _impl_.impl_.device_to_host_copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_device_to_host_copy_thunk(::xla::gpu::DeviceToHostCopyThunkProto* device_to_host_copy_thunk) {
  clear_impl();
  if (device_to_host_copy_thunk) {
    set_has_device_to_host_copy_thunk();
    _impl_.impl_.device_to_host_copy_thunk_ = device_to_host_copy_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.device_to_host_copy_thunk)
}
inline ::xla::gpu::DeviceToHostCopyThunkProto* ThunkProto::_internal_mutable_device_to_host_copy_thunk() {
  if (!_internal_has_device_to_host_copy_thunk()) {
    clear_impl();
    set_has_device_to_host_copy_thunk();
    _impl_.impl_.device_to_host_copy_thunk_ = CreateMaybeMessage< ::xla::gpu::DeviceToHostCopyThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.device_to_host_copy_thunk_;
}
inline ::xla::gpu::DeviceToHostCopyThunkProto* ThunkProto::mutable_device_to_host_copy_thunk() {
  ::xla::gpu::DeviceToHostCopyThunkProto* _msg = _internal_mutable_device_to_host_copy_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.device_to_host_copy_thunk)
  return _msg;
}

// .xla.gpu.HostToDeviceCopyThunkProto host_to_device_copy_thunk = 5;
inline bool ThunkProto::_internal_has_host_to_device_copy_thunk() const {
  return impl_case() == kHostToDeviceCopyThunk;
}
inline bool ThunkProto::has_host_to_device_copy_thunk() const {
  return _internal_has_host_to_device_copy_thunk();
}
inline void ThunkProto::set_has_host_to_device_copy_thunk() {
  _impl_._oneof_case_[0] = kHostToDeviceCopyThunk;
}
inline void ThunkProto::clear_host_to_device_copy_thunk() {
  if (_internal_has_host_to_device_copy_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.host_to_device_copy_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::HostToDeviceCopyThunkProto* ThunkProto::release_host_to_device_copy_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.host_to_device_copy_thunk)
  if (_internal_has_host_to_device_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::HostToDeviceCopyThunkProto* temp = _impl_.impl_.host_to_device_copy_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.host_to_device_copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::HostToDeviceCopyThunkProto& ThunkProto::_internal_host_to_device_copy_thunk() const {
  return _internal_has_host_to_device_copy_thunk()
      ? *_impl_.impl_.host_to_device_copy_thunk_
      : reinterpret_cast< ::xla::gpu::HostToDeviceCopyThunkProto&>(::xla::gpu::_HostToDeviceCopyThunkProto_default_instance_);
}
inline const ::xla::gpu::HostToDeviceCopyThunkProto& ThunkProto::host_to_device_copy_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.host_to_device_copy_thunk)
  return _internal_host_to_device_copy_thunk();
}
inline ::xla::gpu::HostToDeviceCopyThunkProto* ThunkProto::unsafe_arena_release_host_to_device_copy_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.host_to_device_copy_thunk)
  if (_internal_has_host_to_device_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::HostToDeviceCopyThunkProto* temp = _impl_.impl_.host_to_device_copy_thunk_;
    _impl_.impl_.host_to_device_copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_host_to_device_copy_thunk(::xla::gpu::HostToDeviceCopyThunkProto* host_to_device_copy_thunk) {
  clear_impl();
  if (host_to_device_copy_thunk) {
    set_has_host_to_device_copy_thunk();
    _impl_.impl_.host_to_device_copy_thunk_ = host_to_device_copy_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.host_to_device_copy_thunk)
}
inline ::xla::gpu::HostToDeviceCopyThunkProto* ThunkProto::_internal_mutable_host_to_device_copy_thunk() {
  if (!_internal_has_host_to_device_copy_thunk()) {
    clear_impl();
    set_has_host_to_device_copy_thunk();
    _impl_.impl_.host_to_device_copy_thunk_ = CreateMaybeMessage< ::xla::gpu::HostToDeviceCopyThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.host_to_device_copy_thunk_;
}
inline ::xla::gpu::HostToDeviceCopyThunkProto* ThunkProto::mutable_host_to_device_copy_thunk() {
  ::xla::gpu::HostToDeviceCopyThunkProto* _msg = _internal_mutable_host_to_device_copy_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.host_to_device_copy_thunk)
  return _msg;
}

// .xla.gpu.ConditionalThunkProto conditional_thunk = 6;
inline bool ThunkProto::_internal_has_conditional_thunk() const {
  return impl_case() == kConditionalThunk;
}
inline bool ThunkProto::has_conditional_thunk() const {
  return _internal_has_conditional_thunk();
}
inline void ThunkProto::set_has_conditional_thunk() {
  _impl_._oneof_case_[0] = kConditionalThunk;
}
inline void ThunkProto::clear_conditional_thunk() {
  if (_internal_has_conditional_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.conditional_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::ConditionalThunkProto* ThunkProto::release_conditional_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.conditional_thunk)
  if (_internal_has_conditional_thunk()) {
    clear_has_impl();
    ::xla::gpu::ConditionalThunkProto* temp = _impl_.impl_.conditional_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.conditional_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::ConditionalThunkProto& ThunkProto::_internal_conditional_thunk() const {
  return _internal_has_conditional_thunk()
      ? *_impl_.impl_.conditional_thunk_
      : reinterpret_cast< ::xla::gpu::ConditionalThunkProto&>(::xla::gpu::_ConditionalThunkProto_default_instance_);
}
inline const ::xla::gpu::ConditionalThunkProto& ThunkProto::conditional_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.conditional_thunk)
  return _internal_conditional_thunk();
}
inline ::xla::gpu::ConditionalThunkProto* ThunkProto::unsafe_arena_release_conditional_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.conditional_thunk)
  if (_internal_has_conditional_thunk()) {
    clear_has_impl();
    ::xla::gpu::ConditionalThunkProto* temp = _impl_.impl_.conditional_thunk_;
    _impl_.impl_.conditional_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_conditional_thunk(::xla::gpu::ConditionalThunkProto* conditional_thunk) {
  clear_impl();
  if (conditional_thunk) {
    set_has_conditional_thunk();
    _impl_.impl_.conditional_thunk_ = conditional_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.conditional_thunk)
}
inline ::xla::gpu::ConditionalThunkProto* ThunkProto::_internal_mutable_conditional_thunk() {
  if (!_internal_has_conditional_thunk()) {
    clear_impl();
    set_has_conditional_thunk();
    _impl_.impl_.conditional_thunk_ = CreateMaybeMessage< ::xla::gpu::ConditionalThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.conditional_thunk_;
}
inline ::xla::gpu::ConditionalThunkProto* ThunkProto::mutable_conditional_thunk() {
  ::xla::gpu::ConditionalThunkProto* _msg = _internal_mutable_conditional_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.conditional_thunk)
  return _msg;
}

// .xla.gpu.WhileThunkProto while_thunk = 7;
inline bool ThunkProto::_internal_has_while_thunk() const {
  return impl_case() == kWhileThunk;
}
inline bool ThunkProto::has_while_thunk() const {
  return _internal_has_while_thunk();
}
inline void ThunkProto::set_has_while_thunk() {
  _impl_._oneof_case_[0] = kWhileThunk;
}
inline void ThunkProto::clear_while_thunk() {
  if (_internal_has_while_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.while_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::WhileThunkProto* ThunkProto::release_while_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.while_thunk)
  if (_internal_has_while_thunk()) {
    clear_has_impl();
    ::xla::gpu::WhileThunkProto* temp = _impl_.impl_.while_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.while_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::WhileThunkProto& ThunkProto::_internal_while_thunk() const {
  return _internal_has_while_thunk()
      ? *_impl_.impl_.while_thunk_
      : reinterpret_cast< ::xla::gpu::WhileThunkProto&>(::xla::gpu::_WhileThunkProto_default_instance_);
}
inline const ::xla::gpu::WhileThunkProto& ThunkProto::while_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.while_thunk)
  return _internal_while_thunk();
}
inline ::xla::gpu::WhileThunkProto* ThunkProto::unsafe_arena_release_while_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.while_thunk)
  if (_internal_has_while_thunk()) {
    clear_has_impl();
    ::xla::gpu::WhileThunkProto* temp = _impl_.impl_.while_thunk_;
    _impl_.impl_.while_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_while_thunk(::xla::gpu::WhileThunkProto* while_thunk) {
  clear_impl();
  if (while_thunk) {
    set_has_while_thunk();
    _impl_.impl_.while_thunk_ = while_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.while_thunk)
}
inline ::xla::gpu::WhileThunkProto* ThunkProto::_internal_mutable_while_thunk() {
  if (!_internal_has_while_thunk()) {
    clear_impl();
    set_has_while_thunk();
    _impl_.impl_.while_thunk_ = CreateMaybeMessage< ::xla::gpu::WhileThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.while_thunk_;
}
inline ::xla::gpu::WhileThunkProto* ThunkProto::mutable_while_thunk() {
  ::xla::gpu::WhileThunkProto* _msg = _internal_mutable_while_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.while_thunk)
  return _msg;
}

// .xla.gpu.KernelThunkProto kernel_thunk = 8;
inline bool ThunkProto::_internal_has_kernel_thunk() const {
  return impl_case() == kKernelThunk;
}
inline bool ThunkProto::has_kernel_thunk() const {
  return _internal_has_kernel_thunk();
}
inline void ThunkProto::set_has_kernel_thunk() {
  _impl_._oneof_case_[0] = kKernelThunk;
}
inline void ThunkProto::clear_kernel_thunk() {
  if (_internal_has_kernel_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.kernel_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::KernelThunkProto* ThunkProto::release_kernel_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.kernel_thunk)
  if (_internal_has_kernel_thunk()) {
    clear_has_impl();
    ::xla::gpu::KernelThunkProto* temp = _impl_.impl_.kernel_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.kernel_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::KernelThunkProto& ThunkProto::_internal_kernel_thunk() const {
  return _internal_has_kernel_thunk()
      ? *_impl_.impl_.kernel_thunk_
      : reinterpret_cast< ::xla::gpu::KernelThunkProto&>(::xla::gpu::_KernelThunkProto_default_instance_);
}
inline const ::xla::gpu::KernelThunkProto& ThunkProto::kernel_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.kernel_thunk)
  return _internal_kernel_thunk();
}
inline ::xla::gpu::KernelThunkProto* ThunkProto::unsafe_arena_release_kernel_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.kernel_thunk)
  if (_internal_has_kernel_thunk()) {
    clear_has_impl();
    ::xla::gpu::KernelThunkProto* temp = _impl_.impl_.kernel_thunk_;
    _impl_.impl_.kernel_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_kernel_thunk(::xla::gpu::KernelThunkProto* kernel_thunk) {
  clear_impl();
  if (kernel_thunk) {
    set_has_kernel_thunk();
    _impl_.impl_.kernel_thunk_ = kernel_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.kernel_thunk)
}
inline ::xla::gpu::KernelThunkProto* ThunkProto::_internal_mutable_kernel_thunk() {
  if (!_internal_has_kernel_thunk()) {
    clear_impl();
    set_has_kernel_thunk();
    _impl_.impl_.kernel_thunk_ = CreateMaybeMessage< ::xla::gpu::KernelThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.kernel_thunk_;
}
inline ::xla::gpu::KernelThunkProto* ThunkProto::mutable_kernel_thunk() {
  ::xla::gpu::KernelThunkProto* _msg = _internal_mutable_kernel_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.kernel_thunk)
  return _msg;
}

// .xla.gpu.GemmThunkProto gemm_thunk = 9;
inline bool ThunkProto::_internal_has_gemm_thunk() const {
  return impl_case() == kGemmThunk;
}
inline bool ThunkProto::has_gemm_thunk() const {
  return _internal_has_gemm_thunk();
}
inline void ThunkProto::set_has_gemm_thunk() {
  _impl_._oneof_case_[0] = kGemmThunk;
}
inline void ThunkProto::clear_gemm_thunk() {
  if (_internal_has_gemm_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.gemm_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::GemmThunkProto* ThunkProto::release_gemm_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.gemm_thunk)
  if (_internal_has_gemm_thunk()) {
    clear_has_impl();
    ::xla::gpu::GemmThunkProto* temp = _impl_.impl_.gemm_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.gemm_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::GemmThunkProto& ThunkProto::_internal_gemm_thunk() const {
  return _internal_has_gemm_thunk()
      ? *_impl_.impl_.gemm_thunk_
      : reinterpret_cast< ::xla::gpu::GemmThunkProto&>(::xla::gpu::_GemmThunkProto_default_instance_);
}
inline const ::xla::gpu::GemmThunkProto& ThunkProto::gemm_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.gemm_thunk)
  return _internal_gemm_thunk();
}
inline ::xla::gpu::GemmThunkProto* ThunkProto::unsafe_arena_release_gemm_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.gemm_thunk)
  if (_internal_has_gemm_thunk()) {
    clear_has_impl();
    ::xla::gpu::GemmThunkProto* temp = _impl_.impl_.gemm_thunk_;
    _impl_.impl_.gemm_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_gemm_thunk(::xla::gpu::GemmThunkProto* gemm_thunk) {
  clear_impl();
  if (gemm_thunk) {
    set_has_gemm_thunk();
    _impl_.impl_.gemm_thunk_ = gemm_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.gemm_thunk)
}
inline ::xla::gpu::GemmThunkProto* ThunkProto::_internal_mutable_gemm_thunk() {
  if (!_internal_has_gemm_thunk()) {
    clear_impl();
    set_has_gemm_thunk();
    _impl_.impl_.gemm_thunk_ = CreateMaybeMessage< ::xla::gpu::GemmThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.gemm_thunk_;
}
inline ::xla::gpu::GemmThunkProto* ThunkProto::mutable_gemm_thunk() {
  ::xla::gpu::GemmThunkProto* _msg = _internal_mutable_gemm_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.gemm_thunk)
  return _msg;
}

inline bool ThunkProto::has_impl() const {
  return impl_case() != IMPL_NOT_SET;
}
inline void ThunkProto::clear_has_impl() {
  _impl_._oneof_case_[0] = IMPL_NOT_SET;
}
inline ThunkProto::ImplCase ThunkProto::impl_case() const {
  return ThunkProto::ImplCase(_impl_._oneof_case_[0]);
}
// -------------------------------------------------------------------

// SequentialThunkProto

// repeated .xla.gpu.ThunkProto thunks = 1;
inline int SequentialThunkProto::_internal_thunks_size() const {
  return _impl_.thunks_.size();
}
inline int SequentialThunkProto::thunks_size() const {
  return _internal_thunks_size();
}
inline void SequentialThunkProto::clear_thunks() {
  _impl_.thunks_.Clear();
}
inline ::xla::gpu::ThunkProto* SequentialThunkProto::mutable_thunks(int index) {
  // @@protoc_insertion_point(field_mutable:xla.gpu.SequentialThunkProto.thunks)
  return _impl_.thunks_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >*
SequentialThunkProto::mutable_thunks() {
  // @@protoc_insertion_point(field_mutable_list:xla.gpu.SequentialThunkProto.thunks)
  return &_impl_.thunks_;
}
inline const ::xla::gpu::ThunkProto& SequentialThunkProto::_internal_thunks(int index) const {
  return _impl_.thunks_.Get(index);
}
inline const ::xla::gpu::ThunkProto& SequentialThunkProto::thunks(int index) const {
  // @@protoc_insertion_point(field_get:xla.gpu.SequentialThunkProto.thunks)
  return _internal_thunks(index);
}
inline ::xla::gpu::ThunkProto* SequentialThunkProto::_internal_add_thunks() {
  return _impl_.thunks_.Add();
}
inline ::xla::gpu::ThunkProto* SequentialThunkProto::add_thunks() {
  ::xla::gpu::ThunkProto* _add = _internal_add_thunks();
  // @@protoc_insertion_point(field_add:xla.gpu.SequentialThunkProto.thunks)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >&
SequentialThunkProto::thunks() const {
  // @@protoc_insertion_point(field_list:xla.gpu.SequentialThunkProto.thunks)
  return _impl_.thunks_;
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace gpu
}  // namespace xla

// @@protoc_insertion_point(global_scope)

#include <google/protobuf/port_undef.inc>
#endif  // GOOGLE_PROTOBUF_INCLUDED_GOOGLE_PROTOBUF_INCLUDED_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto
