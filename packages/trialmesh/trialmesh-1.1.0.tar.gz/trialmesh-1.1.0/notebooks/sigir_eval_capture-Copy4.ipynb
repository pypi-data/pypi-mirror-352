{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac0e7e2-8e7f-4c4b-afbc-ff2b612bf9a4",
   "metadata": {},
   "source": [
    "## **Notebook Summary**\n",
    "\n",
    "### **Purpose**\n",
    "This notebook is designed to analyze and audit retrieval results from various clinical trial search models (e.g., BGE, SapBERT, etc.) by comparing their top-retrieved trials for a set of queries against gold-standard relevance annotations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Steps and Logic**\n",
    "\n",
    "#### **1. Configuration**\n",
    "- The notebook sets up paths to:\n",
    "  - Retrieval search result files (per model, e.g., `\"bge-large-en-v1.5_hnsw_search_results.json\"`).\n",
    "  - Processed gold-standard files: `test.tsv`, `queries.jsonl`, `corpus.jsonl`.\n",
    "\n",
    "#### **2. Data Loading**\n",
    "- **Gold Data:** Loads truth labels (`test.tsv`), queries (`queries.jsonl`), and clinical trial info (`corpus.jsonl`) using pandas.\n",
    "- **Retrieval Results:** Loads search results per model (typically nested JSON output, where each query contains a results list).\n",
    "\n",
    "#### **3. Results Flattening**\n",
    "- Defines a function (`flatten_results`) to transform nested search results into a flat DataFrame:\n",
    "  - Columns: `query-id`, `corpus-id`, `score` (retrieval score).\n",
    "\n",
    "#### **4. Merging & Alignment**\n",
    "- Merges the model's flat results with gold-standard relevance labels, and adds in query/trial metadata for further analysis.\n",
    "\n",
    "#### **5. Error/Quality Analysis**\n",
    "- **Missing Pairs:** Identifies gold-standard query-trial relevance pairs not retrieved at all by the model (i.e., relevant trial not among predictions).\n",
    "- **Extra Pairs:** Identifies predicted pairs that are not in the gold set (possible false positives).\n",
    "\n",
    "#### **6. Per-Query Statistics**\n",
    "- For every query:\n",
    "  - Counts for \"score 1\" and \"score 2\" pairs in the gold set (typically 'relevant', 'definitely relevant').\n",
    "  - Fraction/number of such pairs recovered by the model.\n",
    "  - Computes the percentage of missing relevant trials (by score, per query).\n",
    "\n",
    "#### **7. Overall Summary**\n",
    "- Totals the above statistics across all queries:\n",
    "  - Total number of \"score 1\"/\"score 2\" pairs in the gold set.\n",
    "  - Total number missed by the model.\n",
    "  - Percentage missed, for reporting model coverage/gaps.\n",
    "\n",
    "#### **8. Main Analysis Loop**\n",
    "- For each specified model:\n",
    "  - Runs the above analysis.\n",
    "  - Displays per-query recovery stats.\n",
    "  - Prints a summary row.\n",
    "- Final output is a summary table, with one row per model.\n",
    "\n",
    "---\n",
    "\n",
    "### **Outputs**\n",
    "- **Per-Query Table:** For each query, presents coverage of relevant trials by the model.\n",
    "- **Summary Table:** Easy comparison across models: how many gold-standard relevant pairs each misses, by relevance level.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intended Use**\n",
    "- **Sanity check / QA:** For dataset and retrieval result integration.\n",
    "- **Model evaluation:** Not full IR metrics (e.g., MAP/MRR), but focuses on recall/gap analysis for critical pairs in the gold set.\n",
    "- **Model comparison:** Facilitates head-to-head comparison of clinical retrieval modelsâ€™ ability to surface relevant trials.\n",
    "\n",
    "---\n",
    "\n",
    "### **Technical Notes**\n",
    "- **Modularity:** Can be easily adapted to add more models/files via the `RESULT_FILES` dictionary.\n",
    "- **Not a full evaluation script:** Does not compute MAP/MRR; focuses on missing-relevant analysis.\n",
    "- **Extendable:** Additional analysis, plotting etc. can be built on top of output tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b4bc6bd-3d60-4b6e-b235-2a91198a95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48fe3231-caf6-4593-acf5-a35b69ad346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bge-large-en-v1.5_hnsw_search_results.json\n",
      "Bio_ClinicalBERT_hnsw_search_results.json\n",
      "bluebert_hnsw_search_results.json\n",
      "e5-large-v2_hnsw_search_results.json\n",
      "SapBERT_hnsw_search_results.json\n"
     ]
    }
   ],
   "source": [
    "ls ../data/sigir2016/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102e5c4b-e0cd-4e8b-9aae-0e43a7d075b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "# RESULT_FILES = {\n",
    "#     \"SapBERT\": \"../data/sigir2016/results/SapBERT_flat_search_results.json\",\n",
    "#     \"bge-large-en-v1.5\": \"../data/sigir2016/results/bge-large-en-v1.5_flat_search_results.json\",\n",
    "#     \"e5-large-v2\": \"../data/sigir2016/results/e5-large-v2_flat_search_results.json\",\n",
    "#     \"Bio_ClinicalBERT\": \"../data/sigir2016/results/Bio_ClinicalBERT_flat_search_results.json\",\n",
    "#     \"bluebert\": \"../data/sigir2016/results/bluebert_flat_search_results.json\",\n",
    "# }\n",
    "\n",
    "RESULT_FILES = {\n",
    "    \"SapBERT\": \"../data/sigir2016/results/SapBERT_hnsw_search_results.json\",\n",
    "    \"bge-large-en-v1.5\": \"../data/sigir2016/results/bge-large-en-v1.5_hnsw_search_results.json\",\n",
    "    \"e5-large-v2\": \"../data/sigir2016/results/e5-large-v2_hnsw_search_results.json\",\n",
    "    \"Bio_ClinicalBERT\": \"../data/sigir2016/results/Bio_ClinicalBERT_hnsw_search_results.json\",\n",
    "    \"bluebert\": \"../data/sigir2016/results/bluebert_hnsw_search_results.json\",\n",
    "}\n",
    "\n",
    "# RESULT_FILES = {\"bge-large-en-v1.5\": \"../data/sigir2016/results/bge-large-en-v1.5_hnsw_search_results.json\"}\n",
    "DATA_DIR = \"../data/sigir2016/processed_cut\"\n",
    "TSV_FILE = os.path.join(DATA_DIR, \"test.tsv\")\n",
    "QUERIES_FILE = os.path.join(DATA_DIR, \"queries.jsonl\")\n",
    "CORPUS_FILE = os.path.join(DATA_DIR, \"corpus.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84bc979-e000-4473-91e9-0cab210e22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading ---\n",
    "def load_gold_data(tsv_file, queries_file, corpus_file):\n",
    "    df_tsv = pd.read_csv(tsv_file, sep='\\t')\n",
    "    df_queries = pd.read_json(queries_file, lines=True)\n",
    "    df_corpus = pd.read_json(corpus_file, lines=True)\n",
    "    print(f\" df_tsv {len(df_tsv)} df_queries {len(df_queries)} df_corpus {len(df_corpus)}\")\n",
    "    return df_tsv, df_queries, df_corpus\n",
    "\n",
    "def load_search_results(results_file):\n",
    "    return pd.read_json(results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4bf3e9-acea-4b77-81e2-39e6fb2e4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Processing ---\n",
    "def flatten_results(results_df):\n",
    "    \"\"\"Flatten the nested results into a long DataFrame.\"\"\"\n",
    "    \n",
    "    dfs = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        for result in row['results']:\n",
    "            dfs.append({\n",
    "                'query-id': row['query_id'],\n",
    "                'corpus-id': result['doc_id'],\n",
    "                'score': result['score']\n",
    "            })\n",
    "    print(f\" results_df {len(dfs)}\")\n",
    "    return pd.DataFrame(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9af1c322-fe02-4416-8c23-ae2db121db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_gold(df_results_long, df_tsv, df_queries, df_corpus):\n",
    "    eval_df = df_results_long.merge(\n",
    "        df_tsv,\n",
    "        how='left',\n",
    "        on=['query-id', 'corpus-id'],\n",
    "        suffixes=('_pred', '_true')\n",
    "    )\n",
    "    eval_df = eval_df.merge(df_queries.rename(columns={'_id':'query-id'}), on='query-id', how='left')\n",
    "    eval_df = eval_df.merge(df_corpus.rename(columns={'_id':'corpus-id'}), on='corpus-id', how='left')\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b59b8de-00f2-4524-9573-9dfa08aa2a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_missing_pairs(df_tsv, df_results_long):\n",
    "    merged = df_tsv.merge(\n",
    "        df_results_long[['query-id', 'corpus-id']],\n",
    "        on=['query-id', 'corpus-id'],\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "    missing_gold = merged[merged['_merge'] == 'left_only']\n",
    "    missing_gold = missing_gold[missing_gold['score'] != 0].reset_index(drop=True)\n",
    "    return missing_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "673aa489-9300-4a51-8edc-9f0b372d7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_extra_pairs(df_tsv, df_results_long):\n",
    "    extra_preds = df_results_long.merge(\n",
    "        df_tsv[['query-id', 'corpus-id']],\n",
    "        on=['query-id', 'corpus-id'],\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "    extras = extra_preds[extra_preds['_merge'] == 'left_only']\n",
    "    return extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe02e7d-6e06-4583-a9c7-853479bbc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_query_stats(df_tsv, df_results_long):\n",
    "    df_tsv = df_tsv.copy()\n",
    "    df_tsv['label_1'] = (df_tsv['score'] == 1).astype(int)\n",
    "    df_tsv['label_2'] = (df_tsv['score'] == 2).astype(int)\n",
    "    pred_pairs = set(zip(df_results_long['query-id'], df_results_long['corpus-id']))\n",
    "    df_tsv['found'] = df_tsv.apply(lambda row: (row['query-id'], row['corpus-id']) in pred_pairs, axis=1)\n",
    "    per_query = (\n",
    "        df_tsv\n",
    "        .groupby('query-id', as_index=True)\n",
    "        .agg(\n",
    "            total_score_1=('label_1', 'sum'),\n",
    "            total_score_2=('label_2', 'sum'),\n",
    "            found_score_1=('found', lambda x: int(((x) & (df_tsv.loc[x.index, 'label_1'] == 1)).sum())),\n",
    "            found_score_2=('found', lambda x: int(((x) & (df_tsv.loc[x.index, 'label_2'] == 1)).sum()))\n",
    "        )\n",
    "    )\n",
    "    per_query['missing_score_1'] = per_query['total_score_1'] - per_query['found_score_1']\n",
    "    per_query['missing_score_2'] = per_query['total_score_2'] - per_query['found_score_2']\n",
    "    per_query['percent_missing_1'] = np.where(\n",
    "        per_query['total_score_1'] == 0, 0.0,\n",
    "        100 * per_query['missing_score_1'] / per_query['total_score_1']\n",
    "    ).round(1)\n",
    "    per_query['percent_missing_2'] = np.where(\n",
    "        per_query['total_score_2'] == 0, 0.0,\n",
    "        100 * per_query['missing_score_2'] / per_query['total_score_2']\n",
    "    ).round(1)\n",
    "    return per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ed1ad5b-c6e3-4f1a-bbe8-f02dc5e455ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_stats(per_query):\n",
    "    total_1 = int(per_query['total_score_1'].sum())\n",
    "    total_2 = int(per_query['total_score_2'].sum())\n",
    "    missing_1 = int(per_query['missing_score_1'].sum())\n",
    "    missing_2 = int(per_query['missing_score_2'].sum())\n",
    "    percent_missing_1 = (missing_1 / total_1 * 100) if total_1 > 0 else 0\n",
    "    percent_missing_2 = (missing_2 / total_2 * 100) if total_2 > 0 else 0\n",
    "    return {\n",
    "        \"Total Score 1\": total_1,\n",
    "        \"Total Score 2\": total_2,\n",
    "        \"Missing Score 1\": missing_1,\n",
    "        \"Missing Score 2\": missing_2,\n",
    "        \"Percent Missing 1\": percent_missing_1,\n",
    "        \"Percent Missing 2\": percent_missing_2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2611bb2d-b1f1-425e-9d2b-51f92df2feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Loop ---\n",
    "def analyze_all_models(result_files, tsv_file, queries_file, corpus_file):\n",
    "    df_tsv, df_queries, df_corpus = load_gold_data(tsv_file, queries_file, corpus_file)\n",
    "    summary = []\n",
    "    for model_name, results_file in result_files.items():\n",
    "        print(f\"\\n--- {model_name} ---\")\n",
    "        results_df = load_search_results(results_file)\n",
    "        df_results_long = flatten_results(results_df)\n",
    "        per_query = per_query_stats(df_tsv, df_results_long)\n",
    "        # Display or export per_query\n",
    "        # display(per_query)\n",
    "        stats = overall_stats(per_query)\n",
    "        print(f\"Total Score 1: {stats['Total Score 1']}\")\n",
    "        print(f\"Total Score 2: {stats['Total Score 2']}\")\n",
    "        print(f\"Missing Score 1: {stats['Missing Score 1']} ({stats['Percent Missing 1']:.1f}%)\")\n",
    "        print(f\"Missing Score 2: {stats['Missing Score 2']} ({stats['Percent Missing 2']:.1f}%)\")\n",
    "        summary.append({\n",
    "            \"Model\": model_name,\n",
    "            **stats\n",
    "        })\n",
    "    return pd.DataFrame(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ab07a2-9242-4405-a800-204431ef22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options \n",
    "pd.set_option('display.max_rows', 4)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f9bdb3-314a-45fc-ae29-fbb5bacceaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " df_tsv 3870 df_queries 59 df_corpus 3626\n",
      "\n",
      "--- SapBERT ---\n",
      " results_df 7552\n",
      "Total Score 1: 685\n",
      "Total Score 2: 421\n",
      "Missing Score 1: 259 (37.8%)\n",
      "Missing Score 2: 133 (31.6%)\n",
      "\n",
      "--- bge-large-en-v1.5 ---\n",
      " results_df 7552\n",
      "Total Score 1: 685\n",
      "Total Score 2: 421\n",
      "Missing Score 1: 152 (22.2%)\n",
      "Missing Score 2: 61 (14.5%)\n",
      "\n",
      "--- e5-large-v2 ---\n",
      " results_df 7552\n",
      "Total Score 1: 685\n",
      "Total Score 2: 421\n",
      "Missing Score 1: 162 (23.6%)\n",
      "Missing Score 2: 77 (18.3%)\n",
      "\n",
      "--- Bio_ClinicalBERT ---\n",
      " results_df 7552\n",
      "Total Score 1: 685\n",
      "Total Score 2: 421\n",
      "Missing Score 1: 476 (69.5%)\n",
      "Missing Score 2: 315 (74.8%)\n",
      "\n",
      "--- bluebert ---\n",
      " results_df 7552\n",
      "Total Score 1: 685\n",
      "Total Score 2: 421\n",
      "Missing Score 1: 619 (90.4%)\n",
      "Missing Score 2: 374 (88.8%)\n",
      "\n",
      "=== Summary Table ===\n",
      "            Model  Total Score 1  Total Score 2  Missing Score 1  Missing Score 2  Percent Missing 1  Percent Missing 2\n",
      "          SapBERT            685            421              259              133          37.810219          31.591449\n",
      "bge-large-en-v1.5            685            421              152               61          22.189781          14.489311\n",
      "      e5-large-v2            685            421              162               77          23.649635          18.289786\n",
      " Bio_ClinicalBERT            685            421              476              315          69.489051          74.821853\n",
      "         bluebert            685            421              619              374          90.364964          88.836105\n"
     ]
    }
   ],
   "source": [
    "# --- Run ---\n",
    "# if __name__ == \"__main__\":\n",
    "summary_df = analyze_all_models(RESULT_FILES, TSV_FILE, QUERIES_FILE, CORPUS_FILE)\n",
    "print(\"\\n=== Summary Table ===\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a4c67-6232-429b-9d55-eb183c38423b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
