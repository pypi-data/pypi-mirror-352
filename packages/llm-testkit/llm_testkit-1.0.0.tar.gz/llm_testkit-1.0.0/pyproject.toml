[build-system]
requires = ["setuptools>=45", "wheel", "setuptools_scm[toml]>=6.2"]
build-backend = "setuptools.build_meta"

[project]
name = "llm-testkit"
dynamic = ["version"]
description = "Professional LLM Evaluation Framework with Beautiful HTML Reports"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Matthias De Paolis", email = "mattdepaolis@users.noreply.github.com"}
]
maintainers = [
    {name = "Matthias De Paolis", email = "mattdepaolis@users.noreply.github.com"}
]
keywords = ["llm", "evaluation", "language-models", "ai", "machine-learning", "nlp", "transformers", "html-reports"]
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Text Processing :: Linguistic",
]
requires-python = ">=3.8"
dependencies = [
    "torch>=2.7.0",
    "transformers>=4.20.0",
    "numpy>=1.21.0",
    "pandas>=1.3.0",
    "tqdm>=4.62.0",
    "matplotlib>=3.4.0",
    "seaborn>=0.11.0",
    "datasets>=2.0.0",
    "accelerate>=0.12.0",
    "sacrebleu>=2.0.0",
    "sqlitedict>=2.0.0",
    "lm-eval>=0.4.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=6.0",
    "pytest-cov>=2.0",
    "black>=21.0",
    "flake8>=3.9",
    "mypy>=0.910",
]
gpu = [
    "torch[cuda]",
    "accelerate[gpu]",
]
all = [
    "pytest>=6.0",
    "pytest-cov>=2.0",
    "black>=21.0",
    "flake8>=3.9",
    "mypy>=0.910",
    "torch[cuda]",
    "accelerate[gpu]",
]

[project.urls]
Homepage = "https://github.com/mattdepaolis/llm-eval"
Documentation = "https://github.com/mattdepaolis/llm-eval#readme"
Repository = "https://github.com/mattdepaolis/llm-eval"
"Bug Tracker" = "https://github.com/mattdepaolis/llm-eval/issues"
Changelog = "https://github.com/mattdepaolis/llm-eval/blob/main/CHANGELOG.md"

[project.scripts]
llm-eval = "llm_eval.cli:main"
llm-eval-demo = "llm_eval.cli:demo_main"
llm-eval-html = "llm_eval.cli:html_main"
llm-eval-showcase = "llm_eval.cli:showcase_main"

[tool.setuptools]
packages = ["llm_eval"]
include-package-data = true

[tool.setuptools.package-data]
"llm_eval" = [
    "templates/*.html",
    "static/*.css", 
    "static/*.js",
    "configs/*.yaml",
    "configs/*.json",
]

[tool.setuptools_scm]
write_to = "llm_eval/_version.py"

[tool.black]
line-length = 100
target-version = ['py38']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.mypy]
python_version = "3.8"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[tool.pytest.ini_options]
minversion = "6.0"
addopts = "-ra -q --strict-markers"
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]

[tool.coverage.run]
source = ["llm_eval"]
omit = ["*/tests/*", "*/test_*"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod",
] 