# Xpidy - æ™ºèƒ½ç½‘é¡µæ•°æ®æå–æ¡†æ¶

ä¸€ä¸ªåŸºäºPlaywrightçš„ç°ä»£åŒ–æ™ºèƒ½çˆ¬è™«åº“ï¼Œé‡‡ç”¨"é…ç½®é©±åŠ¨"è®¾è®¡ç†å¿µï¼Œè®©ç½‘é¡µæ•°æ®æå–å˜å¾—æå…¶ç®€å•ã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¯ **é…ç½®é©±åŠ¨** - åŸºäºé…ç½®çš„è‡ªåŠ¨åŒ–æ•°æ®æå–ï¼Œé…ç½®å³æ–‡æ¡£
- ğŸš€ **æç®€API** - é…ç½®å®Œæˆååªéœ€ `await spider.crawl(url)` 
- ğŸ­ **Playwrighté©±åŠ¨** - æ”¯æŒJavaScriptæ¸²æŸ“å’ŒSPAåº”ç”¨
- âš¡ **é«˜æ€§èƒ½** - å¤šæå–å™¨å¹¶å‘æ‰§è¡Œï¼Œå†…ç½®æ™ºèƒ½ç¼“å­˜
- ğŸ§  **LLMå¢å¼º** - å¯é€‰çš„å¤§è¯­è¨€æ¨¡å‹åå¤„ç†ä¼˜åŒ–
- ğŸ“‹ **å…¨é¢æå–** - æ–‡æœ¬ã€é“¾æ¥ã€å›¾ç‰‡ã€è¡¨æ ¼ã€è¡¨å•ç­‰å¤šç§æ•°æ®
- ğŸ” **çµæ´»é€‰æ‹©å™¨** - æ”¯æŒCSSé€‰æ‹©å™¨å’ŒXPathèŒƒå›´é™åˆ¶
- ğŸ›¡ï¸ **åçˆ¬è™«** - å†…ç½®éšèº«æ¨¡å¼å’Œéšæœºå»¶è¿Ÿ
- ğŸ”§ **CLIå·¥å…·** - å®Œæ•´çš„å‘½ä»¤è¡Œå·¥å…·æ”¯æŒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å®‰è£…uvï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
curl -LsSf https://astral.sh/uv/install.sh | sh

# å…‹éš†é¡¹ç›®
git clone https://github.com/Ciciy-l/Xpidy.git
cd Xpidy

# å®‰è£…ä¾èµ–
uv sync

# å®‰è£…Playwrightæµè§ˆå™¨
uv run playwright install
```

### æœ€ç®€ä½¿ç”¨æ–¹å¼

```python
import asyncio
from xpidy import Spider

async def main():
    # å¿«é€Ÿåˆ›å»ºçˆ¬è™«å®ä¾‹
    spider = Spider.quick_create(
        enable_text=True,      # æå–æ–‡æœ¬
        enable_links=True,     # æå–é“¾æ¥
        enable_images=True     # æå–å›¾ç‰‡
    )
    
    # ä¸€è¡Œä»£ç å®Œæˆæ‰€æœ‰æå–
    async with spider:
        result = await spider.crawl("https://example.com")
        print(f"æå–å®Œæˆï¼æ–‡æœ¬: {len(result['results']['text']['content'])}å­—ç¬¦")

asyncio.run(main())
```

### é…ç½®é©±åŠ¨æ–¹å¼

```python
import asyncio
from xpidy import Spider, XpidyConfig, ExtractionConfig, SpiderConfig

async def main():
    # 1. å®šä¹‰é…ç½®ï¼ˆé…ç½®å³æ–‡æ¡£ï¼‰
    config = XpidyConfig(
        spider_config=SpiderConfig(
            headless=True,
            timeout=30000,
            stealth_mode=True
        ),
        extraction_config=ExtractionConfig(
            enable_text=True,
            enable_links=True, 
            enable_images=True,
            text_config={"min_text_length": 10},
            links_config={"max_items": 50},
            images_config={"max_items": 20}
        )
    )
    
    # 2. åˆ›å»ºçˆ¬è™«å®ä¾‹
    async with Spider(config) as spider:
        # 3. ä¸€é”®æå–æ‰€æœ‰é…ç½®çš„æ•°æ®
        result = await spider.crawl("https://example.com")
        
        # 4. ä½¿ç”¨ç»“æ„åŒ–ç»“æœ
        print(f"URL: {result['url']}")
        print(f"æå–å™¨: {result['extractors_used']}")
        print(f"è€—æ—¶: {result['extraction_time']:.2f}ç§’")
        
        # 5. è®¿é—®å„æå–å™¨ç»“æœ
        if 'text' in result['results']:
            text_data = result['results']['text']
            print(f"æ–‡æœ¬é•¿åº¦: {len(text_data['content'])}å­—ç¬¦")
            print(f"æ ‡é¢˜: {text_data['metadata']['title']}")
        
        if 'links' in result['results']:
            links_data = result['results']['links']
            print(f"é“¾æ¥æ•°é‡: {links_data['total_links']}")
            print(f"å†…éƒ¨é“¾æ¥: {len(links_data['internal_links'])}")
        
        if 'images' in result['results']:
            images_data = result['results']['images']
            print(f"å›¾ç‰‡æ•°é‡: {images_data['total_images']}")

asyncio.run(main())
```

## ğŸ“‹ CLIä½¿ç”¨æŒ‡å—

### åŸºç¡€å‘½ä»¤

```bash
# 1. ç”Ÿæˆé…ç½®æ¨¡æ¿
xpidy init basic --output my_config.json

# 2. éªŒè¯é…ç½®æ–‡ä»¶
xpidy validate my_config.json

# 3. æ‰§è¡Œçˆ¬å–ä»»åŠ¡
xpidy run my_config.json --output results.json

# 4. å¿«é€Ÿçˆ¬å–å•ä¸ªURL
xpidy quick https://example.com --enable-links --enable-images
```

### é…ç½®æ–‡ä»¶ç¤ºä¾‹

```json
{
  "spider_config": {
    "headless": true,
    "timeout": 30000,
    "stealth_mode": true
  },
  "extraction_config": {
    "enable_text": true,
    "enable_links": true,
    "enable_images": true,
    "text_config": {
      "min_text_length": 10,
      "extract_metadata": true
    },
    "links_config": {
      "include_internal": true,
      "include_external": true,
      "max_items": 50
    },
    "images_config": {
      "min_width": 100,
      "min_height": 100,
      "max_items": 20
    }
  },
  "tasks": [
    {
      "url": "https://example.com",
      "name": "example_site"
    }
  ]
}
```

### å¯ç”¨æ¨¡æ¿

```bash
# åŸºç¡€æ–‡æœ¬æå–
xpidy init basic

# é“¾æ¥åˆ†æ  
xpidy init links

# å›¾ç‰‡åˆ†æ
xpidy init images

# å…¨é¢æ•°æ®æå–
xpidy init comprehensive

# ç»“æ„åŒ–æ•°æ®æå–
xpidy init data

# è¡¨å•æ•°æ®æå–
xpidy init form
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. é€‰æ‹©å™¨èŒƒå›´é™åˆ¶

```python
from xpidy import XpidyConfig, ExtractionConfig, SpiderConfig

config = XpidyConfig(
    extraction_config=ExtractionConfig(
        enable_text=True,
        enable_links=True,
        # å…¨å±€é€‰æ‹©å™¨é™åˆ¶
        css_selector="main .content",  # åªåœ¨ä¸»å†…å®¹åŒºåŸŸæå–
        text_config={
            "css_selector": "article p"  # æ–‡æœ¬æå–å™¨ä¸“ç”¨é€‰æ‹©å™¨
        },
        links_config={
            "css_selector": "nav a, .sidebar a"  # é“¾æ¥æå–å™¨ä¸“ç”¨é€‰æ‹©å™¨
        }
    )
)
```

### 2. é…ç½®æ–‡ä»¶ä¿å­˜ä¸åŠ è½½

```python
# ä¿å­˜é…ç½®
config = XpidyConfig(...)
config.save_to_file("my_config.json")

# ä»æ–‡ä»¶åŠ è½½é…ç½®
config = XpidyConfig.from_file("my_config.json")
```

### 3. LLMåå¤„ç†ï¼ˆå¯é€‰ï¼‰

```python
from xpidy import LLMConfig

config = XpidyConfig(
    extraction_config=ExtractionConfig(enable_text=True),
    llm_config=LLMConfig(
        provider="openai",
        model="gpt-3.5-turbo", 
        api_key="your-api-key",
        enabled=True
    )
)

async with Spider(config) as spider:
    # ä½¿ç”¨LLMä¼˜åŒ–ç»“æœ
    result = await spider.crawl(
        "https://example.com",
        prompt="è¯·æå–å…³é”®ä¿¡æ¯å¹¶æ•´ç†ä¸ºç»“æ„åŒ–æ ¼å¼"
    )
```

### 4. å¹¶å‘æ‰¹é‡å¤„ç†

```python
urls = [
    "https://example1.com",
    "https://example2.com", 
    "https://example3.com"
]

async with Spider(config) as spider:
    # å¹¶å‘å¤„ç†å¤šä¸ªURL
    results = []
    tasks = [spider.crawl(url) for url in urls]
    results = await asyncio.gather(*tasks, return_exceptions=True)
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

1. **é…ç½®é©±åŠ¨**ï¼šæ‰€æœ‰æå–é€»è¾‘é€šè¿‡é…ç½®å£°æ˜ï¼Œè¿è¡Œæ—¶è‡ªåŠ¨æ‰§è¡Œ
2. **å¹¶å‘æ¶æ„**ï¼šå¤šæå–å™¨è‡ªåŠ¨å¹¶å‘æ‰§è¡Œï¼Œæé«˜æ•ˆç‡  
3. **æ™ºèƒ½ç¼“å­˜**ï¼šæå–å™¨å†…éƒ¨ç»“æœæš‚å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
4. **ä¼˜é›…é”™è¯¯å¤„ç†**ï¼šå•ç‚¹å¤±è´¥ä¸å½±å“æ•´ä½“ï¼Œè¯¦ç»†é”™è¯¯æ—¥å¿—

### æ¶æ„å±‚æ¬¡

```
ç”¨æˆ·å±‚: æç®€API (Spider.crawl)
é…ç½®å±‚: å£°æ˜å¼é…ç½® (XpidyConfig, ExtractionConfig, SpiderConfig)
æ§åˆ¶å±‚: æ™ºèƒ½è°ƒåº¦ (Spiderå†…éƒ¨å¹¶å‘ç®¡ç†)
æ‰§è¡Œå±‚: ä¸“ç”¨æå–å™¨ (TextExtractor, LinkExtractor, ImageExtractorç­‰)
åŸºç¡€å±‚: ç»Ÿä¸€æŠ½è±¡ (BaseExtractor, Playwright)
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
Xpidy/
â”œâ”€â”€ xpidy/                      # ä¸»åŒ…
â”‚   â”œâ”€â”€ core/                   # æ ¸å¿ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ spider.py           # ä¸»çˆ¬è™«ç±»
â”‚   â”‚   â”œâ”€â”€ config.py           # é…ç½®ç±»å®šä¹‰  
â”‚   â”‚   â”œâ”€â”€ llm_processor.py    # LLMå¤„ç†å™¨
â”‚   â”‚   â””â”€â”€ __init__.py         # æ ¸å¿ƒæ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ extractors/             # æ•°æ®æå–å™¨æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ base_extractor.py   # æå–å™¨åŸºç±»
â”‚   â”‚   â”œâ”€â”€ text_extractor.py   # æ–‡æœ¬æå–å™¨
â”‚   â”‚   â”œâ”€â”€ link_extractor.py   # é“¾æ¥æå–å™¨
â”‚   â”‚   â”œâ”€â”€ image_extractor.py  # å›¾ç‰‡æå–å™¨
â”‚   â”‚   â”œâ”€â”€ data_extractor.py   # ç»“æ„åŒ–æ•°æ®æå–å™¨
â”‚   â”‚   â”œâ”€â”€ form_extractor.py   # è¡¨å•æå–å™¨
â”‚   â”‚   â””â”€â”€ __init__.py         # æå–å™¨æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ utils/                  # å·¥å…·æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ cache.py            # ç¼“å­˜ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ content_utils.py    # å†…å®¹å¤„ç†å·¥å…·
â”‚   â”‚   â”œâ”€â”€ url_utils.py        # URLå¤„ç†å·¥å…·
â”‚   â”‚   â””â”€â”€ __init__.py         # å·¥å…·æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ cli.py                  # é…ç½®é©±åŠ¨çš„å‘½ä»¤è¡Œå·¥å…·
â”‚   â””â”€â”€ __init__.py             # åŒ…ä¸»å…¥å£
â”œâ”€â”€ examples/                   # ç¤ºä¾‹ä»£ç 
â”œâ”€â”€ tests/                      # æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ .venv/                      # è™šæ‹Ÿç¯å¢ƒ (uvç®¡ç†)
â”œâ”€â”€ pyproject.toml              # é¡¹ç›®é…ç½®æ–‡ä»¶
â”œâ”€â”€ uv.lock                     # ä¾èµ–é”å®šæ–‡ä»¶
â””â”€â”€ README.md                   # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## ğŸ”§ é…ç½®è¯¦è§£

### SpiderConfigï¼ˆçˆ¬è™«é…ç½®ï¼‰

```python
from xpidy import SpiderConfig

spider_config = SpiderConfig(
    browser_type="chromium",      # æµè§ˆå™¨ç±»å‹: chromium/firefox/webkit
    headless=True,                # æ— å¤´æ¨¡å¼
    timeout=30000,                # è¶…æ—¶æ—¶é—´(æ¯«ç§’)
    stealth_mode=True,            # éšèº«æ¨¡å¼
    random_delay=True,            # éšæœºå»¶è¿Ÿ
    min_delay=0.5,                # æœ€å°å»¶è¿Ÿ(ç§’)
    max_delay=2.0,                # æœ€å¤§å»¶è¿Ÿ(ç§’)
    max_retries=3,                # æœ€å¤§é‡è¯•æ¬¡æ•°
    user_agent="custom-ua",       # è‡ªå®šä¹‰UA
    viewport_width=1920,          # è§†å£å®½åº¦
    viewport_height=1080          # è§†å£é«˜åº¦
)
```

### ExtractionConfigï¼ˆæå–é…ç½®ï¼‰

```python
from xpidy import ExtractionConfig

extraction_config = ExtractionConfig(
    # å¯ç”¨çš„æå–å™¨
    enable_text=True,
    enable_links=True,
    enable_images=True,
    enable_data=True,
    enable_form=True,
    
    # å…¨å±€é€‰æ‹©å™¨ï¼ˆå½±å“æ‰€æœ‰æå–å™¨ï¼‰
    css_selector="main",          # CSSé€‰æ‹©å™¨èŒƒå›´é™åˆ¶
    xpath_selector="//main",      # XPathé€‰æ‹©å™¨èŒƒå›´é™åˆ¶
    
    # å„æå–å™¨ä¸“ç”¨é…ç½®
    text_config={
        "min_text_length": 10,
        "extract_metadata": True,
        "clean_content": True
    },
    links_config={
        "include_internal": True,
        "include_external": True,
        "max_items": 100,
        "filter_patterns": ["*.pdf", "*.zip"]
    },
    images_config={
        "min_width": 50,
        "min_height": 50,
        "max_items": 50,
        "allowed_formats": ["jpg", "png", "gif", "webp"]
    },
    data_config={
        "extract_json_ld": True,
        "extract_microdata": True,
        "extract_tables": True
    },
    form_config={
        "extract_input_fields": True,
        "extract_buttons": True,
        "include_hidden_fields": False
    }
)
```

### LLMConfigï¼ˆLLMé…ç½®ï¼‰

```python
from xpidy import LLMConfig

llm_config = LLMConfig(
    enabled=True,                 # å¯ç”¨LLMå¤„ç†
    provider="openai",            # æä¾›å•†: openai/anthropic
    model="gpt-3.5-turbo",        # æ¨¡å‹åç§°
    api_key="your-api-key",       # APIå¯†é’¥
    temperature=0.1,              # æ¸©åº¦å‚æ•°
    max_tokens=2000               # æœ€å¤§ä»¤ç‰Œæ•°
)
```

## ğŸ§ª å¼€å‘ç¯å¢ƒ

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate  # Linux/Mac
# æˆ–
.venv\Scripts\activate     # Windows

# è¿è¡Œæµ‹è¯•
uv run pytest

# è¿è¡Œç¤ºä¾‹æµ‹è¯•
uv run python test_refactored.py

# æµ‹è¯•CLIå·¥å…·
uv run xpidy init basic --output test_config.json
uv run xpidy validate test_config.json
uv run xpidy run test_config.json

# ä»£ç æ ¼å¼åŒ–
uvx isort .
uvx black .
```

## ğŸ“Š æ€§èƒ½ç‰¹æ€§

### å¹¶å‘æå–

æ‰€æœ‰æå–å™¨åœ¨çˆ¬å–è¿‡ç¨‹ä¸­å¹¶å‘æ‰§è¡Œï¼š

```python
# å†…éƒ¨å®ç°ä¼ªä»£ç 
async def crawl(self, url):
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = []
    if self.config.extraction_config.enable_text:
        tasks.append(self._extractors['text'].extract(page))
    if self.config.extraction_config.enable_links:
        tasks.append(self._extractors['links'].extract(page))
    if self.config.extraction_config.enable_images:
        tasks.append(self._extractors['images'].extract(page))
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æå–å™¨
    results = await asyncio.gather(*tasks, return_exceptions=True)
```

### æ™ºèƒ½ç¼“å­˜

æ¯ä¸ªæå–å™¨å†…éƒ¨ç»´æŠ¤ç»“æœç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—ï¼š

```python
# è‡ªåŠ¨ç¼“å­˜æœºåˆ¶
class TextExtractor(BaseExtractor):
    async def extract(self, page):
        if self._cached_result:
            return self._cached_result
        
        result = await self._do_extract(page)
        self._cached_result = result  # è‡ªåŠ¨ç¼“å­˜
        return result
```

### é”™è¯¯éš”ç¦»

å•ä¸ªæå–å™¨å¤±è´¥ä¸å½±å“å…¶ä»–æå–å™¨ï¼š

```python
# ä¼˜é›…é”™è¯¯å¤„ç†
results = await asyncio.gather(*tasks, return_exceptions=True)
for extractor_name, result in zip(enabled_extractors, results):
    if isinstance(result, Exception):
        logger.warning(f"æå–å™¨ {extractor_name} å¤±è´¥: {result}")
        # ç»§ç»­å¤„ç†å…¶ä»–æå–å™¨ç»“æœ
```

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

---

**Xpidy - è®©ç½‘é¡µæ•°æ®æå–å˜å¾—ç®€å•è€Œå¼ºå¤§ï¼** ğŸ•·ï¸âœ¨
