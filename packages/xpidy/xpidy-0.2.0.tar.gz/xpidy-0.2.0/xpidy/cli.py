"""
Xpidy é…ç½®é©±åŠ¨çš„å‘½ä»¤è¡Œå·¥å…·
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import Optional

import click

from . import Spider, XpidyConfig
from .utils import URLUtils


@click.group()
@click.version_option(version="0.2.0")
def cli():
    """Xpidy - é…ç½®é©±åŠ¨çš„æ™ºèƒ½çˆ¬è™«å·¥å…·

    åŸºäº"é…ç½®å³æ–‡æ¡£"çš„è®¾è®¡ç†å¿µï¼Œé€šè¿‡é…ç½®æ–‡ä»¶å®šä¹‰çˆ¬å–ä»»åŠ¡ã€‚
    """
    pass


@cli.command()
@click.argument("config_file")
@click.option("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
@click.option("--dry-run", is_flag=True, help="é¢„è§ˆé…ç½®è€Œä¸æ‰§è¡Œ")
def run(config_file: str, output: Optional[str], dry_run: bool):
    """ä½¿ç”¨é…ç½®æ–‡ä»¶æ‰§è¡Œçˆ¬å–ä»»åŠ¡

    ç¤ºä¾‹é…ç½®æ–‡ä»¶ï¼š

    {
      "spider_config": {
        "headless": true,
        "timeout": 30000
      },
      "extraction_config": {
        "enable_text": true,
        "enable_links": true,
        "enable_images": true,
        "text_config": {"min_text_length": 10},
        "links_config": {"max_items": 20}
      },
      "tasks": [
        {
          "url": "https://example.com",
          "name": "example_site"
        }
      ]
    }
    """

    async def run_task():
        try:
            # è¯»å–é…ç½®æ–‡ä»¶
            config_path = Path(config_file)
            if not config_path.exists():
                click.echo(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}", err=True)
                sys.exit(1)

            with open(config_path, "r", encoding="utf-8") as f:
                config_data = json.load(f)

            # è§£æé…ç½® - ä½¿ç”¨æ–°çš„ç»Ÿä¸€é…ç½®
            try:
                config = XpidyConfig.from_dict(config_data)
            except Exception as e:
                click.echo(f"âŒ é…ç½®è§£æå¤±è´¥: {e}", err=True)
                sys.exit(1)

            tasks = config_data.get("tasks", [])

            if dry_run:
                click.echo("ğŸ” é…ç½®é¢„è§ˆ:")
                click.echo(f"  çˆ¬è™«é…ç½®: {config.spider_config}")
                click.echo(f"  æå–é…ç½®: {config.extraction_config}")
                if config.llm_config.enabled:
                    click.echo(f"  LLMé…ç½®: {config.llm_config}")
                click.echo(f"  ä»»åŠ¡æ•°é‡: {len(tasks)}")
                return

            if not tasks:
                click.echo("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰ä»»åŠ¡", err=True)
                sys.exit(1)

            # æ‰§è¡Œä»»åŠ¡
            click.echo(f"ğŸš€ å¼€å§‹æ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡")

            async with Spider(config) as spider:
                results = {}

                for i, task in enumerate(tasks, 1):
                    url = task["url"]
                    name = task.get("name", f"task_{i}")

                    try:
                        click.echo(f"ğŸ“¥ ({i}/{len(tasks)}) å¤„ç†: {name} - {url}")

                        # ä»»åŠ¡çº§åˆ«çš„é…ç½®è¦†ç›–
                        task_options = task.get("options", {})
                        prompt = task_options.get("prompt")

                        result = await spider.crawl(url, prompt=prompt)

                        # åˆ¤æ–­æˆåŠŸçŠ¶æ€
                        success = "error" not in result

                        results[name] = {
                            "url": url,
                            "success": success,
                            "data": result,
                        }

                        if success:
                            click.echo(f"âœ… å®Œæˆ: {name}")
                        else:
                            click.echo(f"âš ï¸ éƒ¨åˆ†å®Œæˆ: {name}")

                    except Exception as e:
                        click.echo(f"âŒ å¤±è´¥: {name} - {e}")
                        results[name] = {"url": url, "success": False, "error": str(e)}

                # ä¿å­˜ç»“æœ
                if output:
                    with open(output, "w", encoding="utf-8") as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                    click.echo(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output}")
                else:
                    click.echo(json.dumps(results, ensure_ascii=False, indent=2))

                # æ˜¾ç¤ºæ€»ç»“
                successful = sum(1 for r in results.values() if r.get("success", False))
                click.echo(f"\nğŸ“Š æ‰§è¡Œæ€»ç»“: æˆåŠŸ {successful}/{len(tasks)} ä¸ªä»»åŠ¡")

        except Exception as e:
            click.echo(f"âŒ æ‰§è¡Œå¤±è´¥: {e}", err=True)
            sys.exit(1)

    asyncio.run(run_task())


@cli.command()
@click.argument(
    "template_name",
    type=click.Choice(["basic", "links", "images", "comprehensive", "data", "form"]),
)
@click.option("--output", "-o", default="xpidy_config.json", help="é…ç½®æ–‡ä»¶è¾“å‡ºè·¯å¾„")
def init(template_name: str, output: str):
    """ç”Ÿæˆé…ç½®æ–‡ä»¶æ¨¡æ¿

    å¯ç”¨æ¨¡æ¿:
    - basic: åŸºç¡€æ–‡æœ¬æå–
    - links: é“¾æ¥æå–
    - images: å›¾ç‰‡æå–
    - comprehensive: å…¨é¢æå–
    - data: ç»“æ„åŒ–æ•°æ®æå–
    - form: è¡¨å•æ•°æ®æå–
    """
    templates = {
        "basic": {
            "spider_config": {"headless": True, "timeout": 30000},
            "extraction_config": {
                "enable_text": True,
                "text_config": {"extract_metadata": True, "min_text_length": 10},
            },
            "tasks": [{"url": "https://example.com", "name": "example_basic"}],
        },
        "links": {
            "spider_config": {"headless": True, "timeout": 30000},
            "extraction_config": {
                "enable_text": True,
                "enable_links": True,
                "links_config": {
                    "include_internal": True,
                    "include_external": True,
                    "max_items": 50,
                },
            },
            "tasks": [{"url": "https://example.com", "name": "example_links"}],
        },
        "images": {
            "spider_config": {"headless": True, "timeout": 30000},
            "extraction_config": {
                "enable_text": True,
                "enable_images": True,
                "images_config": {
                    "min_width": 100,
                    "min_height": 100,
                    "max_items": 20,
                    "allowed_formats": ["jpg", "png", "gif"],
                },
            },
            "tasks": [{"url": "https://example.com", "name": "example_images"}],
        },
        "comprehensive": {
            "spider_config": {"headless": True, "timeout": 30000},
            "extraction_config": {
                "enable_text": True,
                "enable_links": True,
                "enable_images": True,
                "enable_data": True,
                "enable_form": True,
                "text_config": {"extract_metadata": True},
                "links_config": {"max_items": 100},
                "images_config": {"max_items": 30},
                "data_config": {"extract_json_ld": True, "extract_tables": True},
                "form_config": {"extract_input_fields": True},
            },
            "tasks": [
                {"url": "https://example.com", "name": "comprehensive_extraction"}
            ],
        },
        "data": {
            "spider_config": {"headless": True, "timeout": 30000},
            "extraction_config": {
                "enable_text": True,
                "enable_data": True,
                "data_config": {
                    "extract_json_ld": True,
                    "extract_microdata": True,
                    "extract_opengraph": True,
                    "extract_tables": True,
                    "extract_lists": True,
                },
            },
            "tasks": [{"url": "https://example.com", "name": "data_extraction"}],
        },
        "form": {
            "spider_config": {"headless": True, "timeout": 30000},
            "extraction_config": {
                "enable_text": True,
                "enable_form": True,
                "form_config": {
                    "extract_input_fields": True,
                    "extract_buttons": True,
                    "extract_selects": True,
                    "include_hidden_fields": False,
                },
            },
            "tasks": [{"url": "https://example.com", "name": "form_extraction"}],
        },
    }

    template = templates.get(template_name)
    if not template:
        click.echo(f"âŒ æœªçŸ¥æ¨¡æ¿: {template_name}", err=True)
        sys.exit(1)

    # ä¿å­˜æ¨¡æ¿
    with open(output, "w", encoding="utf-8") as f:
        json.dump(template, f, ensure_ascii=False, indent=2)

    click.echo(f"âœ… å·²ç”Ÿæˆ {template_name} é…ç½®æ¨¡æ¿: {output}")
    click.echo(f"ğŸ”§ è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶åä½¿ç”¨: xpidy run {output}")


@cli.command()
@click.argument("config_file")
def validate(config_file: str):
    """éªŒè¯é…ç½®æ–‡ä»¶"""
    try:
        config_path = Path(config_file)
        if not config_path.exists():
            click.echo(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}", err=True)
            sys.exit(1)

        with open(config_path, "r", encoding="utf-8") as f:
            config_data = json.load(f)

        # éªŒè¯é…ç½®ç»“æ„
        errors = []

        try:
            config = XpidyConfig.from_dict(config_data)
        except Exception as e:
            errors.append(f"é…ç½®æ ¼å¼é”™è¯¯: {e}")

        # éªŒè¯tasks
        tasks = config_data.get("tasks", [])
        if not tasks:
            errors.append("tasks ä¸èƒ½ä¸ºç©º")

        for i, task in enumerate(tasks):
            if "url" not in task:
                errors.append(f"ä»»åŠ¡ {i+1} ç¼ºå°‘ url å­—æ®µ")
            elif not URLUtils.is_valid_url(task["url"]):
                errors.append(f"ä»»åŠ¡ {i+1} çš„ URL æ— æ•ˆ: {task['url']}")

        if errors:
            click.echo("âŒ é…ç½®éªŒè¯å¤±è´¥:")
            for error in errors:
                click.echo(f"  - {error}")
            sys.exit(1)
        else:
            click.echo("âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")
            click.echo(f"ğŸ“Š åŒ…å« {len(tasks)} ä¸ªä»»åŠ¡")

            # æ˜¾ç¤ºå¯ç”¨çš„æå–å™¨
            enabled_extractors = []
            if config.extraction_config.enable_text:
                enabled_extractors.append("text")
            if config.extraction_config.enable_links:
                enabled_extractors.append("links")
            if config.extraction_config.enable_images:
                enabled_extractors.append("images")
            if config.extraction_config.enable_data:
                enabled_extractors.append("data")
            if config.extraction_config.enable_form:
                enabled_extractors.append("form")

            click.echo(f"ğŸ”§ å¯ç”¨çš„æå–å™¨: {', '.join(enabled_extractors)}")

    except json.JSONDecodeError as e:
        click.echo(f"âŒ JSONæ ¼å¼é”™è¯¯: {e}", err=True)
        sys.exit(1)
    except Exception as e:
        click.echo(f"âŒ éªŒè¯å¤±è´¥: {e}", err=True)
        sys.exit(1)


@cli.command()
@click.argument("url")
@click.option("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
@click.option("--enable-links", is_flag=True, help="å¯ç”¨é“¾æ¥æå–")
@click.option("--enable-images", is_flag=True, help="å¯ç”¨å›¾ç‰‡æå–")
@click.option("--enable-data", is_flag=True, help="å¯ç”¨æ•°æ®æå–")
def quick(
    url: str,
    output: Optional[str],
    enable_links: bool,
    enable_images: bool,
    enable_data: bool,
):
    """å¿«é€Ÿçˆ¬å–å•ä¸ªURLï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰"""

    async def run_quick():
        try:
            click.echo(f"ğŸš€ å¿«é€Ÿçˆ¬å–: {url}")

            # åˆ›å»ºå¿«é€Ÿé…ç½®
            spider = Spider.quick_create(
                enable_text=True,
                enable_links=enable_links,
                enable_images=enable_images,
                enable_data=enable_data,
            )

            async with spider:
                result = await spider.crawl(url)

            # åˆ¤æ–­æˆåŠŸçŠ¶æ€
            success = "error" not in result

            output_data = json.dumps(result, ensure_ascii=False, indent=2)

            if output:
                with open(output, "w", encoding="utf-8") as f:
                    f.write(output_data)
                click.echo(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output}")
            else:
                click.echo(output_data)

            # æ˜¾ç¤ºæ‘˜è¦
            click.echo(f"\nğŸ“Š çˆ¬å–æ‘˜è¦:")
            click.echo(f"  æˆåŠŸ: {success}")
            click.echo(f"  æå–å™¨: {result.get('extractors_used', [])}")
            click.echo(f"  è€—æ—¶: {result.get('extraction_time', 0):.2f}ç§’")

            # æ˜¾ç¤ºå„æå–å™¨ç»“æœç»Ÿè®¡
            results = result.get("results", {})
            for extractor_name, extractor_result in results.items():
                if (
                    isinstance(extractor_result, dict)
                    and "error" not in extractor_result
                ):
                    if extractor_name == "text":
                        content_length = len(extractor_result.get("content", ""))
                        click.echo(f"  æ–‡æœ¬é•¿åº¦: {content_length}å­—ç¬¦")
                    elif extractor_name == "links":
                        link_count = extractor_result.get("total_links", 0)
                        click.echo(f"  é“¾æ¥æ•°é‡: {link_count}")
                    elif extractor_name == "images":
                        image_count = extractor_result.get("total_images", 0)
                        click.echo(f"  å›¾ç‰‡æ•°é‡: {image_count}")
                    elif extractor_name == "data":
                        stats = extractor_result.get("stats", {})
                        click.echo(f"  ç»“æ„åŒ–æ•°æ®: {stats}")

        except Exception as e:
            click.echo(f"âŒ å¿«é€Ÿçˆ¬å–å¤±è´¥: {e}", err=True)
            sys.exit(1)

    asyncio.run(run_quick())


@cli.command()
@click.argument("urls", nargs=-1)
def validate_urls(urls):
    """éªŒè¯URLçš„æœ‰æ•ˆæ€§"""
    if not urls:
        click.echo("è¯·æä¾›è‡³å°‘ä¸€ä¸ªURL")
        return

    click.echo("ğŸ” URLéªŒè¯ç»“æœ:")
    for url in urls:
        is_valid = URLUtils.is_valid_url(url)
        normalized = URLUtils.normalize_url(url) if is_valid else "æ— æ•ˆURL"
        domain = URLUtils.extract_domain(url) if is_valid else "N/A"

        status = "âœ…" if is_valid else "âŒ"
        click.echo(f"{status} {url}")
        click.echo(f"   æ ‡å‡†åŒ–: {normalized}")
        click.echo(f"   åŸŸå: {domain}")
        click.echo()


if __name__ == "__main__":
    cli()
