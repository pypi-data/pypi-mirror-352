#!/usr/bin/env python3
"""
ä¸­å¤®çºªå§”ç›‘å¯Ÿå§”ç½‘ç«™çˆ¬è™«ç¤ºä¾‹
ä½¿ç”¨Xpidyå†…ç½®å¹¶å‘åŠŸèƒ½ï¼Œç®€æ´é«˜æ•ˆåœ°çˆ¬å–è¿çºªå¤„åˆ†æ¡ˆä¾‹
"""

import asyncio
import csv
import re
import time
from typing import Any, Dict, List

from xpidy import ExtractionConfig, Spider, SpiderConfig, XpidyConfig


async def main():
    """ä¸»å‡½æ•°ï¼šçˆ¬å–ä¸­çºªå§”è¿çºªå¤„åˆ†æ¡ˆä¾‹"""

    # ğŸ¯ ç›®æ ‡é¡µé¢
    urls = [
        "https://www.ccdi.gov.cn/scdcn/sggb/djcf/",  # çœç®¡å¹²éƒ¨è¿çºªå¤„åˆ†
        "https://www.ccdi.gov.cn/scdcn/zyyj/djcf/",  # ä¸­å¤®ä¸€çº§è¿çºªå¤„åˆ†
        "https://www.ccdi.gov.cn/scdcn/zggb/djcf/",  # ä¸­ç®¡å¹²éƒ¨è¿çºªå¤„åˆ†
    ]

    # âš™ï¸ é…ç½®çˆ¬è™«
    config = XpidyConfig(
        spider_config=SpiderConfig(
            headless=True,
            timeout=30000,
            delay=1.0,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        ),
        extraction_config=ExtractionConfig(
            enable_text=True,
            enable_links=True,
            enable_data=False,
            # ä¸“é—¨æå–liå…ƒç´ å†…å®¹
            global_selectors=["li"],
            global_exclude_selectors=["nav", "header", "footer"],
            text_config={
                "content_selectors": ["li", "li a", "li span"],
                "min_text_length": 5,
                "remove_scripts": True,
                "remove_styles": True,
            },
            links_config={
                "extract_internal_links": True,
                "extract_external_links": False,
                "include_link_text": True,
                "deduplicate": True,
                "filter_patterns": [r"\.html$", r"\.shtml$"],
                "base_domain": "ccdi.gov.cn",
            },
        ),
    )

    print("ğŸš€ å¼€å§‹çˆ¬å–ä¸­å¤®çºªå§”ç›‘å¯Ÿå§”ç½‘ç«™...")
    print(f"ğŸ“ ç›®æ ‡é¡µé¢: {len(urls)} ä¸ª")
    print("âš¡ ä½¿ç”¨Xpidyå†…ç½®å¹¶å‘åŠŸèƒ½\n")

    start_time = time.time()

    # ğŸ”¥ æ ¸å¿ƒä»£ç ï¼šä¸€è¡Œå®ç°å¹¶å‘çˆ¬å–
    async with Spider(config) as spider:
        results = await spider.crawl_multiple_urls(
            urls=urls,
            max_concurrent=3,  # æœ€å¤§å¹¶å‘æ•°
            delay_between_batches=0.5,  # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        )

    end_time = time.time()

    # ğŸ“Š å¤„ç†ç»“æœ
    all_cases = []
    for result in results:
        if "error" not in result:
            page_type = get_page_type(result["url"])
            cases = extract_cases_from_result(result)

            for case in cases:
                case["é¡µé¢ç±»å‹"] = page_type
                all_cases.append(case)

    # ğŸ’¾ ä¿å­˜ä¸ºCSV
    save_to_csv(all_cases, "ccdi_cases.csv")

    # ğŸ“ˆ æ˜¾ç¤ºç»Ÿè®¡
    print_summary(results, all_cases, end_time - start_time)


def get_page_type(url: str) -> str:
    """æ ¹æ®URLåˆ¤æ–­é¡µé¢ç±»å‹"""
    if "sggb" in url:
        return "çœç®¡å¹²éƒ¨è¿çºªå¤„åˆ†"
    elif "zyyj" in url:
        return "ä¸­å¤®ä¸€çº§è¿çºªå¤„åˆ†"
    elif "zggb" in url:
        return "ä¸­ç®¡å¹²éƒ¨è¿çºªå¤„åˆ†"
    else:
        return "æœªçŸ¥ç±»å‹"


def extract_cases_from_result(result: Dict[str, Any]) -> List[Dict[str, str]]:
    """ä»çˆ¬å–ç»“æœä¸­æå–æ¡ˆä¾‹ä¿¡æ¯"""
    cases = []

    # è·å–æ–‡æœ¬å†…å®¹å’Œé“¾æ¥
    text_content = result.get("results", {}).get("text", {}).get("content", "")
    links_data = result.get("results", {}).get("links", {}).get("links", [])

    if not text_content:
        return cases

    # åˆ›å»ºé“¾æ¥æ˜ å°„
    link_map = {}
    for link in links_data:
        if link.get("text") and link.get("url"):
            link_url = link["url"]
            link_text = link["text"].strip()

            # æ‰©å¤§é“¾æ¥èŒƒå›´ï¼šåŒ…å«æ›´å¤šç±»å‹çš„é“¾æ¥
            if any(
                pattern in link_url
                for pattern in ["yaowenn", "scdcn", "toutiao", "djcf"]
            ):
                full_url = (
                    f"https://www.ccdi.gov.cn{link_url}"
                    if not link_url.startswith("http")
                    else link_url
                )
                link_map[link_text] = full_url

    # è§£ææ–‡æœ¬å†…å®¹ - æ–°çš„è§£æé€»è¾‘
    lines = text_content.split("\n")

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # åŒ¹é…åŒ…å«è¿çºªå¤„åˆ†å…³é”®è¯çš„è¡Œ
        if any(
            keyword in line
            for keyword in ["è¢«å¼€é™¤å…šç±", 'è¢«"åŒå¼€"', "è¢«å–æ¶ˆ", "è¢«æ’¤é”€"]
        ):
            # å°è¯•ä»è¡Œæœ«æå–æ—¥æœŸ
            date_match = re.search(r"(\d{4}-\d{2}-\d{2})$", line)
            if date_match:
                date = date_match.group(1)
                # ç§»é™¤æ—¥æœŸéƒ¨åˆ†å¾—åˆ°æ ‡é¢˜
                title = line[: date_match.start()].strip()

                if title:  # ç¡®ä¿æ ‡é¢˜ä¸ä¸ºç©º
                    name = extract_name_from_title(title)

                    # æŸ¥æ‰¾åŒ¹é…çš„é“¾æ¥ - æ”¹è¿›åŒ¹é…ç®—æ³•
                    link = ""
                    best_match_score = 0

                    for link_text, link_url in link_map.items():
                        match_score = 0

                        # 1. å®Œå…¨åŒ¹é…æ ‡é¢˜
                        if title == link_text:
                            match_score = 100
                        # 2. æ ‡é¢˜åŒ…å«é“¾æ¥æ–‡æœ¬
                        elif link_text in title:
                            match_score = 80
                        # 3. é“¾æ¥æ–‡æœ¬åŒ…å«æ ‡é¢˜
                        elif title in link_text:
                            match_score = 70
                        # 4. äººååŒ¹é…
                        elif name != "æœªçŸ¥" and name in link_text:
                            match_score = 60
                        # 5. å…³é”®è¯åŒ¹é…ï¼ˆè¿çºªã€å¼€é™¤ç­‰ï¼‰
                        elif any(
                            keyword in link_text for keyword in ["è¿çºª", "å¼€é™¤", "å…šç±"]
                        ):
                            # æ£€æŸ¥æ˜¯å¦æœ‰å…±åŒçš„å…³é”®è¯
                            title_words = set(title.split())
                            link_words = set(link_text.split())
                            common_words = title_words & link_words
                            if len(common_words) >= 2:  # è‡³å°‘2ä¸ªå…±åŒè¯
                                match_score = 40

                        # æ›´æ–°æœ€ä½³åŒ¹é…
                        if match_score > best_match_score:
                            best_match_score = match_score
                            link = link_url

                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥½çš„åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                    if best_match_score < 40:
                        for link_text, link_url in link_map.items():
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸åŒçš„ä¸­æ–‡å§“å
                            if name != "æœªçŸ¥" and len(name) >= 2:
                                if name in link_text:
                                    link = link_url
                                    break

                    case = {"æ ‡é¢˜": title, "æ—¶é—´": date, "äººå": name, "é“¾æ¥": link}
                    cases.append(case)

    return cases


def extract_name_from_title(title: str) -> str:
    """ä»æ ‡é¢˜ä¸­æå–äººå - åŒ¹é…æœ€åä¸€ä¸ªèŒåŠ¡åˆ°ä¸¥é‡/è¢«ä¹‹é—´çš„å†…å®¹"""

    # èŒåŠ¡è¯æ±‡åˆ—è¡¨ï¼ˆæŒ‰é•¿åº¦æ’åºï¼Œé•¿çš„ä¼˜å…ˆåŒ¹é…ï¼‰
    job_titles = [
        "æ€»ä¼šè®¡å¸ˆ",
        "æ€»å·¥ç¨‹å¸ˆ",
        "æ€»ç»æµå¸ˆ",
        "æ€»æ³•å¾‹é¡¾é—®",
        "æ€»å®¡è®¡å¸ˆ",
        "æ€»è§„åˆ’å¸ˆ",
        "æ€»è®¾è®¡å¸ˆ",
        "æ€»æŒ‡æŒ¥",
        "æ€»ç¼–è¾‘",
        "æ€»è£åˆ¤",
        "(ä¸»æŒå·¥ä½œ)",
        "ç§˜ä¹¦é•¿",
        "ä¹¦è®°",
        "ç»„é•¿",
        "ä¸»ä»»",
        "å§”å‘˜",
        "å¸¸å§”",
        "å…é•¿",
        "å±€é•¿",
        "é™¢é•¿",
        "è‘£äº‹",
        "ç»ç†",
        "ä¸»å¸­",
        "éƒ¨é•¿",
        "çœé•¿",
        "å¸‚é•¿",
        "å¿é•¿",
        "é•‡é•¿",
        "ç§˜ä¹¦",
        "è§†å‘˜",
        "æ£€å¯Ÿé•¿",
        "å…³é•¿",
        "è¡Œé•¿",
        "ç½²é•¿",
        "æ€»ç»ç†",
        "è‘£äº‹é•¿",
        "å·¡è§†å‘˜",
        "ç£å¯Ÿå‘˜",
        "å‚äº‹",
        "åŠ©ç†",
        "é¡¾é—®",
        "ç†äº‹",
        "ç›‘äº‹",
        "æ€»è£",
        "æ€»ç›‘",
    ]

    # æ‰¾åˆ°æ‰€æœ‰èŒåŠ¡è¯æ±‡çš„ä½ç½®
    job_positions = []
    for job in job_titles:
        start_pos = 0
        while True:
            pos = title.find(job, start_pos)
            if pos == -1:
                break
            # æ£€æŸ¥æ˜¯å¦è¢«æ›´é•¿çš„èŒåŠ¡è¯æ±‡åŒ…å«
            is_part_of_longer = False
            for longer_job in job_titles:
                if len(longer_job) > len(job) and job in longer_job:
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ›´é•¿çš„åŒ¹é…
                    longer_pos = title.find(
                        longer_job, max(0, pos - len(longer_job) + len(job))
                    )
                    if longer_pos != -1 and longer_pos <= pos < longer_pos + len(
                        longer_job
                    ):
                        is_part_of_longer = True
                        break

            if not is_part_of_longer:
                job_positions.append((pos, pos + len(job), job))
            start_pos = pos + 1

    if not job_positions:
        return "æœªçŸ¥"

    # å–æœ€åä¸€ä¸ªèŒåŠ¡çš„ç»“æŸä½ç½®
    last_job_end = max(job_positions, key=lambda x: x[0])[1]

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ª"ä¸¥é‡"æˆ–"è¢«"çš„ä½ç½®
    serious_pos = title.find("ä¸¥é‡", last_job_end)
    bei_pos = title.find("è¢«", last_job_end)

    # å–æœ€æ—©å‡ºç°çš„ä½ç½®
    end_pos = float("inf")
    if serious_pos != -1:
        end_pos = min(end_pos, serious_pos)
    if bei_pos != -1:
        end_pos = min(end_pos, bei_pos)

    if end_pos == float("inf"):
        return "æœªçŸ¥"

    # æå–èŒåŠ¡ç»“æŸåˆ°å…³é”®è¯ä¹‹é—´çš„å†…å®¹å¹¶æ¸…ç†
    content = title[last_job_end:end_pos].strip()

    # ç®€å•æ¸…ç†ï¼šå»æ‰å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
    content = re.sub(r'[ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿ""' "ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]", "", content)

    return content if content else "æœªçŸ¥"


def save_to_csv(cases: List[Dict[str, str]], filename: str):
    """ä¿å­˜æ¡ˆä¾‹åˆ°CSVæ–‡ä»¶"""
    if not cases:
        print("âš ï¸  æ²¡æœ‰æå–åˆ°æ¡ˆä¾‹æ•°æ®")
        return

    with open(filename, "w", encoding="utf-8", newline="") as f:
        fieldnames = ["é¡µé¢ç±»å‹", "æ ‡é¢˜", "æ—¶é—´", "äººå", "é“¾æ¥"]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(cases)

    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")


def print_summary(results: List[Dict], cases: List[Dict], total_time: float):
    """æ‰“å°çˆ¬å–æ‘˜è¦"""
    success_count = len([r for r in results if "error" not in r])
    total_urls = len(results)

    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»“æœæ‘˜è¦")
    print("=" * 60)
    print(f"âœ… æˆåŠŸé¡µé¢: {success_count}/{total_urls}")
    print(f"ğŸ“‹ æå–æ¡ˆä¾‹: {len(cases)} ä¸ª")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
    print(f"âš¡ å¹³å‡é€Ÿåº¦: {len(results)/total_time:.1f} é¡µé¢/ç§’")

    # æŒ‰é¡µé¢ç±»å‹ç»Ÿè®¡
    type_stats = {}
    for case in cases:
        page_type = case["é¡µé¢ç±»å‹"]
        type_stats[page_type] = type_stats.get(page_type, 0) + 1

    print(f"\nğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
    for page_type, count in type_stats.items():
        print(f"   {page_type}: {count} ä¸ªæ¡ˆä¾‹")

    print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼æ•°æ®å·²ä¿å­˜ä¸ºCSVæ ¼å¼")


if __name__ == "__main__":
    asyncio.run(main())
