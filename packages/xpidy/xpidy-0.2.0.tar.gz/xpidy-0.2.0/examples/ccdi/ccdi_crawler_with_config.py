#!/usr/bin/env python3
"""
ä¸­å¤®çºªå§”ç›‘å¯Ÿå§”ç½‘ç«™çˆ¬è™«ç¤ºä¾‹ - é…ç½®æ–‡ä»¶ç‰ˆæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨é…ç½®æ–‡ä»¶æ¥ç®¡ç†çˆ¬è™«è®¾ç½®
"""

import asyncio
import csv
import json
import re
import time
from pathlib import Path
from typing import Any, Dict, List

from xpidy import ExtractionConfig, Spider, SpiderConfig, XpidyConfig


def load_config(config_path: str = "ccdi_config.json") -> tuple:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = Path(__file__).parent / config_path

    with open(config_file, "r", encoding="utf-8") as f:
        config_data = json.load(f)

    # æ„å»ºXpidyé…ç½®
    xpidy_config = XpidyConfig(
        spider_config=SpiderConfig(**config_data["spider_config"]),
        extraction_config=ExtractionConfig(**config_data["extraction_config"]),
    )

    return xpidy_config, config_data


async def main():
    """ä¸»å‡½æ•°ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶çˆ¬å–ä¸­çºªå§”æ¡ˆä¾‹"""

    # ğŸ“ åŠ è½½é…ç½®
    config, config_data = load_config()
    concurrent_config = config_data.get("concurrent_config", {})
    output_config = config_data.get("output_config", {})

    # ğŸ¯ ç›®æ ‡é¡µé¢
    urls = [
        "https://www.ccdi.gov.cn/scdcn/sggb/djcf/",  # çœç®¡å¹²éƒ¨è¿çºªå¤„åˆ†
        "https://www.ccdi.gov.cn/scdcn/zyyj/djcf/",  # ä¸­å¤®ä¸€çº§è¿çºªå¤„åˆ†
        "https://www.ccdi.gov.cn/scdcn/zggb/djcf/",  # ä¸­ç®¡å¹²éƒ¨è¿çºªå¤„åˆ†
    ]

    print("ğŸš€ å¼€å§‹çˆ¬å–ä¸­å¤®çºªå§”ç›‘å¯Ÿå§”ç½‘ç«™...")
    print(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: ccdi_config.json")
    print(f"ğŸ“ ç›®æ ‡é¡µé¢: {len(urls)} ä¸ª")
    print(f"âš¡ å¹¶å‘æ•°: {concurrent_config.get('max_concurrent', 3)}")
    print()

    start_time = time.time()

    # ğŸ”¥ ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å¹¶å‘è®¾ç½®
    async with Spider(config) as spider:
        results = await spider.crawl_multiple_urls(
            urls=urls,
            max_concurrent=concurrent_config.get("max_concurrent", 3),
            delay_between_batches=concurrent_config.get("delay_between_batches", 0.5),
        )

    end_time = time.time()

    # ğŸ“Š å¤„ç†ç»“æœ
    all_cases = []
    for result in results:
        if "error" not in result:
            page_type = get_page_type(result["url"])
            cases = extract_cases_from_result(result)

            for case in cases:
                case["é¡µé¢ç±»å‹"] = page_type
                all_cases.append(case)

    # ğŸ’¾ ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è¾“å‡ºè®¾ç½®
    csv_filename = output_config.get("csv_filename", "ccdi_cases.csv")
    save_to_csv(all_cases, csv_filename, output_config)

    # ğŸ“ˆ æ˜¾ç¤ºç»Ÿè®¡
    print_summary(results, all_cases, end_time - start_time)


def get_page_type(url: str) -> str:
    """æ ¹æ®URLåˆ¤æ–­é¡µé¢ç±»å‹"""
    if "sggb" in url:
        return "çœç®¡å¹²éƒ¨è¿çºªå¤„åˆ†"
    elif "zyyj" in url:
        return "ä¸­å¤®ä¸€çº§è¿çºªå¤„åˆ†"
    elif "zggb" in url:
        return "ä¸­ç®¡å¹²éƒ¨è¿çºªå¤„åˆ†"
    else:
        return "æœªçŸ¥ç±»å‹"


def extract_cases_from_result(result: Dict[str, Any]) -> List[Dict[str, str]]:
    """ä»çˆ¬å–ç»“æœä¸­æå–æ¡ˆä¾‹ä¿¡æ¯"""
    cases = []

    # è·å–æ–‡æœ¬å†…å®¹å’Œé“¾æ¥
    text_content = result.get("results", {}).get("text", {}).get("content", "")
    links_data = result.get("results", {}).get("links", {}).get("links", [])

    if not text_content:
        return cases

    # åˆ›å»ºé“¾æ¥æ˜ å°„
    link_map = {}
    for link in links_data:
        if link.get("text") and link.get("url"):
            link_url = link["url"]
            link_text = link["text"].strip()

            # æ‰©å¤§é“¾æ¥èŒƒå›´ï¼šåŒ…å«æ›´å¤šç±»å‹çš„é“¾æ¥
            if any(
                pattern in link_url
                for pattern in ["yaowenn", "scdcn", "toutiao", "djcf"]
            ):
                full_url = (
                    f"https://www.ccdi.gov.cn{link_url}"
                    if not link_url.startswith("http")
                    else link_url
                )
                link_map[link_text] = full_url

    # è§£ææ–‡æœ¬å†…å®¹ - æ–°çš„è§£æé€»è¾‘
    lines = text_content.split("\n")

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # åŒ¹é…åŒ…å«è¿çºªå¤„åˆ†å…³é”®è¯çš„è¡Œ
        if any(
            keyword in line
            for keyword in ["è¢«å¼€é™¤å…šç±", 'è¢«"åŒå¼€"', "è¢«å–æ¶ˆ", "è¢«æ’¤é”€"]
        ):
            # å°è¯•ä»è¡Œæœ«æå–æ—¥æœŸ
            date_match = re.search(r"(\d{4}-\d{2}-\d{2})$", line)
            if date_match:
                date = date_match.group(1)
                # ç§»é™¤æ—¥æœŸéƒ¨åˆ†å¾—åˆ°æ ‡é¢˜
                title = line[: date_match.start()].strip()

                if title:  # ç¡®ä¿æ ‡é¢˜ä¸ä¸ºç©º
                    name = extract_name_from_title(title)

                    # æŸ¥æ‰¾åŒ¹é…çš„é“¾æ¥ - æ”¹è¿›åŒ¹é…ç®—æ³•
                    link = ""
                    best_match_score = 0

                    for link_text, link_url in link_map.items():
                        match_score = 0

                        # 1. å®Œå…¨åŒ¹é…æ ‡é¢˜
                        if title == link_text:
                            match_score = 100
                        # 2. æ ‡é¢˜åŒ…å«é“¾æ¥æ–‡æœ¬
                        elif link_text in title:
                            match_score = 80
                        # 3. é“¾æ¥æ–‡æœ¬åŒ…å«æ ‡é¢˜
                        elif title in link_text:
                            match_score = 70
                        # 4. äººååŒ¹é…
                        elif name != "æœªçŸ¥" and name in link_text:
                            match_score = 60
                        # 5. å…³é”®è¯åŒ¹é…ï¼ˆè¿çºªã€å¼€é™¤ç­‰ï¼‰
                        elif any(
                            keyword in link_text for keyword in ["è¿çºª", "å¼€é™¤", "å…šç±"]
                        ):
                            # æ£€æŸ¥æ˜¯å¦æœ‰å…±åŒçš„å…³é”®è¯
                            title_words = set(title.split())
                            link_words = set(link_text.split())
                            common_words = title_words & link_words
                            if len(common_words) >= 2:  # è‡³å°‘2ä¸ªå…±åŒè¯
                                match_score = 40

                        # æ›´æ–°æœ€ä½³åŒ¹é…
                        if match_score > best_match_score:
                            best_match_score = match_score
                            link = link_url

                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥½çš„åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                    if best_match_score < 40:
                        for link_text, link_url in link_map.items():
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸åŒçš„ä¸­æ–‡å§“å
                            if name != "æœªçŸ¥" and len(name) >= 2:
                                if name in link_text:
                                    link = link_url
                                    break

                    case = {"æ ‡é¢˜": title, "æ—¶é—´": date, "äººå": name, "é“¾æ¥": link}
                    cases.append(case)

    return cases


def extract_name_from_title(title: str) -> str:
    """ä»æ ‡é¢˜ä¸­æå–äººå - åŒ¹é…æœ€åä¸€ä¸ªèŒåŠ¡åˆ°ä¸¥é‡/è¢«ä¹‹é—´çš„å†…å®¹"""

    # èŒåŠ¡è¯æ±‡åˆ—è¡¨ï¼ˆæŒ‰é•¿åº¦æ’åºï¼Œé•¿çš„ä¼˜å…ˆåŒ¹é…ï¼‰
    job_titles = [
        "æ€»ä¼šè®¡å¸ˆ",
        "æ€»å·¥ç¨‹å¸ˆ",
        "æ€»ç»æµå¸ˆ",
        "æ€»æ³•å¾‹é¡¾é—®",
        "æ€»å®¡è®¡å¸ˆ",
        "æ€»è§„åˆ’å¸ˆ",
        "æ€»è®¾è®¡å¸ˆ",
        "æ€»æŒ‡æŒ¥",
        "æ€»ç¼–è¾‘",
        "æ€»è£åˆ¤",
        "(ä¸»æŒå·¥ä½œ)",
        "ç§˜ä¹¦é•¿",
        "ä¹¦è®°",
        "ç»„é•¿",
        "ä¸»ä»»",
        "å§”å‘˜",
        "å¸¸å§”",
        "å…é•¿",
        "å±€é•¿",
        "é™¢é•¿",
        "è‘£äº‹",
        "ç»ç†",
        "ä¸»å¸­",
        "éƒ¨é•¿",
        "çœé•¿",
        "å¸‚é•¿",
        "å¿é•¿",
        "é•‡é•¿",
        "ç§˜ä¹¦",
        "è§†å‘˜",
        "æ£€å¯Ÿé•¿",
        "å…³é•¿",
        "è¡Œé•¿",
        "ç½²é•¿",
        "æ€»ç»ç†",
        "è‘£äº‹é•¿",
        "å·¡è§†å‘˜",
        "ç£å¯Ÿå‘˜",
        "å‚äº‹",
        "åŠ©ç†",
        "é¡¾é—®",
        "ç†äº‹",
        "ç›‘äº‹",
        "æ€»è£",
        "æ€»ç›‘",
    ]

    # æ‰¾åˆ°æ‰€æœ‰èŒåŠ¡è¯æ±‡çš„ä½ç½®
    job_positions = []
    for job in job_titles:
        start_pos = 0
        while True:
            pos = title.find(job, start_pos)
            if pos == -1:
                break
            # æ£€æŸ¥æ˜¯å¦è¢«æ›´é•¿çš„èŒåŠ¡è¯æ±‡åŒ…å«
            is_part_of_longer = False
            for longer_job in job_titles:
                if len(longer_job) > len(job) and job in longer_job:
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ›´é•¿çš„åŒ¹é…
                    longer_pos = title.find(
                        longer_job, max(0, pos - len(longer_job) + len(job))
                    )
                    if longer_pos != -1 and longer_pos <= pos < longer_pos + len(
                        longer_job
                    ):
                        is_part_of_longer = True
                        break

            if not is_part_of_longer:
                job_positions.append((pos, pos + len(job), job))
            start_pos = pos + 1

    if not job_positions:
        return "æœªçŸ¥"

    # å–æœ€åä¸€ä¸ªèŒåŠ¡çš„ç»“æŸä½ç½®
    last_job_end = max(job_positions, key=lambda x: x[0])[1]

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ª"ä¸¥é‡"æˆ–"è¢«"çš„ä½ç½®
    serious_pos = title.find("ä¸¥é‡", last_job_end)
    bei_pos = title.find("è¢«", last_job_end)

    # å–æœ€æ—©å‡ºç°çš„ä½ç½®
    end_pos = float("inf")
    if serious_pos != -1:
        end_pos = min(end_pos, serious_pos)
    if bei_pos != -1:
        end_pos = min(end_pos, bei_pos)

    if end_pos == float("inf"):
        return "æœªçŸ¥"

    # æå–èŒåŠ¡ç»“æŸåˆ°å…³é”®è¯ä¹‹é—´çš„å†…å®¹å¹¶æ¸…ç†
    content = title[last_job_end:end_pos].strip()

    # ç®€å•æ¸…ç†ï¼šå»æ‰å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
    content = re.sub(r'[ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿ""' "ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]", "", content)

    return content if content else "æœªçŸ¥"


def save_to_csv(cases: List[Dict[str, str]], filename: str, output_config: Dict):
    """ä¿å­˜æ¡ˆä¾‹åˆ°CSVæ–‡ä»¶"""
    if not cases:
        print("âš ï¸  æ²¡æœ‰æå–åˆ°æ¡ˆä¾‹æ•°æ®")
        return

    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å­—æ®µè®¾ç½®
    fieldnames = output_config.get(
        "include_fields", ["é¡µé¢ç±»å‹", "æ ‡é¢˜", "æ—¶é—´", "äººå", "é“¾æ¥"]
    )
    encoding = output_config.get("encoding", "utf-8")

    with open(filename, "w", encoding=encoding, newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        # åªå†™å…¥é…ç½®ä¸­æŒ‡å®šçš„å­—æ®µ
        filtered_cases = []
        for case in cases:
            filtered_case = {field: case.get(field, "") for field in fieldnames}
            filtered_cases.append(filtered_case)

        writer.writerows(filtered_cases)

    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
    print(f"ğŸ“‹ åŒ…å«å­—æ®µ: {', '.join(fieldnames)}")


def print_summary(results: List[Dict], cases: List[Dict], total_time: float):
    """æ‰“å°çˆ¬å–æ‘˜è¦"""
    success_count = len([r for r in results if "error" not in r])
    total_urls = len(results)

    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»“æœæ‘˜è¦")
    print("=" * 60)
    print(f"âœ… æˆåŠŸé¡µé¢: {success_count}/{total_urls}")
    print(f"ğŸ“‹ æå–æ¡ˆä¾‹: {len(cases)} ä¸ª")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
    print(f"âš¡ å¹³å‡é€Ÿåº¦: {len(results)/total_time:.1f} é¡µé¢/ç§’")

    # æŒ‰é¡µé¢ç±»å‹ç»Ÿè®¡
    type_stats = {}
    for case in cases:
        page_type = case["é¡µé¢ç±»å‹"]
        type_stats[page_type] = type_stats.get(page_type, 0) + 1

    print(f"\nğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
    for page_type, count in type_stats.items():
        print(f"   {page_type}: {count} ä¸ªæ¡ˆä¾‹")

    print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼æ•°æ®å·²ä¿å­˜ä¸ºCSVæ ¼å¼")


if __name__ == "__main__":
    asyncio.run(main())
