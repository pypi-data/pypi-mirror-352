{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 02. Learning LangGraph - Chat Executor"
   ],
   "metadata": {
    "id": "E5TwMbBvpk4K"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAoQEqlXGrWi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "02509019-f945-403b-a92a-2d376e592ac4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m806.2/806.2 kB\u001B[0m \u001B[31m8.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.4/44.4 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m173.7/173.7 kB\u001B[0m \u001B[31m14.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m47.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m252.4/252.4 kB\u001B[0m \u001B[31m23.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m64.2/64.2 kB\u001B[0m \u001B[31m6.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m227.4/227.4 kB\u001B[0m \u001B[31m21.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m58.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.4/49.4 kB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m138.5/138.5 kB\u001B[0m \u001B[31m11.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.6/75.6 kB\u001B[0m \u001B[31m6.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m77.8/77.8 kB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.3/58.3 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet -U langchain langchain_openai langgraph langchainhub langchain_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "modified from https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/base.ipynb"
   ],
   "metadata": {
    "id": "BL-NMZ7ve3Sl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph_02\""
   ],
   "metadata": {
    "id": "guac0Zh7Gz4Q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T03:52:24.913055Z",
     "start_time": "2025-03-30T03:52:24.906275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from agents.utils.utils import init\n",
    "\n",
    "init()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The model"
   ],
   "metadata": {
    "id": "nGkci88EkVwj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0, streaming=True)"
   ],
   "metadata": {
    "id": "58MBHiikkQDb",
    "ExecuteTime": {
     "end_time": "2025-03-30T03:52:28.342311Z",
     "start_time": "2025-03-30T03:52:27.484462Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T03:52:33.567611Z",
     "start_time": "2025-03-30T03:52:33.563029Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x111887f90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x110ffc510>, root_client=<openai.OpenAI object at 0x1103a09d0>, root_async_client=<openai.AsyncOpenAI object at 0x110e63110>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), streaming=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tools"
   ],
   "metadata": {
    "id": "2_I3howTkdUw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "import random\n",
    "\n",
    "@tool(\"lower_case\", return_direct=True)\n",
    "def to_lower_case(input:str) -> str:\n",
    "  \"\"\"Returns the input as all lower case.\"\"\"\n",
    "  return input.lower()\n",
    "\n",
    "@tool(\"random_number\", return_direct=True)\n",
    "def random_number_maker(input:str) -> str:\n",
    "    \"\"\"Returns a random number between 0-100. input the word 'random'\"\"\"\n",
    "    return random.randint(0, 100)\n",
    "\n",
    "tools = [to_lower_case,random_number_maker]"
   ],
   "metadata": {
    "id": "OLeIVeaEJltj",
    "ExecuteTime": {
     "end_time": "2025-03-30T03:52:47.669174Z",
     "start_time": "2025-03-30T03:52:47.657833Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "\n",
    "tool_executor = ToolExecutor(tools)"
   ],
   "metadata": {
    "id": "IQkcYH78mRmk",
    "ExecuteTime": {
     "end_time": "2025-03-30T03:52:49.667028Z",
     "start_time": "2025-03-30T03:52:49.551607Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fx/hq7s2yzs0cj06n4k_1p8hm9w0000gn/T/ipykernel_43260/1552369938.py:3: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  tool_executor = ToolExecutor(tools)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "model = model.bind_functions(functions)"
   ],
   "metadata": {
    "id": "aqJWD8X1ke5q",
    "ExecuteTime": {
     "end_time": "2025-03-30T03:52:54.303599Z",
     "start_time": "2025-03-30T03:52:54.297451Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fx/hq7s2yzs0cj06n4k_1p8hm9w0000gn/T/ipykernel_43260/2046976494.py:3: LangChainDeprecationWarning: The function `_format_tool_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 1.0. Use :meth:`~langchain_core.utils.function_calling.convert_to_openai_function()` instead.\n",
      "  functions = [format_tool_to_openai_function(t) for t in tools]\n",
      "/var/folders/fx/hq7s2yzs0cj06n4k_1p8hm9w0000gn/T/ipykernel_43260/2046976494.py:4: LangChainDeprecationWarning: The method `BaseChatOpenAI.bind_functions` was deprecated in langchain-openai 0.2.1 and will be removed in 1.0.0. Use :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind_tools` instead.\n",
      "  model = model.bind_functions(functions)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AgentState"
   ],
   "metadata": {
    "id": "DRyUkY3cktGP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ],
   "metadata": {
    "id": "I1H-xbWNkpSv",
    "ExecuteTime": {
     "end_time": "2025-03-30T03:53:11.280403Z",
     "start_time": "2025-03-30T03:53:11.277759Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nodes"
   ],
   "metadata": {
    "id": "FbQtOmzQk27f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.agents import AgentFinish\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state['messages']\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n",
    "    )\n",
    "    print(f\"The agent action is {action}\")\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    print(f\"The tool result is: {response}\")\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ],
   "metadata": {
    "id": "2HoxaGZbkvi5",
    "ExecuteTime": {
     "end_time": "2025-03-30T04:09:31.596300Z",
     "start_time": "2025-03-30T04:09:31.587979Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph"
   ],
   "metadata": {
    "id": "UvaLZp3jlM9F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "# Set the entrypoint as `agent` where we start\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge('action', 'agent')\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ],
   "metadata": {
    "id": "2Vxw2TOClGYm",
    "ExecuteTime": {
     "end_time": "2025-03-30T05:04:23.085939Z",
     "start_time": "2025-03-30T05:04:23.069656Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run it"
   ],
   "metadata": {
    "id": "AAVFBk3GlY5-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# inputs = {\"input\": \"give me a random number and then write in words and make it lower case\", \"chat_history\": []}\n",
    "\n",
    "system_message = SystemMessage(content=\"you are a helpful assistant\")\n",
    "user_01 = HumanMessage(content=\"give me a random number and then write in words and make it lower case\")\n",
    "# user_01 = HumanMessage(content=\"plear write 'Merlion' in lower case\")\n",
    "# user_01 = HumanMessage(content=\"what is a Merlion?\")\n",
    "\n",
    "inputs = {\"messages\": [system_message,user_01]}\n",
    "\n",
    "app.invoke(inputs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELSoxDK_lRO_",
    "outputId": "5882db0a-eeea-4094-ee36-d21ca8d0e46c",
    "ExecuteTime": {
     "end_time": "2025-03-30T05:04:28.787169Z",
     "start_time": "2025-03-30T05:04:26.513875Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fx/hq7s2yzs0cj06n4k_1p8hm9w0000gn/T/ipykernel_43260/1940995734.py:31: LangGraphDeprecationWarning: ToolInvocation is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  action = ToolInvocation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent action is tool='random_number' tool_input={'input': 'random'}\n",
      "The tool result is: 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fx/hq7s2yzs0cj06n4k_1p8hm9w0000gn/T/ipykernel_43260/1940995734.py:31: LangGraphDeprecationWarning: ToolInvocation is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  action = ToolInvocation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent action is tool='lower_case' tool_input={'input': 'sixty five'}\n",
      "The tool result is: sixty five\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content='you are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='give me a random number and then write in words and make it lower case', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"input\":\"random\"}', 'name': 'random_number'}}, response_metadata={'finish_reason': 'function_call', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-35c9c3b5-ec83-4863-ad0d-6ce8059ec09e-0'),\n",
       "  FunctionMessage(content='65', additional_kwargs={}, response_metadata={}, name='random_number'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"input\":\"sixty five\"}', 'name': 'lower_case'}}, response_metadata={'finish_reason': 'function_call', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-53228020-8b8c-4c39-b5c7-85e8387a8576-0'),\n",
       "  FunctionMessage(content='sixty five', additional_kwargs={}, response_metadata={}, name='lower_case'),\n",
       "  AIMessage(content='The random number is 65, written in words it is \"sixty five\", and in lower case it is \"sixty five\".', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-4e2f7771-7d33-4c29-81d6-4486cfbefc50-0')]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# inputs = {\"input\": \"give me a random number and then write in words and make it lower case\", \"chat_history\": []}\n",
    "\n",
    "system_message = SystemMessage(content=\"you are a helpful assistant\")\n",
    "# user_01 = HumanMessage(content=\"give me a random number and then write in words and make it lower case\")\n",
    "user_01 = HumanMessage(content=\"plear write 'Merlion' in lower case\")\n",
    "# user_01 = HumanMessage(content=\"what is a Merlion?\")\n",
    "\n",
    "inputs = {\"messages\": [system_message,user_01]}\n",
    "\n",
    "app.invoke(inputs)"
   ],
   "metadata": {
    "id": "bhW-wd2Olh6H",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "30b3aa2b-f74a-4956-ca91-39b5caf8d9ee",
    "ExecuteTime": {
     "end_time": "2025-03-30T05:04:51.794066Z",
     "start_time": "2025-03-30T05:04:50.824753Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fx/hq7s2yzs0cj06n4k_1p8hm9w0000gn/T/ipykernel_43260/1940995734.py:31: LangGraphDeprecationWarning: ToolInvocation is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  action = ToolInvocation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent action is tool='lower_case' tool_input={'input': 'Merlion'}\n",
      "The tool result is: merlion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content='you are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"plear write 'Merlion' in lower case\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"input\":\"Merlion\"}', 'name': 'lower_case'}}, response_metadata={'finish_reason': 'function_call', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-b9cefb0a-2346-452d-898c-45036102d1e0-0'),\n",
       "  FunctionMessage(content='merlion', additional_kwargs={}, response_metadata={}, name='lower_case'),\n",
       "  AIMessage(content='The word \"Merlion\" in lower case is \"merlion\".', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-e3bd06ea-6906-4613-b13d-6932bc70013a-0')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# inputs = {\"input\": \"give me a random number and then write in words and make it lower case\", \"chat_history\": []}\n",
    "\n",
    "system_message = SystemMessage(content=\"you are a helpful assistant\")\n",
    "# user_01 = HumanMessage(content=\"give me a random number and then write in words and make it lower case\")\n",
    "# user_01 = HumanMessage(content=\"plear write 'Merlion' in lower case\")\n",
    "user_01 = HumanMessage(content=\"what is a Merlion?\")\n",
    "\n",
    "inputs = {\"messages\": [system_message,user_01]}\n",
    "\n",
    "app.invoke(inputs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zdwMgR5Ag6pi",
    "outputId": "f107fe06-29e5-4686-de05-2612956e9fef",
    "ExecuteTime": {
     "end_time": "2025-03-30T05:04:55.693714Z",
     "start_time": "2025-03-30T05:04:55.031396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content='you are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is a Merlion?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='A Merlion is a mythical creature with the head of a lion and the body of a fish. It is a symbol of Singapore and is often seen as a mascot representing the city-state. The Merlion is a popular tourist attraction in Singapore, with statues of the creature located at various spots around the country.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-55abf684-5d7c-4e60-9235-fd520efc0732-0')]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "t6nGDuI6g-Dg"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
