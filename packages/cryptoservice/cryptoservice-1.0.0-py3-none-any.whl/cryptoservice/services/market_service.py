"""å¸‚åœºæ•°æ®æœåŠ¡æ¨¡å—ã€‚

æä¾›åŠ å¯†è´§å¸å¸‚åœºæ•°æ®è·å–ã€å¤„ç†å’Œå­˜å‚¨åŠŸèƒ½ã€‚
"""

import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import nullcontext
from datetime import datetime, timedelta
from pathlib import Path
from threading import Lock
from typing import Any

import pandas as pd
from rich.logging import RichHandler
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
)

from cryptoservice.client import BinanceClientFactory
from cryptoservice.config import settings
from cryptoservice.data import MarketDB
from cryptoservice.exceptions import (
    InvalidSymbolError,
    MarketDataFetchError,
    RateLimitError,
)
from cryptoservice.interfaces import IMarketDataService
from cryptoservice.models import (
    DailyMarketTicker,
    Freq,
    HistoricalKlinesType,
    KlineMarketTicker,
    PerpetualMarketTicker,
    SortBy,
    SymbolTicker,
    UniverseConfig,
    UniverseDefinition,
    UniverseSnapshot,
)
from cryptoservice.utils import DataConverter

# é…ç½® rich logger
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    handlers=[RichHandler(rich_tracebacks=True)],
)
logger = logging.getLogger(__name__)

cache_lock = Lock()


class MarketDataService(IMarketDataService):
    """å¸‚åœºæ•°æ®æœåŠ¡å®ç°ç±»ã€‚"""

    def __init__(self, api_key: str, api_secret: str) -> None:
        """åˆå§‹åŒ–å¸‚åœºæ•°æ®æœåŠ¡ã€‚

        Args:
            api_key: ç”¨æˆ·APIå¯†é’¥
            api_secret: ç”¨æˆ·APIå¯†é’¥
        """
        self.client = BinanceClientFactory.create_client(api_key, api_secret)
        self.converter = DataConverter()
        self.db: MarketDB | None = None

    def get_symbol_ticker(self, symbol: str | None = None) -> SymbolTicker | list[SymbolTicker]:
        """è·å–å•ä¸ªæˆ–æ‰€æœ‰äº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°

        Returns:
            SymbolTicker | list[SymbolTicker]: å•ä¸ªäº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®æˆ–æ‰€æœ‰äº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®
        """
        try:
            ticker = self.client.get_symbol_ticker(symbol=symbol)
            if not ticker:
                raise InvalidSymbolError(f"Invalid symbol: {symbol}")

            if isinstance(ticker, list):
                return [SymbolTicker.from_binance_ticker(t) for t in ticker]
            return SymbolTicker.from_binance_ticker(ticker)

        except Exception as e:
            logger.error(f"[red]Error fetching ticker for {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"Failed to fetch ticker: {e}") from e

    def get_perpetual_symbols(self, only_trading: bool = True) -> list[str]:
        """è·å–å½“å‰å¸‚åœºä¸Šæ‰€æœ‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹ã€‚

        Args:
            only_trading: æ˜¯å¦åªè¿”å›å½“å‰å¯äº¤æ˜“çš„äº¤æ˜“å¯¹

        Returns:
            list[str]: æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹åˆ—è¡¨
        """
        try:
            logger.info("è·å–å½“å‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹åˆ—è¡¨")
            futures_info = self.client.futures_exchange_info()
            perpetual_symbols = [
                symbol["symbol"]
                for symbol in futures_info["symbols"]
                if symbol["contractType"] == "PERPETUAL"
                and (not only_trading or symbol["status"] == "TRADING")
            ]

            return perpetual_symbols

        except Exception as e:
            logger.error(f"[red]è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}") from e

    def check_symbol_exists_on_date(self, symbol: str, date: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦å­˜åœ¨è¯¥äº¤æ˜“å¯¹ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            date: æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            bool: æ˜¯å¦å­˜åœ¨è¯¥äº¤æ˜“å¯¹
        """
        try:
            # å°†æ—¥æœŸè½¬æ¢ä¸ºæ—¶é—´æˆ³èŒƒå›´
            start_time = int(
                datetime.strptime(f"{date} 00:00:00", "%Y-%m-%d %H:%M:%S").timestamp() * 1000
            )
            end_time = int(
                datetime.strptime(f"{date} 23:59:59", "%Y-%m-%d %H:%M:%S").timestamp() * 1000
            )

            # å°è¯•è·å–è¯¥æ—¶é—´èŒƒå›´å†…çš„Kçº¿æ•°æ®
            klines = self.client.futures_klines(
                symbol=symbol,
                interval="1d",
                startTime=start_time,
                endTime=end_time,
                limit=1,
            )

            # å¦‚æœæœ‰æ•°æ®ï¼Œè¯´æ˜è¯¥æ—¥æœŸå­˜åœ¨è¯¥äº¤æ˜“å¯¹
            return bool(klines and len(klines) > 0)

        except Exception as e:
            logger.debug(f"æ£€æŸ¥äº¤æ˜“å¯¹ {symbol} åœ¨ {date} æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
            return False

    def get_top_coins(
        self,
        limit: int = settings.DEFAULT_LIMIT,
        sort_by: SortBy = SortBy.QUOTE_VOLUME,
        quote_asset: str | None = None,
    ) -> list[DailyMarketTicker]:
        """è·å–å‰Nä¸ªäº¤æ˜“å¯¹ã€‚

        Args:
            limit: æ•°é‡
            sort_by: æ’åºæ–¹å¼
            quote_asset: åŸºå‡†èµ„äº§

        Returns:
            list[DailyMarketTicker]: å‰Nä¸ªäº¤æ˜“å¯¹
        """
        try:
            tickers = self.client.get_ticker()
            market_tickers = [DailyMarketTicker.from_binance_ticker(t) for t in tickers]

            if quote_asset:
                market_tickers = [t for t in market_tickers if t.symbol.endswith(quote_asset)]

            return sorted(
                market_tickers,
                key=lambda x: getattr(x, sort_by.value),
                reverse=True,
            )[:limit]

        except Exception as e:
            logger.error(f"[red]Error getting top coins: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get top coins: {e}") from e

    def get_market_summary(self, interval: Freq = Freq.d1) -> dict[str, Any]:
        """è·å–å¸‚åœºæ¦‚è§ˆã€‚

        Args:
            interval: æ—¶é—´é—´éš”

        Returns:
            dict[str, Any]: å¸‚åœºæ¦‚è§ˆ
        """
        try:
            summary: dict[str, Any] = {"snapshot_time": datetime.now(), "data": {}}
            tickers_result = self.get_symbol_ticker()
            if isinstance(tickers_result, list):
                tickers = [ticker.to_dict() for ticker in tickers_result]
            else:
                tickers = [tickers_result.to_dict()]
            summary["data"] = tickers

            return summary

        except Exception as e:
            logger.error(f"[red]Error getting market summary: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get market summary: {e}") from e

    def get_historical_klines(
        self,
        symbol: str,
        start_time: str | datetime,
        end_time: str | datetime | None = None,
        interval: Freq = Freq.h1,
        klines_type: HistoricalKlinesType = HistoricalKlinesType.SPOT,
    ) -> list[KlineMarketTicker]:
        """è·å–å†å²è¡Œæƒ…æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ºå½“å‰æ—¶é—´
            interval: æ—¶é—´é—´éš”
            klines_type: Kçº¿ç±»å‹ï¼ˆç°è´§æˆ–æœŸè´§ï¼‰

        Returns:
            list[KlineMarketTicker]: å†å²è¡Œæƒ…æ•°æ®
        """
        try:
            if isinstance(start_time, str):
                start_time = datetime.fromisoformat(start_time)
            if isinstance(end_time, str):
                end_time = datetime.fromisoformat(end_time)

            # è¿™é‡Œåº”è¯¥è°ƒç”¨ç›¸åº”çš„APIè·å–å†å²æ•°æ®
            # ç®€åŒ–å®ç°ï¼Œè¿”å›ç©ºåˆ—è¡¨
            logger.info(f"è·å– {symbol} çš„å†å²æ•°æ® ({interval.value})")

            return []

        except Exception as e:
            logger.error(f"[red]Error getting historical data for {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get historical data: {e}") from e

    def _fetch_symbol_data(
        self,
        symbol: str,
        start_ts: str,
        end_ts: str,
        interval: Freq,
        klines_type: HistoricalKlinesType = HistoricalKlinesType.FUTURES,
    ) -> list[PerpetualMarketTicker]:
        """è·å–å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®.

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            start_ts: å¼€å§‹æ—¶é—´ (YYYY-MM-DD)
            end_ts: ç»“æŸæ—¶é—´ (YYYY-MM-DD)
            interval: æ—¶é—´é—´éš”
            klines_type: è¡Œæƒ…ç±»å‹
        """
        try:
            # æ£€æŸ¥äº¤æ˜“å¯¹æ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸå­˜åœ¨
            if start_ts and end_ts:
                start_date = datetime.fromtimestamp(int(start_ts) / 1000).strftime("%Y-%m-%d")
                if not self.check_symbol_exists_on_date(symbol, start_date):
                    logger.warning(
                        f"äº¤æ˜“å¯¹ {symbol} åœ¨å¼€å§‹æ—¥æœŸ {start_date} ä¸å­˜åœ¨æˆ–æ²¡æœ‰äº¤æ˜“æ•°æ®ï¼Œ"
                        "å°è¯•è·å–å¯ç”¨æ•°æ®"
                    )
                    # ä¸å†æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œæ˜¯ç»§ç»­æ‰§è¡Œï¼Œè®©APIè¿”å›æœ‰æ•ˆçš„æ•°æ®èŒƒå›´

            klines = self.client.futures_klines(
                symbol=symbol,
                interval=interval.value,
                startTime=start_ts,
                endTime=end_ts,
                limit=1500,
            )

            data = list(klines)
            if not data:
                logger.warning(f"æœªæ‰¾åˆ°äº¤æ˜“å¯¹ {symbol} åœ¨ {start_ts} åˆ° {end_ts} ä¹‹é—´çš„æ•°æ®")
                return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸

            # å¤„ç†æœ‰æ•°æ®çš„æƒ…å†µ
            return [
                PerpetualMarketTicker(
                    symbol=symbol,
                    open_time=kline[0],
                    raw_data=kline,  # ä¿å­˜åŸå§‹æ•°æ®
                )
                for kline in data
            ]

        except InvalidSymbolError:
            # äº¤æ˜“å¯¹ä¸å­˜åœ¨çš„æƒ…å†µç›´æ¥é‡æ–°æŠ›å‡º
            raise
        except Exception as e:
            logger.warning(f"è·å–äº¤æ˜“å¯¹ {symbol} æ•°æ®æ—¶å‡ºé”™: {e}")
            if "Invalid symbol" in str(e):
                raise InvalidSymbolError(f"æ— æ•ˆçš„äº¤æ˜“å¯¹: {symbol}") from e
            else:
                # å¯¹äºå…¶ä»–å¼‚å¸¸ï¼Œä»ç„¶æŠ›å‡ºä»¥ä¾¿ä¸Šå±‚å¤„ç†
                raise MarketDataFetchError(f"è·å–äº¤æ˜“å¯¹ {symbol} æ•°æ®å¤±è´¥: {e}") from e

    def get_perpetual_data(
        self,
        symbols: list[str],
        start_time: str,
        data_path: Path | str,
        end_time: str | None = None,
        interval: Freq = Freq.m1,
        max_workers: int = 1,
        max_retries: int = 3,
        progress: Progress | None = None,
    ) -> None:
        """è·å–æ°¸ç»­åˆçº¦æ•°æ®å¹¶å­˜å‚¨.

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            start_time: å¼€å§‹æ—¶é—´ (YYYY-MM-DD)
            data_path: æ•°æ®å­˜å‚¨è·¯å¾„
            end_time: ç»“æŸæ—¶é—´ (YYYY-MM-DD)
            interval: æ—¶é—´é—´éš”
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            progress: è¿›åº¦æ˜¾ç¤ºå™¨
        """
        try:
            if not symbols:
                raise ValueError("Symbols list cannot be empty")

            data_path = Path(data_path)
            end_time = end_time or datetime.now().strftime("%Y-%m-%d")

            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            if self.db is None:
                self.db = MarketDB(str(data_path))

            # å¦‚æœæ²¡æœ‰ä¼ å…¥progressï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
            if progress is None:
                progress = Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    BarColumn(),
                    TimeElapsedColumn(),
                )

            def process_symbol(symbol: str) -> None:
                """å¤„ç†å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®è·å–ã€‚"""
                retry_count = 0
                while retry_count < max_retries:
                    try:
                        data = self._fetch_symbol_data(
                            symbol=symbol,
                            start_ts=start_time,
                            end_ts=end_time or "",
                            interval=interval,
                        )

                        if data:
                            # ç¡®ä¿ db ä¸ä¸º None
                            if self.db is None:
                                raise MarketDataFetchError("Database pool is not initialized")
                            self.db.store_data(data, interval)  # ç›´æ¥ä¼ é€’ dataï¼Œä¸éœ€è¦åŒ…è£…æˆåˆ—è¡¨
                            return
                        else:
                            logger.info(f"äº¤æ˜“å¯¹ {symbol} åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æ— æ•°æ®")
                            return

                    except InvalidSymbolError as e:
                        # å¯¹äºäº¤æ˜“å¯¹ä¸å­˜åœ¨çš„æƒ…å†µï¼Œè®°å½•ä¿¡æ¯åç›´æ¥è¿”å›ï¼Œä¸éœ€è¦é‡è¯•
                        logger.warning(f"è·³è¿‡äº¤æ˜“å¯¹ {symbol}: {e}")
                        return
                    except RateLimitError:
                        wait_time = min(2**retry_count + 1, 30)
                        time.sleep(wait_time)
                        retry_count += 1
                    except Exception as e:
                        if retry_count < max_retries - 1:
                            retry_count += 1
                            logger.warning(f"é‡è¯• {retry_count}/{max_retries} - {symbol}: {str(e)}")
                            time.sleep(1)
                        else:
                            logger.error(f"å¤„ç†å¤±è´¥ - {symbol}: {str(e)}")
                            break

            with progress if progress is not None else nullcontext():
                overall_task = (
                    progress.add_task("[cyan]å¤„ç†æ‰€æœ‰äº¤æ˜“å¯¹", total=len(symbols))
                    if progress
                    else None
                )

                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [executor.submit(process_symbol, symbol) for symbol in symbols]

                    # è·Ÿè¸ªå®Œæˆè¿›åº¦
                    for future in as_completed(futures):
                        try:
                            future.result()
                            if progress and overall_task is not None:
                                progress.update(overall_task, advance=1)
                        except Exception as e:
                            logger.error(f"å¤„ç†å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"Failed to fetch perpetual data: {e}")
            raise MarketDataFetchError(f"Failed to fetch perpetual data: {e}") from e

    def define_universe(
        self,
        start_date: str,
        end_date: str,
        t1_months: int,
        t2_months: int,
        t3_months: int,
        top_k: int,
        data_path: Path | str,
        output_path: Path | str,
        description: str | None = None,
        strict_date_range: bool = False,
    ) -> UniverseDefinition:
        """å®šä¹‰universeå¹¶ä¿å­˜åˆ°æ–‡ä»¶.

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD æˆ– YYYYMMDD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD æˆ– YYYYMMDD)
            t1_months: T1æ—¶é—´çª—å£ï¼ˆæœˆï¼‰ï¼Œç”¨äºè®¡ç®—mean daily amount
            t2_months: T2æ»šåŠ¨é¢‘ç‡ï¼ˆæœˆï¼‰ï¼Œuniverseé‡æ–°é€‰æ‹©çš„é¢‘ç‡
            t3_months: T3åˆçº¦æœ€å°åˆ›å»ºæ—¶é—´ï¼ˆæœˆï¼‰ï¼Œç”¨äºç­›é™¤æ–°åˆçº¦
            top_k: é€‰å–çš„topåˆçº¦æ•°é‡
            data_path: å†å²æ•°æ®è·¯å¾„ï¼ˆæ•°æ®åº“è·¯å¾„ï¼‰
            output_path: universeè¾“å‡ºæ–‡ä»¶è·¯å¾„
            description: æè¿°ä¿¡æ¯
            strict_date_range: æ˜¯å¦ä¸¥æ ¼é™åˆ¶åœ¨è¾“å…¥çš„æ—¥æœŸèŒƒå›´å†…
                - False (é»˜è®¤): å…è®¸å›çœ‹åˆ°start_dateä¹‹å‰çš„æ•°æ®
                - True: ä¸¥æ ¼é™åˆ¶ï¼Œç¬¬ä¸€ä¸ªå‘¨æœŸå¯èƒ½æ•°æ®ä¸è¶³ä½†ä¸ä¼šè¶…å‡ºèŒƒå›´

        Returns:
            UniverseDefinition: å®šä¹‰çš„universe
        """
        try:
            # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
            start_date = self._standardize_date_format(start_date)
            end_date = self._standardize_date_format(end_date)

            # åˆ›å»ºé…ç½®
            config = UniverseConfig(
                start_date=start_date,
                end_date=end_date,
                t1_months=t1_months,
                t2_months=t2_months,
                t3_months=t3_months,
                top_k=top_k,
            )

            logger.info(f"å¼€å§‹å®šä¹‰universe: {start_date} åˆ° {end_date}")
            logger.info(
                f"å‚æ•°: T1={t1_months}æœˆ, T2={t2_months}æœˆ, T3={t3_months}æœˆ, Top-K={top_k}"
            )

            # ç”Ÿæˆé‡æ–°é€‰æ‹©æ—¥æœŸåºåˆ— (æ¯T2ä¸ªæœˆ)
            # ä½¿ç”¨æœˆæœ«é‡å¹³è¡¡ï¼šåœ¨æœˆæœ«åŸºäºå½“æœˆæ•°æ®é€‰æ‹©ä¸‹æœˆçš„universe
            rebalance_dates = self._generate_rebalance_dates(
                start_date, end_date, t2_months, use_month_end=True
            )
            logger.info(f"å°†åœ¨ä»¥ä¸‹æ—¥æœŸé‡æ–°é€‰æ‹©universe: {rebalance_dates}")

            # æ”¶é›†æ‰€æœ‰å‘¨æœŸçš„snapshots
            all_snapshots = []

            # åœ¨æ¯ä¸ªé‡æ–°é€‰æ‹©æ—¥æœŸè®¡ç®—universe
            for i, rebalance_date in enumerate(rebalance_dates):
                logger.info(f"å¤„ç†æ—¥æœŸ {i + 1}/{len(rebalance_dates)}: {rebalance_date}")

                # è®¡ç®—T1å›çœ‹æœŸé—´çš„å¼€å§‹æ—¥æœŸ
                calculated_t1_start = self._subtract_months(rebalance_date, t1_months)

                # æ ¹æ® strict_date_range é€‰é¡¹å†³å®šå®é™…çš„å¼€å§‹æ—¥æœŸ
                if strict_date_range:
                    # ä¸¥æ ¼æ¨¡å¼ï¼šä¸è¶…å‡ºç”¨æˆ·æŒ‡å®šçš„start_date
                    t1_start_date = max(start_date, calculated_t1_start)
                    if t1_start_date > calculated_t1_start:
                        logger.warning(
                            f"å‘¨æœŸ {i + 1}: ç”±äºstrict_date_rangeé™åˆ¶ï¼ŒT1å¼€å§‹æ—¥æœŸä» "
                            f"{calculated_t1_start} è°ƒæ•´ä¸º {t1_start_date}ï¼Œæ•°æ®æœŸé—´ç¼©çŸ­"
                        )
                else:
                    # å®½æ¾æ¨¡å¼ï¼šå…è®¸å›çœ‹åˆ°start_dateä¹‹å‰
                    t1_start_date = calculated_t1_start
                    if calculated_t1_start < start_date:
                        logger.info(
                            f"å‘¨æœŸ {i + 1}: T1å›çœ‹æœŸ {calculated_t1_start} æ—©äºè¾“å…¥start_date "
                            f"{start_date}ï¼Œå°†ä½¿ç”¨é¢å¤–çš„å†å²æ•°æ®"
                        )

                # è·å–ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹å’Œå®ƒä»¬çš„mean daily amount
                universe_symbols, mean_amounts = self._calculate_universe_for_date(
                    rebalance_date=rebalance_date,
                    t1_start_date=t1_start_date,
                    t3_months=t3_months,
                    top_k=top_k,
                )

                # åˆ›å»ºè¯¥å‘¨æœŸçš„snapshot
                snapshot = UniverseSnapshot(
                    effective_date=rebalance_date,
                    period_start_date=t1_start_date,
                    period_end_date=rebalance_date,
                    symbols=universe_symbols,
                    mean_daily_amounts=mean_amounts,
                    metadata={
                        "t1_start_date": t1_start_date,
                        "calculated_t1_start": calculated_t1_start,
                        "period_adjusted": t1_start_date != calculated_t1_start,
                        "strict_date_range": strict_date_range,
                        "selected_symbols_count": len(universe_symbols),
                        "total_candidates": len(mean_amounts),
                    },
                )

                all_snapshots.append(snapshot)

                logger.info(f"âœ… æ—¥æœŸ {rebalance_date}: é€‰æ‹©äº† {len(universe_symbols)} ä¸ªäº¤æ˜“å¯¹")

            # åˆ›å»ºå®Œæ•´çš„universeå®šä¹‰
            universe_def = UniverseDefinition(
                config=config,
                snapshots=all_snapshots,
                creation_time=datetime.now(),
                description=description,
            )

            # ç¡®å®šæœ€ç»ˆè¾“å‡ºè·¯å¾„
            data_path = Path(data_path)  # ç¡®ä¿ data_path æ˜¯ Path å¯¹è±¡
            output_path = Path(output_path)

            # ç”Ÿæˆé»˜è®¤æ–‡ä»¶å
            filename = (
                f"universe_{start_date}_{end_date}_T1_{t1_months}m_T2_{t2_months}m_"
                f"T3_{t3_months}m_K{top_k}.json"
            )

            # åˆ¤æ–­ output_path æ˜¯æ–‡ä»¶åè¿˜æ˜¯å®Œæ•´è·¯å¾„
            if output_path.parts == (output_path.name,):  # åªæ˜¯æ–‡ä»¶åï¼Œæ²¡æœ‰è·¯å¾„åˆ†éš”ç¬¦
                # ä½¿ç”¨ data_path ä½œä¸ºåŸºç¡€ç›®å½•ï¼Œoutput_path ä½œä¸ºæ–‡ä»¶å
                if output_path.suffix:  # å¦‚æœæœ‰æ–‡ä»¶æ‰©å±•åï¼Œç›´æ¥ä½¿ç”¨
                    final_output_path = data_path / output_path.name
                else:  # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å
                    final_output_path = data_path / filename
            else:
                # å¦‚æœæ˜¯å®Œæ•´è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
                final_output_path = output_path

            # åˆ›å»ºè¾“å‡ºç›®å½•
            final_output_path.parent.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜æ±‡æ€»çš„universeå®šä¹‰
            universe_def.save_to_file(final_output_path)

            logger.info("ğŸ‰ Universeå®šä¹‰å®Œæˆï¼")
            logger.info(f"ğŸ“ åŒ…å« {len(all_snapshots)} ä¸ªé‡æ–°å¹³è¡¡å‘¨æœŸ")
            logger.info(f"ğŸ“‹ æ±‡æ€»æ–‡ä»¶: {final_output_path}")

            return universe_def

        except Exception as e:
            logger.error(f"[red]å®šä¹‰universeå¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"å®šä¹‰universeå¤±è´¥: {e}") from e

    def _standardize_date_format(self, date_str: str) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DDã€‚"""
        if len(date_str) == 8:  # YYYYMMDD
            return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        return date_str

    def _generate_rebalance_dates(
        self, start_date: str, end_date: str, t2_months: int, use_month_end: bool = True
    ) -> list[str]:
        """ç”Ÿæˆé‡æ–°é€‰æ‹©universeçš„æ—¥æœŸåºåˆ—ã€‚

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            t2_months: é‡æ–°å¹³è¡¡é—´éš”ï¼ˆæœˆï¼‰
            use_month_end: æ˜¯å¦ä½¿ç”¨æœˆæœ«æ—¥æœŸ

        Returns:
            list[str]: é‡å¹³è¡¡æ—¥æœŸåˆ—è¡¨
        """
        dates = []
        current_date = pd.to_datetime(start_date)
        end_date_obj = pd.to_datetime(end_date)

        # å¦‚æœä½¿ç”¨æœˆæœ«é‡å¹³è¡¡ï¼Œè°ƒæ•´é€»è¾‘
        if use_month_end:
            # ä»start_dateæ‰€åœ¨æœˆçš„æœˆæœ«å¼€å§‹
            current_date = current_date + pd.offsets.MonthEnd(0)  # å½“æœˆæœˆæœ«

            while current_date <= end_date_obj:
                dates.append(current_date.strftime("%Y-%m-%d"))
                # æ·»åŠ T2ä¸ªæœˆçš„æœˆæœ«
                current_date = current_date + pd.offsets.MonthEnd(t2_months)
        else:
            # åŸæœ‰é€»è¾‘ï¼šä½¿ç”¨æœˆåˆ
            while current_date <= end_date_obj:
                dates.append(current_date.strftime("%Y-%m-%d"))
                # æ·»åŠ T2ä¸ªæœˆ
                if current_date.month + t2_months <= 12:
                    current_date = current_date.replace(month=current_date.month + t2_months)
                else:
                    years_to_add = (current_date.month + t2_months - 1) // 12
                    new_month = (current_date.month + t2_months - 1) % 12 + 1
                    current_date = current_date.replace(
                        year=current_date.year + years_to_add, month=new_month
                    )

        return dates

    def _subtract_months(self, date_str: str, months: int) -> str:
        """ä»æ—¥æœŸå‡å»æŒ‡å®šæœˆæ•°ã€‚"""
        date_obj = pd.to_datetime(date_str)
        # ä½¿ç”¨pandasçš„DateOffsetæ¥æ­£ç¡®å¤„ç†æœˆä»½è¾¹ç•Œé—®é¢˜
        result_date = date_obj - pd.DateOffset(months=months)
        return str(result_date.strftime("%Y-%m-%d"))

    def _get_available_symbols_for_period(self, start_date: str, end_date: str) -> list[str]:
        """è·å–æŒ‡å®šæ—¶é—´æ®µå†…å®é™…å¯ç”¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹ã€‚

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            list[str]: åœ¨è¯¥æ—¶é—´æ®µå†…æœ‰æ•°æ®çš„äº¤æ˜“å¯¹åˆ—è¡¨
        """
        try:
            # å…ˆè·å–å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦ä½œä¸ºå€™é€‰
            candidate_symbols = self.get_perpetual_symbols(only_trading=True)
            logger.info(
                f"æ£€æŸ¥ {len(candidate_symbols)} ä¸ªå€™é€‰äº¤æ˜“å¯¹åœ¨ {start_date} åˆ° "
                f"{end_date} æœŸé—´çš„å¯ç”¨æ€§..."
            )

            available_symbols = []
            batch_size = 50
            for i in range(0, len(candidate_symbols), batch_size):
                batch = candidate_symbols[i : i + batch_size]
                for symbol in batch:
                    # æ£€æŸ¥åœ¨èµ·å§‹æ—¥æœŸæ˜¯å¦æœ‰æ•°æ®
                    if self.check_symbol_exists_on_date(symbol, start_date):
                        available_symbols.append(symbol)

                # æ˜¾ç¤ºè¿›åº¦
                processed = min(i + batch_size, len(candidate_symbols))
                logger.info(
                    f"å·²æ£€æŸ¥ {processed}/{len(candidate_symbols)} ä¸ªäº¤æ˜“å¯¹ï¼Œ"
                    f"æ‰¾åˆ° {len(available_symbols)} ä¸ªå¯ç”¨äº¤æ˜“å¯¹"
                )

                # é¿å…APIé¢‘ç‡é™åˆ¶
                time.sleep(0.1)

            logger.info(
                f"åœ¨ {start_date} åˆ° {end_date} æœŸé—´æ‰¾åˆ° {len(available_symbols)} "
                "ä¸ªå¯ç”¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹"
            )
            return available_symbols

        except Exception as e:
            logger.warning(f"è·å–å¯ç”¨äº¤æ˜“å¯¹æ—¶å‡ºé”™: {e}")
            # å¦‚æœAPIæ£€æŸ¥å¤±è´¥ï¼Œè¿”å›å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦
            return self.get_perpetual_symbols(only_trading=True)

    def _calculate_universe_for_date(
        self, rebalance_date: str, t1_start_date: str, t3_months: int, top_k: int
    ) -> tuple[list[str], dict[str, float]]:
        """è®¡ç®—æŒ‡å®šæ—¥æœŸçš„universeã€‚"""
        try:
            # è·å–åœ¨è¯¥æ—¶é—´æ®µå†…å®é™…å­˜åœ¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹
            actual_symbols = self._get_available_symbols_for_period(t1_start_date, rebalance_date)

            # ç­›é™¤æ–°åˆçº¦ (åˆ›å»ºæ—¶é—´ä¸è¶³T3ä¸ªæœˆçš„)
            cutoff_date = self._subtract_months(rebalance_date, t3_months)
            eligible_symbols = [
                symbol
                for symbol in actual_symbols
                if self._symbol_exists_before_date(symbol, cutoff_date)
            ]

            if not eligible_symbols:
                logger.warning(f"æ—¥æœŸ {rebalance_date}: æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹")
                return [], {}

            # é€šè¿‡APIè·å–æ•°æ®è®¡ç®—mean daily amount
            mean_amounts = {}

            logger.info(f"å¼€å§‹é€šè¿‡APIè·å– {len(eligible_symbols)} ä¸ªäº¤æ˜“å¯¹çš„å†å²æ•°æ®...")

            for i, symbol in enumerate(eligible_symbols):
                try:
                    # è·å–å†å²Kçº¿æ•°æ®
                    klines = self._fetch_symbol_data(
                        symbol=symbol,
                        start_ts=t1_start_date,
                        end_ts=rebalance_date,
                        interval=Freq.d1,
                    )

                    if klines:
                        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
                        expected_days = (
                            pd.to_datetime(rebalance_date) - pd.to_datetime(t1_start_date)
                        ).days + 1
                        actual_days = len(klines)

                        if actual_days < expected_days * 0.8:  # å…è®¸20%çš„æ•°æ®ç¼ºå¤±
                            logger.warning(
                                f"äº¤æ˜“å¯¹ {symbol} æ•°æ®ä¸å®Œæ•´: æœŸæœ›{expected_days}å¤©ï¼Œ"
                                f"å®é™…{actual_days}å¤©"
                            )

                        # è®¡ç®—å¹³å‡æ—¥æˆäº¤é¢
                        amounts = []
                        for kline in klines:
                            try:
                                # kline.raw_data[7] æ˜¯æˆäº¤é¢ï¼ˆUSDTï¼‰
                                if kline.raw_data and len(kline.raw_data) > 7:
                                    amount = float(kline.raw_data[7])
                                    amounts.append(amount)
                            except (ValueError, IndexError):
                                continue

                        if amounts:
                            mean_amount = sum(amounts) / len(amounts)
                            mean_amounts[symbol] = mean_amount
                        else:
                            logger.warning(f"äº¤æ˜“å¯¹ {symbol} åœ¨æœŸé—´å†…æ²¡æœ‰æœ‰æ•ˆçš„æˆäº¤é‡æ•°æ®")

                    # æ·»åŠ çŸ­æš‚å»¶è¿Ÿé¿å…APIé¢‘ç‡é™åˆ¶
                    if (i + 1) % 10 == 0:
                        logger.info(f"å·²å¤„ç† {i + 1}/{len(eligible_symbols)} ä¸ªäº¤æ˜“å¯¹")
                        time.sleep(0.5)

                except Exception as e:
                    logger.warning(f"è·å– {symbol} æ•°æ®æ—¶å‡ºé”™ï¼Œè·³è¿‡: {e}")
                    continue

            # æŒ‰mean daily amountæ’åºå¹¶é€‰æ‹©top_k
            if mean_amounts:
                sorted_symbols = sorted(mean_amounts.items(), key=lambda x: x[1], reverse=True)
                top_symbols = sorted_symbols[:top_k]

                universe_symbols = [symbol for symbol, _ in top_symbols]
                final_amounts = dict(top_symbols)

                # æ˜¾ç¤ºé€‰æ‹©ç»“æœ
                if len(universe_symbols) <= 10:
                    logger.info(f"é€‰ä¸­çš„äº¤æ˜“å¯¹: {universe_symbols}")
                else:
                    logger.info(f"Top 5: {universe_symbols[:5]}")
                    logger.info("å®Œæ•´åˆ—è¡¨å·²ä¿å­˜åˆ°æ–‡ä»¶ä¸­")
            else:
                # å¦‚æœæ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œè‡³å°‘è¿”å›å‰top_kä¸ªeligible symbols
                universe_symbols = eligible_symbols[:top_k]
                final_amounts = dict.fromkeys(universe_symbols, 0.0)
                logger.warning(f"æ— æ³•é€šè¿‡APIè·å–æ•°æ®ï¼Œè¿”å›å‰{len(universe_symbols)}ä¸ªäº¤æ˜“å¯¹")

            return universe_symbols, final_amounts

        except Exception as e:
            logger.error(f"è®¡ç®—æ—¥æœŸ {rebalance_date} çš„universeæ—¶å‡ºé”™: {e}")
            return [], {}

    def _symbol_exists_before_date(self, symbol: str, cutoff_date: str) -> bool:
        """æ£€æŸ¥äº¤æ˜“å¯¹æ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸä¹‹å‰å°±å­˜åœ¨ã€‚"""
        try:
            # æ£€æŸ¥åœ¨cutoff_dateä¹‹å‰æ˜¯å¦æœ‰æ•°æ®
            # è¿™é‡Œæˆ‘ä»¬æ£€æŸ¥cutoff_dateå‰ä¸€å¤©çš„æ•°æ®
            check_date = (pd.to_datetime(cutoff_date) - timedelta(days=1)).strftime("%Y-%m-%d")
            return self.check_symbol_exists_on_date(symbol, check_date)
        except Exception:
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºå­˜åœ¨
            return True

    def download_universe_data(
        self,
        universe_file: Path | str,
        data_path: Path | str,
        interval: Freq = Freq.h1,
        max_workers: int = 4,
        max_retries: int = 3,
        include_buffer_days: int = 7,
        extend_to_present: bool = True,
    ) -> None:
        """æ ¹æ®universeå®šä¹‰æ–‡ä»¶ä¸‹è½½ç›¸åº”çš„å†å²æ•°æ®åˆ°æ•°æ®åº“ã€‚

        Args:
            universe_file: universeå®šä¹‰æ–‡ä»¶è·¯å¾„
            data_path: æ•°æ®åº“å­˜å‚¨è·¯å¾„
            interval: æ•°æ®é¢‘ç‡ (1m, 1h, 4h, 1dç­‰)
            max_workers: å¹¶å‘çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            include_buffer_days: åœ¨æ•°æ®æœŸé—´å‰åå¢åŠ çš„ç¼“å†²å¤©æ•°
            extend_to_present: æ˜¯å¦å°†æ•°æ®æ‰©å±•åˆ°å½“å‰æ—¥æœŸ

        Example:
            service.download_universe_data(
                universe_file="./data/universe.json",
                data_path="./data",
                interval=Freq.h1,
                max_workers=4
            )
        """
        try:
            # åŠ è½½universeå®šä¹‰
            universe_def = UniverseDefinition.load_from_file(universe_file)

            # åˆ†ææ•°æ®ä¸‹è½½éœ€æ±‚
            download_plan = self._analyze_universe_data_requirements(
                universe_def, include_buffer_days, extend_to_present
            )

            logger.info("ğŸ“Š æ•°æ®ä¸‹è½½è®¡åˆ’:")
            logger.info(f"   - æ€»äº¤æ˜“å¯¹æ•°: {download_plan['total_symbols']}")
            logger.info(
                f"   - æ—¶é—´èŒƒå›´: {download_plan['overall_start_date']} åˆ° "
                f"{download_plan['overall_end_date']}"
            )
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(f"   - å¹¶å‘çº¿ç¨‹: {max_workers}")

            # æ‰§è¡Œæ•°æ®ä¸‹è½½
            self.get_perpetual_data(
                symbols=download_plan["unique_symbols"],
                start_time=download_plan["overall_start_date"],
                end_time=download_plan["overall_end_date"],
                data_path=data_path,
                interval=interval,
                max_workers=max_workers,
                max_retries=max_retries,
            )

            logger.info("âœ… Universeæ•°æ®ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {Path(data_path) / 'market.db'}")

            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            self._verify_universe_data_integrity(universe_def, data_path, interval, download_plan)

        except Exception as e:
            logger.error(f"[red]ä¸‹è½½universeæ•°æ®å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"ä¸‹è½½universeæ•°æ®å¤±è´¥: {e}") from e

    def _analyze_universe_data_requirements(
        self,
        universe_def: UniverseDefinition,
        buffer_days: int = 7,
        extend_to_present: bool = True,
    ) -> dict[str, Any]:
        """åˆ†æuniverseæ•°æ®ä¸‹è½½éœ€æ±‚ã€‚

        Args:
            universe_def: Universeå®šä¹‰
            buffer_days: ç¼“å†²å¤©æ•°
            extend_to_present: æ˜¯å¦æ‰©å±•åˆ°å½“å‰æ—¥æœŸ

        Returns:
            Dict: ä¸‹è½½è®¡åˆ’è¯¦æƒ…
        """
        import pandas as pd

        # æ”¶é›†æ‰€æœ‰çš„äº¤æ˜“å¯¹å’Œæ—¶é—´èŒƒå›´
        all_symbols = set()
        all_dates = []

        for snapshot in universe_def.snapshots:
            all_symbols.update(snapshot.symbols)
            all_dates.extend(
                [
                    snapshot.period_start_date,
                    snapshot.period_end_date,
                    snapshot.effective_date,
                ]
            )

        # è®¡ç®—æ€»ä½“æ—¶é—´èŒƒå›´
        start_date = pd.to_datetime(min(all_dates)) - timedelta(days=buffer_days)
        end_date = pd.to_datetime(max(all_dates)) + timedelta(days=buffer_days)

        if extend_to_present:
            end_date = max(end_date, pd.to_datetime("today"))

        return {
            "unique_symbols": sorted(all_symbols),
            "total_symbols": len(all_symbols),
            "overall_start_date": start_date.strftime("%Y-%m-%d"),
            "overall_end_date": end_date.strftime("%Y-%m-%d"),
        }

    def _verify_universe_data_integrity(
        self,
        universe_def: UniverseDefinition,
        data_path: Path | str,
        interval: Freq,
        download_plan: dict[str, Any],
    ) -> None:
        """éªŒè¯ä¸‹è½½çš„universeæ•°æ®å®Œæ•´æ€§ã€‚

        Args:
            universe_def: Universeå®šä¹‰
            data_path: æ•°æ®è·¯å¾„
            interval: æ•°æ®é¢‘ç‡
            download_plan: ä¸‹è½½è®¡åˆ’
        """
        try:
            from cryptoservice.data import MarketDB

            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            db = MarketDB(str(data_path))

            logger.info("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
            incomplete_symbols: list[str] = []
            missing_data: list[dict[str, str]] = []

            for snapshot in universe_def.snapshots:
                try:
                    # æ£€æŸ¥è¯¥å¿«ç…§çš„ä¸»è¦äº¤æ˜“å¯¹æ•°æ®
                    df = db.read_data(
                        symbols=snapshot.symbols[:5],  # åªæ£€æŸ¥å‰5ä¸ª
                        start_time=snapshot.period_start_date,
                        end_time=snapshot.period_end_date,
                        freq=interval,
                    )

                    if df is not None and not df.empty:
                        # æ£€æŸ¥æ•°æ®è¦†ç›–çš„äº¤æ˜“å¯¹æ•°é‡
                        available_symbols = df.index.get_level_values("symbol").unique()
                        missing_symbols = set(snapshot.symbols[:5]) - set(available_symbols)
                        if missing_symbols:
                            incomplete_symbols.extend(missing_symbols)

                except Exception as e:
                    logger.warning(f"éªŒè¯å¿«ç…§ {snapshot.effective_date} æ—¶å‡ºé”™: {e}")
                    missing_data.append({"snapshot_date": snapshot.effective_date, "error": str(e)})

            # æŠ¥å‘ŠéªŒè¯ç»“æœ
            if not incomplete_symbols and not missing_data:
                logger.info("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
                logger.info(f"   - å·²ä¸‹è½½äº¤æ˜“å¯¹: {download_plan['total_symbols']} ä¸ª")
                logger.info(
                    f"   - æ—¶é—´èŒƒå›´: {download_plan['overall_start_date']} åˆ° "
                    f"{download_plan['overall_end_date']}"
                )
                logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            else:
                if incomplete_symbols:
                    unique_incomplete = set(incomplete_symbols)
                    logger.warning(f"âš ï¸ å‘ç° {len(unique_incomplete)} ä¸ªæ•°æ®ä¸å®Œæ•´çš„äº¤æ˜“å¯¹")
                    logger.warning(f"   - ç¤ºä¾‹: {list(unique_incomplete)[:5]}")

                if missing_data:
                    logger.warning(f"âš ï¸ å‘ç° {len(missing_data)} ä¸ªå¿«ç…§æ•°æ®ç¼ºå¤±")

        except Exception as e:
            logger.warning(f"æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {e}")

    def download_universe_data_by_periods(
        self,
        universe_file: Path | str,
        data_path: Path | str,
        interval: Freq = Freq.h1,
        max_workers: int = 4,
        max_retries: int = 3,
        include_buffer_days: int = 7,
    ) -> None:
        """æŒ‰å‘¨æœŸåˆ†åˆ«ä¸‹è½½universeæ•°æ®ï¼ˆæ›´ç²¾ç¡®çš„ä¸‹è½½æ–¹å¼ï¼‰ã€‚

        è¿™ç§æ–¹å¼ä¸ºæ¯ä¸ªé‡å¹³è¡¡å‘¨æœŸå•ç‹¬ä¸‹è½½æ•°æ®ï¼Œå¯ä»¥é¿å…ä¸‹è½½ä¸å¿…è¦çš„æ•°æ®ã€‚

        Args:
            universe_file: universeå®šä¹‰æ–‡ä»¶è·¯å¾„
            data_path: æ•°æ®åº“å­˜å‚¨è·¯å¾„
            interval: æ•°æ®é¢‘ç‡
            max_workers: å¹¶å‘çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            include_buffer_days: ç¼“å†²å¤©æ•°
        """
        try:
            # åŠ è½½universeå®šä¹‰
            universe_def = UniverseDefinition.load_from_file(universe_file)

            logger.info("ğŸ“Š æŒ‰å‘¨æœŸä¸‹è½½æ•°æ®:")
            logger.info(f"   - æ€»å¿«ç…§æ•°: {len(universe_def.snapshots)}")
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(f"   - å¹¶å‘çº¿ç¨‹: {max_workers}")

            # ä¸ºæ¯ä¸ªå‘¨æœŸå•ç‹¬ä¸‹è½½æ•°æ®
            for i, snapshot in enumerate(universe_def.snapshots):
                logger.info(
                    f"ğŸ“… å¤„ç†å¿«ç…§ {i + 1}/{len(universe_def.snapshots)}: {snapshot.effective_date}"
                )

                # è®¡ç®—ä¸‹è½½æ—¶é—´èŒƒå›´
                start_date = pd.to_datetime(snapshot.period_start_date) - timedelta(
                    days=include_buffer_days
                )
                end_date = pd.to_datetime(snapshot.period_end_date) + timedelta(
                    days=include_buffer_days
                )

                logger.info(f"   - äº¤æ˜“å¯¹æ•°é‡: {len(snapshot.symbols)}")
                logger.info(
                    f"   - æ•°æ®æœŸé—´: {start_date.strftime('%Y-%m-%d')} åˆ° "
                    f"{end_date.strftime('%Y-%m-%d')}"
                )

                # ä¸‹è½½è¯¥å‘¨æœŸçš„æ•°æ®
                self.get_perpetual_data(
                    symbols=snapshot.symbols,
                    start_time=start_date.strftime("%Y-%m-%d"),
                    end_time=end_date.strftime("%Y-%m-%d"),
                    data_path=data_path,
                    interval=interval,
                    max_workers=max_workers,
                    max_retries=max_retries,
                )

                logger.info(f"   âœ… å¿«ç…§ {snapshot.effective_date} ä¸‹è½½å®Œæˆ")

            logger.info("ğŸ‰ æ‰€æœ‰universeæ•°æ®ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {Path(data_path) / 'market.db'}")

        except Exception as e:
            logger.error(f"[red]æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}") from e
