# Imports
import os

import networkx as nx
import obonet
import pandas as pd

from gdbcore.helpers.utils import assert_path
from gdbcore.helpers.utils_cypher import (
    cypher_camelcase,
    cypher_upper,
    get_cypher_named_columns,
)


def convertOBOtoNet(ontologyFile: str):
    """
    * FROM CKG *
    Takes an .obo file and returns a NetworkX graph representation of the ontology, that holds multiple \
    edges between two nodes.

    :param str ontologyFile: path to ontology file.
    :return: NetworkX graph.
    """
    # PRECONDITION
    assert isinstance(ontologyFile, str), "path or url to obo ontoloty must be a string"

    # MAIN FUNCTION
    graph = obonet.read_obo(ontologyFile)

    return graph


### Functions for generating the node lists from nx.Graph -> pd.DataFrame
def get_node_subgraphs(
    graph: nx.Graph, by_attribute: bool | str = False, assign_label: str = ""
) -> dict:
    """
    - Generates subgraphs from a networkx graph grouped by a specified node attribute
        - One subgraph per unique value in the attribute
        - For example: in Gene Ontology the node attribute 'namespace' has 3 unique values
            (biological process, cellular component, molecular function)
            so 3 subgraphs would be generated, and the labels for each subgraph
            corresponds to the unique value e.g. BiologicalProcess
    - IF NO GROUPING BY NODE ATTRIBUTE then
        one subgraph (the original graph) returned with node label = assign_label

    PARAMS
    -----
    - graph (nx graph): e.g. graph returned from function convertOBOtoNet()
    - by_attribute (False or str): the node attribute name to group by,
        if no subgroups of nodes then set to False.
        Use of this param requires prior exploration of the graph.
        Assigned in config/config.yaml by 'labels in:'
    - assign_label (str): The node label to assign to the original graph
        note: if by_attribute is given then assign_label is ignored and
            the labels are autogenerated based on the values in by_attribute.

    OUTPUTS
    -----
    - if by_attribute is specified:
        subgraphs (dict): a dictionary with key:value pair as node_label:subgraph
    - if by_attribute is False:
       graph_in_dict (dict): basically {assign_label: graph} is returned.
    """

    # if node attribute to group subgraphs by is given
    if by_attribute is not False:
        # PRECONDITIONS
        assert isinstance(by_attribute, str), "by_attribute must be a string"
        # does the node attribute exist?
        all_attr = nx.get_node_attributes(graph, by_attribute)
        assert (
            len(all_attr) > 0
        ), f"by_attribute {by_attribute} does not exist in the graph {graph}"

        # MAIN FUNCTION

        # getting the unique nodetypes
        nodetypes = set(all_attr.values())
        # verbosity
        print(f"There are node types: {nodetypes}")
        # instantiate dict
        subgraphs = {}

        for each_nodetype in nodetypes:
            # filtering nodes
            sub_node_list = [
                x for x, y in graph.nodes(data=True) if y[by_attribute] == each_nodetype
            ]
            subgraph = graph.subgraph(sub_node_list)
            # add subgraph to dict
            subgraphs[each_nodetype] = subgraph

        return subgraphs

    else:
        # PRECONDITIONS
        assert len(assign_label) > 0, "assigned label name must not be empty"

        # MAIN FUNCTION
        # basically return graph in the dictionary with assigned label
        graph_in_dict = {assign_label: graph}

        return graph_in_dict


def get_node_attribute_dtypes(subgraph: nx.Graph) -> dict:
    """
    - Getting the python datatypes of the node attributes/properties
    - Outputs for this function will later be used for cypher column naming, used in get_cypher_named_columns()

    PARAMS
    -----
    - subgraph (nx.Graph): a networkx graph or subgraph. Should only consist of ONE node type in.

    OUTPUTS
    -----
    - attr_dict (dict): key:value pair is equal to node-attribute-name:python datatype


    """

    # instantiate dict
    attr_dict = {}

    # for each node
    for node_name, node_attributes in subgraph.nodes(data=True):
        # get out the attributes
        key_pairs = node_attributes.items()

        # getting the data types
        for attr_name, attr_value in key_pairs:
            attr_dict[attr_name] = (type(attr_value)).__name__

    return attr_dict


def init_node_df(
    subgraph: nx.Graph,
    label: str,
    fill_na="Unknown",
    col_names: dict = {"node_ids": "id:ID", "node_labels": ":LABEL"},
) -> pd.DataFrame:
    """
    Creates Node List as an initial pandas dataframe for a graph.
    Should only contain one node type in the subgraph.

    PARAMS
    -----
    - subgraph (nx.Graph): a networkx graph or subgraph. Should only consist of ONE node type in.
    - label (str): the node label (provided in config/config.yaml)
    - fill_na (str): what null values should be filled with.
        Should be consistent for the whole database.
    - col_names (dict): keys should include 'node_ids' and 'node_labels', providing the desired column name in config

    OUTPUTS
    -----
    - df (pd.DataFrame): node list of the subgraph
    """
    # PRECONDITIONS
    assert isinstance(label, str), "label must be a string"
    assert fill_na is not None, "fill_na must be something"
    assert isinstance(
        col_names, dict
    ), f"col_names must be provided as a dictionary:{col_names}"
    assert (
        "node_ids" in col_names.keys()
    ), f"node_ids must be a key in col_names: {col_names}"
    assert (
        "node_labels" in col_names.keys()
    ), f"node_labels must be a key in col_names: {col_names}"

    # MAIN FUNCTION
    # creating the dataframe
    df = pd.DataFrame(
        data=[y for x, y in subgraph.nodes(data=True)],
        index=[x for x, y in subgraph.nodes(data=True)],
    ).reset_index()

    # filling null values
    df = df.fillna(fill_na)

    # formating node labels to camel case e.g. BiologicalInsights
    cypher_label = cypher_camelcase(label, lower_first=False)

    # renaming columns
    # get expected datatypes to be casted
    column_dtypes = get_node_attribute_dtypes(subgraph)
    # convert to cypher column naming e.g. firstName:STRING
    renamed_cols = get_cypher_named_columns(column_dtypes)
    # rename df columns
    # df = df.rename(columns=renamed_cols)
    df = df.rename(columns={"index": "node_ids"})
    df["node_labels"] = cypher_label

    # rename columns based on output > col_names
    df = df.rename(columns=col_names)

    return df


### Functions for generating the edge lists from nx.Graph -> pd.DataFrame
def get_edge_types(graph: nx.Graph) -> list:
    """
    Retrieves all the edge types in a given graph and returns in a list.

    PARAMS
    -----
    graph (nx.Graph): a graph

    OUTPUTS
    -----
    edges (list): the different edge types
    """
    # PRECONDITIONS
    assert isinstance(
        graph, nx.Graph
    ), f"graph should be a networkx Graph: {type(graph)}"

    # MAIN FUNCTION
    edges = list(set([k for x, y, k in graph.edges(keys=True)]))

    # POSTCONDITIONS
    assert len(edges) > 0, f"there are no edge types/labels in the graph: {edges}"
    for each_type in edges:
        assert isinstance(
            each_type, str
        ), f"edge type/label should be a string: {each_type}"

    return edges


def get_edge_attribute_dtypes(graph: nx.Graph, edge_type: str) -> dict:
    """
    - Getting the python datatypes of the edge attributes/properties
    - Outputs for this function will later be used for cypher column naming, used in get_cypher_named_columns()

    PARAMS
    -----
    - graph (nx.Graph): a networkx graph, can consist of however many node types.
    - edge_type (str): the edge label/type of interest

    OUTPUTS
    -----
    - dtypes_dict (dict): key:value pair is equal to edge-attribute-name:python datatype


    """
    # PRECONDITIONS
    assert isinstance(
        graph, nx.Graph
    ), f"graph should be a networkx graph: {type(graph)}"
    assert isinstance(edge_type, str), f"{edge_type} edge_type must be a string"

    # MAIN FUNCTION
    # instantiate dict
    dtypes_dict = {}

    attribute_dicts = [
        a for x, y, k, a in graph.edges(keys=True, data=True) if k == edge_type
    ]

    for each_edge_dict in attribute_dicts:
        # get out the attributes
        key_pairs = each_edge_dict.items()

        # getting the data types
        for attr_name, attr_value in key_pairs:
            dtypes_dict[attr_name] = (type(attr_value)).__name__

    return dtypes_dict


def init_edge_df(
    graph: nx.Graph,
    edge_type: str,
    fill_na="Unknown",
    col_names: dict = {
        "start_ids": ":START_ID",
        "end_ids": ":END_ID",
        "edge_labels": ":TYPE",
    },
) -> pd.DataFrame:
    """
    From nx graph, creates Edge List as an initial pandas dataframe for a given edge_type

    PARAMS
    -----
    - graph (nx.Graph): a networkx graph.
    - edge_type (str): the edge label that the df will be fore
    - fill_na (str): what null values should be filled with.
        Should be consistent for the whole database.
    - col_names (dict): keys should include 'start_ids', 'end_ids', and 'edge_labels',
        providing the desired column name in config

    OUTPUTS
    -----
    - df (pd.DataFrame): edge list for that edge_type
    """

    # PRECONDITIONS
    assert isinstance(edge_type, str), "edge_type must be a string"
    assert fill_na is not None, "fill_na must be something"
    assert isinstance(
        col_names, dict
    ), f"col_names must be provided as a dictionary:{col_names}"
    assert (
        "start_ids" in col_names.keys()
    ), f"start_ids must be a key in col_names: {col_names}"
    assert (
        "end_ids" in col_names.keys()
    ), f"end_ids must be a key in col_names: {col_names}"
    assert (
        "edge_labels" in col_names.keys()
    ), f"edge_labels must be a key in col_names: {col_names}"

    # MAIN FUNCTION
    # creating dataframe
    df = pd.DataFrame(
        data=[
            (x, y, k, a)
            for x, y, k, a in graph.edges(keys=True, data=True)
            if k == edge_type
        ],
        columns=["start_ids", "end_ids", "edge_labels", "attributes"],
    )

    # get expected cypher datatypes
    column_dtypes = get_edge_attribute_dtypes(graph, edge_type)

    # init rename dict
    renamed_cols = {}

    # if no attributes then skip
    if len(column_dtypes) > 0:
        # need to extend attributes column to a df and concat back on
        df_attributes = pd.json_normalize(df["attributes"])
        df = pd.concat([df.drop(["attributes"], axis=1), df_attributes], axis=1)

        # getting converted cypher naming
        renamed_cols = get_cypher_named_columns(column_dtypes)
    else:
        df = df.drop("attributes", axis=1)

    # renaming
    # df = df.rename(columns=renamed_cols)

    # format relationships UPPERCASE (but if all same edge_type, apply unnecessary..)
    df["edge_labels"] = df["edge_labels"].apply(lambda x: cypher_upper(x))

    # rename columns based on output > col_names
    df = df.rename(columns=col_names)

    return df


### After init node and edge lists as a pandas dataframe, can clean manually then can use these:
### function(s) for exporting the node and edge lists (from pandas dataframes to csvs)
def save_csv_header(
    df_graph: pd.DataFrame,
    label: str,
    prefix: str,
    folder: str = "",
    sep: str = "|",
    quotechar: str = '"',
):
    """
    Saving node and edge lists as csvs, where there is a separate header and data file

    PARAMS
    -----
    - df_graph (pd.DataFrame): node and edge lists from init_node_df() or init_edge_df()
    - label (str): node/edge label/type
    - prefix (str): specific to the obo file (from config/config.yaml)
    - folder (str): filepath to output folder (from config/config.yaml)
    - sep (str): delimiter to use for the file (from config/config.yaml)
    - quotechar (str): quote character to use in the csv

    OUTPUTS
    -----
    csvs saved in folder location
    """
    # PRECONDITIONS
    assert isinstance(
        df_graph, pd.DataFrame
    ), f"df_graph must be a pandas dataframe: {type(df_graph)}"
    assert isinstance(label, str), f"label must be a string: {label}"
    assert isinstance(prefix, str), f"prefix must be a string: {prefix}"
    assert_path(folder)
    assert isinstance(sep, str), f"sep must be provided as a string: {sep}"
    assert isinstance(
        quotechar, str
    ), f"quotechar must be provided as a string: {quotechar}"

    # MAIN FUNCTION
    # getting filepath
    data_path = os.path.join(folder, f"{prefix}_{label}-data.csv")
    header_path = os.path.join(folder, f"{prefix}_{label}-header.csv")
    # saving data file
    df_graph.to_csv(data_path, sep=sep, index=False, header=False, quotechar=quotechar)
    # saving header file
    df_graph.head(0).to_csv(
        header_path, sep=sep, index=False, header=True, quotechar=quotechar
    )

    # absolute paths
    abs_data_path = os.path.abspath(data_path)
    abs_header_path = os.path.abspath(header_path)

    # POSTCONDITIONS
    assert_path(abs_data_path)
    assert_path(abs_header_path)

    # verbose
    print(
        f"For {label}, {prefix}: \n\tHeader file saved to {abs_header_path},\n\tData file(s) saved to {abs_data_path}"
    )


def save_csv(
    df_graph: pd.DataFrame,
    label: str,
    prefix: str,
    folder: str = "",
    sep: str = "|",
    quotechar: str = '"',
):
    """
    Saving node and edge lists as csvs, NO SEPARATE HEADER FILE

    PARAMS
    -----
    - df_graph (pd.DataFrame): node and edge lists from init_node_df() or init_edge_df()
    - label (str): node/edge label/type
    - prefix (str): specific to the obo file (from config/config.yaml)
    - folder (str): filepath to output folder (from config/config.yaml)
    - sep (str): delimiter to use for the file (from config/config.yaml)
    - quotechar (str): quote character to use in the csv

    OUTPUTS
    -----
    csvs saved in folder location
    """
    # PRECONDITIONS
    assert isinstance(
        df_graph, pd.DataFrame
    ), f"df_graph must be a pandas dataframe: {type(df_graph)}"
    assert isinstance(label, str), f"label must be a string: {label}"
    assert isinstance(prefix, str), f"prefix must be a string: {prefix}"
    assert_path(folder)
    assert isinstance(sep, str), f"sep must be provided as a string: {sep}"
    assert isinstance(
        quotechar, str
    ), f"quotechar must be provided as a string: {quotechar}"

    # MAIN FUNCTION
    # getting filepath
    data_path = os.path.join(folder, f"{prefix}_{label}-data.csv")
    # saving file
    df_graph.to_csv(data_path, sep=sep, index=False, header=True, quotechar=quotechar)
    # absolute path
    abs_data_path = os.path.abspath(data_path)

    # POSTCONDITIONS
    assert_path(abs_data_path)

    # final verbose
    print(
        f"For {label}, {prefix}: \n\tData file (no sep header) saved to {abs_data_path}"
    )


"""
- Functions that put all of the above together to auto generate node and edge lists as csvs
- If wanting to do additional cleaning to the node and edge list dfs:
    - can use the below functions to generate the csvs, 
        then clean wahtever way you want (e.g., reading in as pd.DataFrames and cleaning there),
        then save to csv again
    - OR don't use the below functions. Instead use above functions up to init df ones
"""


def generate_node_files(
    subgraph_dict: dict,
    prefix: str,
    folderpath: str = "",
    sep: str = "|",
    quotechar: str = '"',
    fill_na="Unknown",
    sep_header: bool = True,
    col_names: dict = {"node_ids": "id:ID", "node_labels": ":LABEL"},
):
    """
    - Generates header and data csv files for each node type

    PARAMS
    -----
    - subgraph_dict (dict): output from get_node_subgraphs(), key:value pairs are node_label:nx subgraph
    - prefix (str): naming for the specific obo file e.g. 'go' or 'mondo'
        derived from config/config.yaml
    - folderpath (str): where to save the header and data csv files to
        derived from config/config.yaml
    - sep (str): the delimiter for the output files
    - quotechar (str): the quote character for the output files
    - fill_na (anything?): what value should be used when empty property
    - sep_header (boolean): if true then separate header csv, false keep the column header with the data
    - col_names (dict): keys should include 'node_ids' and 'node_labels', providing the desired column name in config

    OUTPUTS
    -----
    header and data files to the designated folderpath

    """
    # PRECONDITIONS
    assert isinstance(
        subgraph_dict, dict
    ), f"subgraph_dict must be a dict: {subgraph_dict}"
    assert isinstance(prefix, str), f"prefix must be a string: {prefix}"
    assert_path(folderpath)
    assert isinstance(sep, str), f"sep must be provided as a string: {sep}"
    assert isinstance(
        quotechar, str
    ), f"quotechar must be provided as a string: {quotechar}"
    assert isinstance(sep_header, bool), f"sep_header must be a boolean: {sep_header}"

    # MAIN FUNCTION
    for each_type in subgraph_dict:
        # create node list as df
        df = init_node_df(
            subgraph_dict[each_type], each_type, fill_na=fill_na, col_names=col_names
        )

        if sep_header:
            # save the df as a csv (header and data files)
            save_csv_header(
                df_graph=df,
                label=each_type,
                prefix=prefix,
                folder=folderpath,
                sep=sep,
                quotechar=quotechar,
            )
        else:
            # no sep header file
            save_csv(
                df_graph=df,
                label=each_type,
                prefix=prefix,
                folder=folderpath,
                sep=sep,
                quotechar=quotechar,
            )
        # sanity check
        print(each_type, "saved in", os.path.abspath(folderpath))


def generate_edge_files(
    graph: nx.Graph,
    prefix: str,
    folderpath: str = "",
    sep: str = "|",
    quotechar: str = '"',
    fill_na="Unknown",
    sep_header=True,
    col_names: dict = {
        "start_ids": ":START_ID",
        "end_ids": ":END_ID",
        "edge_labels": ":TYPE",
    },
):
    """
    - Generates header and data csv files for each edge type

    PARAMS
    -----
    graph (nx.Graph): the whole graph
    prefix (str): naming for the specific obo file e.g. 'go' or 'mondo'
        derived from config/config.yaml
    folderpath (str): where to save the header and data csv files to
        derived from config/config.yaml
    - sep (str): the delimiter for the output files
    - quotechar (str): the quote character for the output files
    - fill_na (anything?): what value should be used when empty property
    - sep_header (boolean): if true then separate header csv, false keep the column header with the data
    - col_names (dict): keys should include 'start_ids', 'end_ids', and 'edge_labels',
        providing the desired column name in config

    OUTPUTS
    -----
    header and data files to the designated folderpath

    """
    # PRECONDITIONS
    assert isinstance(graph, nx.Graph), f"subgraph_dict must be a nx.Graph: {graph}"
    assert isinstance(prefix, str), f"prefix must be a string: {prefix}"
    assert_path(folderpath)
    assert isinstance(sep, str), f"sep must be provided as a string: {sep}"
    assert isinstance(
        quotechar, str
    ), f"quotechar must be provided as a string: {quotechar}"
    assert isinstance(sep_header, bool), f"sep_header must be a boolean: {sep_header}"

    # MAIN FUNCTION
    # get edges types/labels that are in the graph
    edgelist = get_edge_types(graph)
    for each_type in edgelist:
        # create edge list as df
        df = init_edge_df(graph, each_type, fill_na=fill_na, col_names=col_names)

        if sep_header:
            # save the df as a csv (header and data files)
            save_csv_header(
                df_graph=df,
                label=each_type,
                prefix=prefix,
                folder=folderpath,
                sep=sep,
                quotechar=quotechar,
            )
        else:
            # save df as one csv (no sep header)
            save_csv(
                df_graph=df,
                label=each_type,
                prefix=prefix,
                folder=folderpath,
                sep=sep,
                quotechar=quotechar,
            )
        # sanity check
        print(each_type, "saved in", os.path.abspath(folderpath))
