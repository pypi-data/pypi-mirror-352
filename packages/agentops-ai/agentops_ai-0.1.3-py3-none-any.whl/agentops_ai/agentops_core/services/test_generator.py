import ast
import os
from typing import List, Dict, Any, Optional
import openai
from dotenv import load_dotenv
import re

load_dotenv()

class TestGenerator:
    """Service for generating tests using OpenAI API."""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o-mini"):
        """Initialize the test generator.
        Args:
            api_key: OpenAI API key (defaults to env var)
            model: OpenAI model to use
        """
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY")
        self.model = model
        import openai
        openai.api_key = self.api_key
        self.client = openai
    
    def parse_code(self, code: str) -> Dict[str, Any]:
        """Parse Python code to extract structure.
        
        Args:
            code: Python code as string
            
        Returns:
            Dict containing code structure
        """
        try:
            tree = ast.parse(code)
            # Extract classes, functions, etc.
            # Implementation here
            return {"success": True, "structure": {}}
        except SyntaxError as e:
            return {"success": False, "error": str(e)}
    
    def generate_tests(self, code: str, framework: str = "pytest", module_path: str = None, api_mode: bool = False) -> Dict[str, Any]:
        """Generate tests for the given code.
        
        Args:
            code: Python code to generate tests for
            framework: Testing framework to use
            module_path: Path to the module being tested
            api_mode: If True, use API endpoint test prompt
            
        Returns:
            Dict containing generated tests and metadata
        """
        # MOCK: If MOCK_AGENTOPS is set, always return a simple valid test file
        if os.environ.get("MOCK_AGENTOPS", "0") == "1":
            test_code = (
                'import pytest\n\n'
                '# Risk: medium | Importance: auto-generated by AgentOps\n'
                '@pytest.mark.generated_by_agentops\n'
                'def test_sample():\n'
                "    assert 1 == 1, 'Assertion failed: 1 == 1'\n"
            )
            return {"success": True, "tests": test_code, "confidence": 0.99}
        
        # Parse the code
        parsed = self.parse_code(code)
        if not parsed["success"]:
            return {"success": False, "error": parsed["error"]}
        
        # Generate prompt for OpenAI
        if api_mode:
            prompt = self._create_api_test_prompt(code, module_path)
        else:
            prompt = self._create_prompt(code, parsed["structure"], framework)
        
        # Call OpenAI API
        response = self._call_openai(prompt)
        
        # Process and return the generated tests
        test_code = self._process_response(response, framework)["tests"]
        
        # QA validation step
        validated_code = self.qa_validate_test_file(test_code)
        if not validated_code:
            return {"success": False, "error": "QA validation failed. No valid code returned."}
        
        # Rewrite imports to absolute
        if module_path:
            validated_code = self._rewrite_imports_to_absolute(validated_code, module_path)
        
        # Syntax check
        try:
            ast.parse(validated_code)
        except Exception as e:
            return {"success": False, "error": f"Syntax error in QA-validated test: {e}"}
        
        return {"success": True, "tests": validated_code, "confidence": 0.95}
    
    def _create_prompt(self, code: str, structure: Dict[str, Any], framework: str) -> str:
        """Create a prompt for the OpenAI API that guarantees only valid Python code in a single code block, no prose or markdown outside."""
        summary = []
        if structure:
            functions = structure.get('functions', [])
            if functions:
                summary.append("Functions:")
                for f in functions:
                    summary.append(f"- {getattr(f, 'name', str(f))}({', '.join(p['name'] for p in getattr(f, 'parameters', []))})")
        summary_text = '\n'.join(summary)
        prompt = (
            "def example(): pass\n\n"
            "You are AgentOps QA Agent. You must output only valid Python code inside a single ```python code block. "
            "Do not explain, comment, or output anything else outside the block. No markdown, no prose, no TODOs.\n\n"
            "Start output with:\n```python\n\n"
            "End output with:\n```\n\n"
            "Your task is to generate a complete, ready-to-run pytest test file for all public functions and methods in the code below. "
            "The file must:\n"
            "- Use idiomatic pytest, with @pytest.mark.parametrize where appropriate\n"
            "- Use assert statements with helpful error messages\n"
            "- Add `@pytest.mark.generated_by_agentops` above every test\n"
            "- Include a Python docstring at the top with this fingerprint block:\n"
            '"""\n'
            "Auto-generated by AgentOps QA Agent v0.4\n"
            "Date: 2025-05-24\n"
            "Target: auto-discovered functions (see below)\n"
            "LLM Confidence Score: 86%\n"
            "Generation Prompt Hash: a9f7e2c3\n"
            "Regeneration Policy: Re-evaluate on diff; regenerate if confidence < 70%\n"
            '"""\n\n'
            "Important:\n"
            "- Do not include any non-Python content.\n"
            "- Do not wrap your output in markdown.\n"
            "- Assume temperature=0.0 and sufficient max_tokens.\n"
            "- The output should be a single test file stored under `.agentops/tests/<mirrored path>/`.\n\n"
            f"# Code summary:\n{summary_text}\n\n# Full code to test:\n{code}"
        )
        return prompt
    
    def _call_openai(self, prompt: str) -> Dict[str, Any]:
        """Call the OpenAI API with the given prompt."""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert test engineer. Your task is to generate comprehensive tests for the provided code."},
                    {"role": "user", "content": prompt}
                ]
            )
            # No debug print of the raw response
            return {"success": True, "data": response}
        except Exception as e:
            print("[AgentOps DEBUG] OpenAI API Exception:", e)
            return {"success": False, "error": str(e)}
    
    def _process_response(self, response: Dict[str, Any], framework: str) -> Dict[str, Any]:
        """Process the OpenAI API response."""
        if not response["success"]:
            print("[AgentOps DEBUG] OpenAI response not successful:", response)
            return {"success": False, "error": response["error"]}
        data = response["data"]
        # Extract the test code from the OpenAI response
        try:
            raw_content = data.choices[0].message.content
            # Extract code from ```python ... ``` block
            match = re.search(r"```python(.*?)```", raw_content, re.DOTALL)
            if match:
                test_code = match.group(1).strip()
            else:
                test_code = raw_content.strip()
            confidence = 1.0 if test_code else 0.0
            return {"success": True, "tests": test_code, "confidence": confidence}
        except Exception as e:
            print("[AgentOps DEBUG] Failed to parse OpenAI response:", e)
            return {"success": False, "error": f"Failed to parse OpenAI response: {e}"}
    
    def write_tests_to_file(self, test_code: str, output_dir: str = "tests", base_name: str = "test_generated.py") -> str:
        """Write the generated test code to a file and return the file path."""
        os.makedirs(output_dir, exist_ok=True)
        file_path = os.path.join(output_dir, base_name)
        with open(file_path, "w") as f:
            f.write(test_code)
        return file_path

    def _dedupe_and_group_imports(self, code):
        lines = code.split('\n')
        import_lines = [l for l in lines if l.strip().startswith('import ') or l.strip().startswith('from ')]
        import_lines = list(dict.fromkeys(import_lines))
        non_import_lines = [l for l in lines if l not in import_lines]
        return '\n'.join(import_lines + [''] + non_import_lines)

    def _decorate_tests(self, code):
        lines = code.split('\n')
        out = []
        for i, line in enumerate(lines):
            if line.strip().startswith('def test_'):
                out.append('@pytest.mark.generated_by_agentops')
            out.append(line)
        return '\n'.join(out)

    def _add_risk_comments(self, code):
        lines = code.split('\n')
        out = []
        for i, line in enumerate(lines):
            if line.strip().startswith('def test_'):
                out.append('# Risk: medium | Importance: auto-generated by AgentOps')
            out.append(line)
        return '\n'.join(out)

    def _add_assertion_messages(self, code):
        import re
        def add_msg(match):
            assertion = match.group(0)
            if 'assert ' in assertion and ',' not in assertion:
                expr = assertion[len('assert '):].strip()
                return f"assert {expr}, 'Assertion failed: {expr}'"
            return assertion
        return re.sub(r'assert [^,\n]+', add_msg, code)

    def qa_validate_test_file(self, test_code: str) -> str:
        """Validate and auto-fix a generated test file using the QA prompt (3-step protocol)."""
        qa_prompt = (
            "\U0001F4CC You are AgentOps QA Validator. Your job is to validate and repair a Python `pytest` test file generated by a code generation agent.\n\n"
            "You must follow this 3-step protocol:\n\n"
            "---\n\n"
            "\U0001F50D 1. Validate the Test File\n\n"
            "Check for:\n"
            "- Syntax errors\n"
            "- Missing or incorrect assertions\n"
            "- Invalid or missing imports\n"
            "- Missing `@pytest.mark.generated_by_agentops`\n"
            "- Incomplete tests or unused code\n"
            "- Broken parameterization\n\n"
            "---\n\n"
            "\U0001F6E0\uFE0F 2. Repair It If Possible\n\n"
            "If errors are found:\n"
            "- Regenerate the entire test file using clean, idiomatic `pytest`\n"
            "- Fix syntax and assertion issues\n"
            "- Include at least one meaningful `assert` per function\n"
            "- Add `@pytest.mark.generated_by_agentops` to each test\n"
            "- Add a top-level docstring fingerprint block like this:\n\n"
            '"""\n'
            "Auto-generated by AgentOps QA Agent v0.4\n"
            "Date: 2025-05-25\n"
            "Target: <function name> (from <path>)\n"
            "LLM Confidence Score: 86%\n"
            "Generation Prompt Hash: a9f7e2c3\n"
            "Regeneration Policy: Re-evaluate on diff; regenerate if confidence < 70%\n"
            '"""\n\n'
            "---\n\n"
            "\u274C If It Cannot Be Repaired\n\n"
            "Still output a valid Python file with:\n\n"
            "@pytest.mark.skip(reason=\"Failed LLM validation — human review required\")\n\n"
            "A comment block at the top:\n"
            "# \u26A0\uFE0F AgentOps was unable to fully validate this test. Please review manually.\n\n"
            "The test body should be syntactically valid but contain only pass\n\n"
            "---\n\n"
            "\u2733\uFE0F Return Format:\n\n"
            "Always return a single valid Python file\n\n"
            "Wrap your entire output in python and end with\n\n"
            "Never include markdown, explanations, or extra text\n\n"
            "---\n\n"
            "\U0001F9EA Test file to review and fix:\n"
            f"{test_code}"
        )
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert test QA agent. Your job is to validate and auto-fix Python test files."},
                    {"role": "user", "content": qa_prompt}
                ],
                temperature=0.0
            )
            raw_content = response.choices[0].message.content
            match = re.search(r"```python(.*?)```", raw_content, re.DOTALL)
            if match:
                return match.group(1).strip()
            return raw_content.strip()
        except Exception as e:
            print("[AgentOps QA DEBUG] QA validation failed:", e)
            return None

    def _rewrite_imports_to_absolute(self, code: str, module_path: str) -> str:
        """Rewrite relative and placeholder imports in the test code to absolute imports from the project root."""
        import re
        # Compute the absolute import base from the module_path
        # E.g., for .agentops/tests/agentops_ai/agentops_cli/test_main.py, base is agentops_ai.agentops_cli
        base = module_path.replace(".agentops/tests/", "").replace("/", ".").rsplit(".", 1)[0]
        def repl(match):
            rel = match.group(1)
            name = match.group(2)
            # Only handle .. imports for now
            if rel == "..":
                # Remove last component from base
                abs_base = ".".join(base.split(".")[:-1])
                return f"from {abs_base} import {name}"
            elif rel == ".":
                return f"from {base} import {name}"
            return match.group(0)
        # Replace relative imports
        code = re.sub(r"from (\.+)\s*import\s*([\w, ]+)", repl, code)
        # Replace 'from your_module import ...' with absolute import
        code = re.sub(r"from your_module import ([\w, ]+)", f"from {base} import \\1", code)
        # Replace 'from . import ...' and 'from .. import ...' at the start of lines
        code = re.sub(r"^from \. import ([\w, ]+)", f"from {base} import \\1", code, flags=re.MULTILINE)
        code = re.sub(r"^from \.\. import ([\w, ]+)", f"from {'.'.join(base.split('.')[:-1])} import \\1", code, flags=re.MULTILINE)
        return code 

    def _create_api_test_prompt(self, code: str, filename: str) -> str:
        return f'''
You are a QA test generation agent.

You are given one or more API endpoint definitions from a Python web framework (e.g., FastAPI, Flask).

Your job is to generate a **valid pytest test file** that tests each endpoint's basic success case, with:
- The correct HTTP method
- Required parameters (from path, query, or JSON body)
- Expected status code (e.g., 200 OK)

Use `TestClient` if the framework is FastAPI or Flask.

Each test must:
- Be wrapped with `@pytest.mark.generated_by_agentops`
- Include a meaningful assert for `status_code` and (if JSON) a key/value check
- Be deterministic, with fixed test data

Embed this fingerprint docstring at the top of the file:

"""
Auto-generated by AgentOps QA Agent v0.4
Date: 2025-05-25
Target: API endpoint(s) from {filename}
LLM Confidence Score: 86%
Generation Prompt Hash: a9f7e2c3
Regeneration Policy: Re-evaluate on diff; regenerate if confidence < 70%
"""

Do NOT include markdown. Return only valid Python code inside a ```python code block.

Example endpoint:
@app.post("/login")
def login(username: str, password: str):
    ...
from fastapi.testclient import TestClient
from app import app
import pytest

client = TestClient(app)

@pytest.mark.generated_by_agentops
def test_login_success():
    response = client.post("/login", json={"username": "test", "password": "secret"})
    assert response.status_code == 200
    assert "token" in response.json()

---

### 🛠 Extend This Prompt For:
- Auth headers (`Authorization: Bearer ...`)
- Query parameters (`/items/?id=1&status=ok`)
- Parametrize input/output cases
- Error scenarios (401, 422, etc.)

Below are the API endpoint definitions:
{code}
''' 