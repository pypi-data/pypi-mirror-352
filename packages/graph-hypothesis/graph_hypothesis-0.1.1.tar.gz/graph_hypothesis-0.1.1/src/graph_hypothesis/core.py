import multiprocessing
import random
import warnings
from typing import Any, Callable, Dict, Optional

import networkx as nx
from tqdm.auto import tqdm

from .axb_motifs import calculate_axb_motifs_metric
from .hypothesis_tester import hypothesis_tester
from .interactions import calculate_interactions_metric
from .interactions_proportion import calculate_interaction_proportion_metric
from .permute_graph import _generate_permuted_graph_and_calc_metric
from .shortest_path import calculate_shortest_path_metric

# --- Map metric names to functions ---
METRIC_FUNCTIONS: Dict[str, Callable[[nx.Graph, Any, Any], float]] = {
    "shortest_path": calculate_shortest_path_metric,
    "interactions": calculate_interactions_metric,
    "axb_motifs": calculate_axb_motifs_metric,
    "interaction_proportion": calculate_interaction_proportion_metric,
}


# --- Main Permutation Test Function ---
def graph_hypothesis(
    original_graph: nx.Graph,
    fixed_type: Any,
    target_type: Any,
    metric_name: str = "interactions",
    test_type: str = "two-sided",
    num_permutations: int = 1000,
    num_processes: Optional[int] = None,
    random_seed: Optional[int] = None,
) -> Dict[str, Any]:
    """
    Performs a permutation test to assess the significance of a graph metric
    between two node types, chosen by name.

    It compares an observed metric value to a null distribution generated by
    randomly shuffling the types of all nodes *except* those of the 'fixed_type'.

    Args:
        original_graph (nx.Graph): The observed graph. Must be undirected, unweighted,
                                   and all nodes must have a 'type' attribute.
        fixed_type (Any): The node type that remains fixed during permutations.
                          This also serves as one of the types for the metric calculation.
        target_type (Any): The other node type involved in the metric calculation.
                           Its nodes' types will be shuffled if they are not 'fixed_type'.
        metric_name (str): The name of the metric function to use. Choose from:
                           "shortest_path", "interactions", "axb_motifs", "interaction_proportion".
        num_permutations (int): The number of random permutations to perform to
                                generate the null distribution.
        num_processes (Optional[int]): The number of CPU processes to use for
                                       parallelizing permutations. If None, uses
                                       all available CPU cores.
        random_seed (Optional[int]): An optional seed for reproducibility. If provided,
                                     it seeds the overall randomness and ensures that
                                     the permutation test produces the same results
                                     across runs.

    Returns:
        Dict[str, Any]: A dictionary containing:
            - "test_type": The type of test requested ('lesser', 'greater' or 'two-sided')
            - "observed_statistic": The metric value calculated on the original graph.
            - "permutation_statistics": A list of metric values from all permuted graphs.
            - "p_value": The calculated p-value (one-sided: observed >= permuted).
            - "num_permutations": The total number of permutations performed.
            - "num_processes_used": The number of processes actually used.

    Raises:
        AssertionError:
            - If `metric_name` is not one of the recognized names.
            - If `test_type` is not one of the three hypothesis tests defined.
            - Propagates assertions from `validate_graph_properties`
              and chosen `metric_func`'s internal validations.
    """
    # Assert that the chosen metric_name is valid
    assert (
        metric_name in METRIC_FUNCTIONS
    ), f"Invalid metric_name: '{metric_name}'. Choose from {list(METRIC_FUNCTIONS.keys())}"

    valid_test_types = {"greater", "less", "two-sided"}
    assert (
        test_type in valid_test_types
    ), f"Invalid test_type: '{test_type}'. Choose from {list(valid_test_types)}."

    # Get the actual metric function based on the name
    metric_func = METRIC_FUNCTIONS[metric_name]

    # Determine number of processes to use and validate
    available_cpu_count = multiprocessing.cpu_count()
    actual_num_processes = 0  # Initialize

    if num_processes is None:
        actual_num_processes = available_cpu_count
    else:
        assert (
            isinstance(num_processes, int) and num_processes > 0
        ), "num_processes must be a positive integer or None."
        assert (
            num_processes <= available_cpu_count
        ), f"Requested num_processes ({num_processes}) exceeds available CPU count ({available_cpu_count})."
        actual_num_processes = num_processes

    # Issue a warning if user explicitly set fewer than available
    if num_processes is not None and actual_num_processes < available_cpu_count:
        warnings.warn(
            f"Using {actual_num_processes} processes, but {available_cpu_count} are available. "
            "Consider setting num_processes=None to utilize all available cores for potentially faster execution.",
            UserWarning,
        )

    # Issue a warning if there are too many permutations
    if num_permutations > 2000:
        warnings.warn(
            f"Using {num_permutations} permutations, may take a very long time."
            " Consider setting 2000 permutations",
            UserWarning,
        )

    print(
        f"Calculating observed statistic for {fixed_type}-{target_type} using '{metric_name}' metric..."
    )
    observed_statistic = metric_func(original_graph, fixed_type, target_type)
    print(f"Observed statistic: {observed_statistic}")

    if random_seed is not None:
        random.seed(random_seed)

    permutation_seeds = [random.randint(0, 2**32 - 1) for _ in range(num_permutations)]

    worker_args = [
        (original_graph, fixed_type, target_type, metric_func, seed)
        for seed in permutation_seeds
    ]

    if num_processes is None:
        num_processes = multiprocessing.cpu_count()
    if num_processes <= 0:
        num_processes = 1

    print(
        f"Performing {num_permutations} permutations using {num_processes} processes..."
    )
    pool = multiprocessing.Pool(processes=actual_num_processes)
    try:
        # Wrap pool.imap_unordered with tqdm for the progress bar
        # imap_unordered is good for progress bars as it yields results as they finish
        # desc provides a description, leave=True keeps the bar after completion
        permutation_statistics = list(
            tqdm(
                pool.imap_unordered(
                    _generate_permuted_graph_and_calc_metric, worker_args
                ),
                total=num_permutations,
                desc=f"Permutations ({metric_name})",
                unit="perm",
                leave=True,
            )
        )
    except Exception as e:
        print(f"An error occurred during multiprocessing: {e}")
        raise
    finally:
        pool.close()
        pool.join()
    print("Permutations complete.")

    # Calculate p-value (one-sided, observed >= permuted)
    # extreme_count = sum(1 for stat in permutation_statistics if stat >= observed_statistic)
    p_value = hypothesis_tester(observed_statistic, permutation_statistics, test_type)

    print(f"P-value: {p_value}")

    return {
        "test_type": test_type,
        "observed_statistic": observed_statistic,
        "permutation_statistics": permutation_statistics,
        "p_value": p_value,
        "num_permutations": num_permutations,
        "num_processes_used": num_processes,
    }
