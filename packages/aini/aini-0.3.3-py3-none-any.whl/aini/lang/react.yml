# defaults:
#   model:
#     class: langchain_deepseek.ChatDeepSeek
#     params:
#       model: deepseek-chat
#   tools:
#     - class: langchain_tavily.TavilySearch
#   checkpointer:
#     class: langgraph.checkpoint.memory.MemorySaver

agent:
  class: langgraph.prebuilt.create_react_agent
  params:
    name: ${name}
    model: ${model}
    # A list of tools or a ToolNode instance.
    tools: ${tools}
    # An optional prompt for the LLM. Can take a few different forms:
    # - str: This is converted to a SystemMessage and added to the beginning of the list of messages in state["messages"].
    # - SystemMessage: this is added to the beginning of the list of messages in state["messages"].
    # - Callable: This function should take in full graph state and the output is then passed to the language model.
    # - Runnable: This runnable should take in full graph state and the output is then passed to the language model.
    prompt: ${prompt}
    # An optional schema for the final agent output. Can be passed in as:
    # - an OpenAI function/tool schema,
    # - a JSON Schema,
    # - a TypedDict class,
    # - or a Pydantic class.
    # - a tuple (prompt, schema), where schema is one of the above.
    #   The prompt will be used together with the model that is being used to generate the structured response.
    response_format: ${response_format}
    # An optional node to add before the `agent` node (i.e., the node that calls the LLM).
    #   Useful for managing long message histories (e.g., message trimming, summarization, etc.).
    #   Pre-model hook must be a callable or a runnable that takes in current graph state and returns a state update in the form of
    pre_model_hook: ${pre_model_hook}
    # An optional state schema that defines graph state.
    #   Must have `messages` and `remaining_steps` keys.
    #   Defaults to `AgentState` that defines those two keys.
    state_schema: ${state_schema}
    # An optional schema for configuration.
    #   Use this to expose configurable parameters via agent.config_specs.
    config_schema: ${config_schema}
    # An optional checkpoint saver object. This is used for persisting
    #   the state of the graph (e.g., as chat memory) for a single thread (e.g., a single conversation).
    checkpointer: ${checkpointer}
    # An optional store object. This is used for persisting data
    #   across multiple threads (e.g., multiple conversations / users).
    store: ${store}
    # An optional list of node names to interrupt before.
    #   Should be one of the following: "agent", "tools".
    #   This is useful if you want to add a user confirmation or other interrupt before taking an action.
    interrupt_before: ${interrupt_before}
    # An optional list of node names to interrupt after.
    #   Should be one of the following: "agent", "tools".
    #   This is useful if you want to return directly or run additional processing on an output.
    interrupt_after: ${interrupt_after}
    debug: ${debug|false}
    # Determines the version of the graph to create. Can be one of:
    # - `"v1"`: The tool node processes a single message.
    #   All tool calls in the message are executed in parallel within the tool node.
    # - `"v2"`: The tool node processes a tool call.
    #   Tool calls are distributed across multiple instances of the tool node using the
    #   [Send](https://langchain-ai.github.io/langgraph/concepts/low_level/#send) API.
    version: ${version|'v1'}
