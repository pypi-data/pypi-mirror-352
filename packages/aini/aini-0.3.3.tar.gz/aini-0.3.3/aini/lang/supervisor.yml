# pip install langgraph-supervisor

defaults:
  state_schema:
    class: langgraph.prebuilt.chat_agent_executor.AgentState
    init: false

supervisor:
  class: langgraph_supervisor.create_supervisor
  params:
    # list[Pregel]
    # List of agents to manage.
    #   An agent can be a LangGraph [CompiledStateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.CompiledStateGraph),
    #   a functional API [workflow](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.entrypoint),
    #   or any other [Pregel](https://langchain-ai.github.io/langgraph/reference/pregel/#langgraph.pregel.Pregel) object.
    agents: ${agents}
    # LanguageModelLike
    model: ${model}
    # list[BaseTool | Callable]
    tools: ${tools}
    # Prompt
    # Optional prompt to use for the supervisor. Can be one of:
    # - str: This is converted to a SystemMessage and added to the beginning of the list of messages in state["messages"].
    # - SystemMessage: this is added to the beginning of the list of messages in state["messages"].
    # - Callable: This function should take in full graph state and the output is then passed to the language model.
    # - Runnable: This runnable should take in full graph state and the output is then passed to the language model.
    prompt: ${prompt}
    # Union[StructuredResponseSchema, tuple[str, StructuredResponseSchema]]
    # An optional schema for the final supervisor output.
    # If provided, output will be formatted to match the given schema and returned in the 'structured_response' state key.
    # If not provided, `structured_response` will not be present in the output state.
    # Can be passed in as:
    # - an OpenAI function/tool schema,
    # - a JSON Schema,
    # - a TypedDict class,
    # - or a Pydantic class.
    # - a tuple (prompt, schema), where schema is one of the above.
    #   The prompt will be used together with the model that is being used to generate the structured response.
    response_format: ${response_format}
    # Whether to allow the supervisor LLM to call tools in parallel (only OpenAI and Anthropic).
    #   Use this to control whether the supervisor can hand off to multiple agents at once.
    #   If True, will enable parallel tool calls.
    #   If False, will disable parallel tool calls (default).
    parallel_tool_calls: ${parallel_tool_calls|false}
    # StateSchemaType - State schema to use for the supervisor graph.
    state_schema: ${state_schema}
    # Type[Any]
    # An optional schema for configuration.
    #   Use this to expose configurable parameters via `supervisor.config_specs`.
    config_schema: ${config_schema}
    # OutputMode
    # Mode for adding managed agents' outputs to the message history in the multi-agent workflow. Can be one of:
    # - `full_history`: add the entire agent message history
    # - `last_message`: add only the last message (default)
    output_mode: ${output_mode|'last_message'}
    # Whether to add a pair of (AIMessage, ToolMessage) to the message history when a handoff occurs.
    add_handoff_messages: ${add_handoff_messages|true}
    # str
    # Optional prefix for the handoff tools (e.g., "delegate_to_" or "transfer_to_")
    #   If provided, the handoff tools will be named `handoff_tool_prefix_agent_name`.
    #   If not provided, the handoff tools will be named `transfer_to_agent_name`.
    handoff_tool_prefix: ${handoff_tool_prefix}
    # Whether to add a pair of (AIMessage, ToolMessage) to the message history
    #   when returning control to the supervisor to indicate that a handoff has occurred.
    add_handoff_back_messages: ${add_handoff_back_messages}
    supervisor_name: ${supervisor_name|'supervisor'}
    # AgentNameMode = Literal["inline"]
    # Use to specify how to expose the agent name to the underlying supervisor LLM.
    # - None: Relies on the LLM provider using the name attribute on the AI message. Currently, only OpenAI supports this.
    # - `"inline"`: Add the agent name directly into the content field of the AI message using XML-style tags.
    #   Example: `"How can I help you"` -> `"<name>agent_name</name><content>How can I help you?</content>"`
    include_agent_name: ${include_agent_name}
