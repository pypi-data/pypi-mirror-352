# tools/plugin_search.py

import requests
from bs4 import BeautifulSoup
import urllib.parse
from mcp.server import Server
from mcp.types import Tool, TextContent


# æ’ä»¶æœç´¢å·¥å…·å®ç°å‡½æ•° - ä¸å†åŒ…å«è£…é¥°å™¨ï¼Œç”±ç»Ÿä¸€å…¥å£è°ƒç”¨


async def _search_minecraft_plugins(arguments: dict) -> list[TextContent]:
    """æ‰§è¡ŒMinecraftæ’ä»¶æœç´¢"""
    
    keyword = arguments.get("keyword")
    limit = arguments.get("limit", 5)
    
    if not keyword:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šæœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º")]
    
    try:
        # æ„å»ºæœç´¢URL
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"https://www.minebbs.com/search/15498982/?q={encoded_keyword}&t=resource&c[categories][0]=63&c[categories][1]=70&c[categories][2]=75&c[child_categories]=1&o=relevance"
        base_url = "https://www.minebbs.com"
        
        # æ·»åŠ è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # å‘é€GETè¯·æ±‚
        response = requests.get(search_url, headers=headers, allow_redirects=True, timeout=10)
        
        if response.status_code != 200:
            return [TextContent(type="text", text=f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")]
        
        soup = BeautifulSoup(response.text, 'html.parser')
        all_results = soup.find_all('li', class_='block-row block-row--separated js-inlineModContainer')
        
        if not all_results:
            return [TextContent(type="text", text=f"ğŸ” æ²¡æœ‰æ‰¾åˆ°å…³äº '{keyword}' çš„æ’ä»¶æœç´¢ç»“æœ")]
        
        # è§£ææœç´¢ç»“æœ
        parsed_data = _parse_search_results(all_results[:limit], base_url)
        
        # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
        result_text = _format_search_results(keyword, len(all_results), parsed_data)
        
        return [TextContent(type="text", text=result_text)]
        
    except requests.RequestException as e:
        return [TextContent(type="text", text=f"âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}")]
    except Exception as e:
        return [TextContent(type="text", text=f"âŒ è§£æé”™è¯¯: {str(e)}")]


def _parse_search_results(results, base_url: str) -> list[dict]:
    """è§£ææœç´¢ç»“æœ"""
    
    parsed_data = []
    
    for item_index, item in enumerate(results):
        # 1. æ ‡é¢˜å’Œèµ„æºé“¾æ¥
        title_tag = item.find('h3', class_='contentRow-title')
        resource_link_tag = title_tag.find('a') if title_tag else None
        
        title = "N/A"
        resource_url = "N/A"
        
        if resource_link_tag and resource_link_tag.has_attr('href'):
            title = resource_link_tag.get_text(strip=True).replace('\n', ' ')
            resource_url = urllib.parse.urljoin(base_url, resource_link_tag['href'])
        
        # 2. ç®€ä»‹
        snippet_tag = item.find('div', class_='contentRow-snippet')
        snippet = snippet_tag.get_text(strip=True) if snippet_tag else "N/A"
        
        # 3. ä½œè€…ã€å‘å¸ƒæ—¥æœŸã€æ ‡ç­¾ã€åˆ†ç±»
        minor_info_tag = item.find('div', class_='contentRow-minor')
        author = "N/A"
        publish_date = "N/A"
        tags_list = []
        category = "N/A"
        
        if minor_info_tag:
            author_tag = minor_info_tag.find('a', class_='username')
            author = author_tag.get_text(strip=True) if author_tag else "N/A"
            
            time_tag = minor_info_tag.find('time', class_='u-dt')
            publish_date = time_tag.get_text(strip=True) if time_tag else "N/A"
            
            # æ ‡ç­¾
            tags_list = [tag.get_text(strip=True) for tag in minor_info_tag.find_all('span', class_='tagItem')]
            
            category_link_tag = minor_info_tag.find('a', href=lambda href: href and "/resources/categories/" in href)
            if category_link_tag:
                category = category_link_tag.get_text(strip=True)
        
        plugin_info = {
            "åºå·": item_index + 1,
            "æ ‡é¢˜": title,
            "æ’ä»¶é“¾æ¥": resource_url,
            "ç®€ä»‹": snippet[:200] + "..." if len(snippet) > 200 else snippet,
            "ä½œè€…": author,
            "å‘å¸ƒæ—¥æœŸ": publish_date,
            "æ ‡ç­¾": ", ".join(tags_list) if tags_list else "N/A",
            "åˆ†ç±»": category
        }
        parsed_data.append(plugin_info)
    
    return parsed_data


def _format_search_results(keyword: str, total_count: int, parsed_data: list[dict]) -> str:
    """æ ¼å¼åŒ–æœç´¢ç»“æœè¾“å‡º"""
    
    result_text = f"ğŸ” æœç´¢å…³é”®è¯: {keyword}\n"
    result_text += f"ğŸ“Š æ‰¾åˆ° {total_count} ä¸ªç»“æœï¼Œæ˜¾ç¤ºå‰ {len(parsed_data)} ä¸ª:\n\n"
    
    for plugin in parsed_data:
        result_text += f"ã€{plugin['åºå·']}ã€‘{plugin['æ ‡é¢˜']}\n"
        result_text += f"ğŸ‘¤ ä½œè€…: {plugin['ä½œè€…']}\n"
        result_text += f"ğŸ“… å‘å¸ƒæ—¥æœŸ: {plugin['å‘å¸ƒæ—¥æœŸ']}\n"
        result_text += f"ğŸ·ï¸ æ ‡ç­¾: {plugin['æ ‡ç­¾']}\n"
        result_text += f"ğŸ“‚ åˆ†ç±»: {plugin['åˆ†ç±»']}\n"
        result_text += f"ğŸ“ ç®€ä»‹: {plugin['ç®€ä»‹']}\n"
        result_text += f"ğŸ”— é“¾æ¥: {plugin['æ’ä»¶é“¾æ¥']}\n"
        result_text += "-" * 50 + "\n\n"
    
    return result_text