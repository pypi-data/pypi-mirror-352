Metadata-Version: 2.1
Name: sparseml
Version: 1.9.0
Summary: [DEPRECATED] Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models
Home-page: https://github.com/neuralmagic/sparseml
Author: Neuralmagic, Inc.
License: Apache
Keywords: inference,machine learning,neural network,computer vision,nlp,cv,deep learning,torch,pytorch,tensorflow,keras,sparsity,pruning,deep learning libraries,onnx,quantization,automl
Classifier: Development Status :: 7 - Inactive
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Information Technology
Classifier: Intended Audience :: Science/Research
Classifier: Operating System :: POSIX :: Linux
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Scientific/Engineering :: Mathematics
Classifier: Topic :: Software Development
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.8.0,<3.12
Description-Content-Type: text/markdown
License-File: LICENSE
License-File: LICENSE-ULTRALYTICS
License-File: NOTICE

<!--
Copyright (c) 2021 - 2025 / Neuralmagic, Inc. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<h1 style="display: flex; align-items: center;" >
     <img width="100" height="100" alt="tool icon" src="https://neuralmagic.com/wp-content/uploads/2024/03/icon_SparseML-002.svg" />
      <span>&nbsp;&nbsp;SparseML</span>
  </h1>

<h3>Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models</h3>

## ðŸš¨ 2025 End of Life Announcement: DeepSparse, SparseML, SparseZoo, and Sparsify

Dear Community,

Weâ€™re reaching out with heartfelt thanks and important news. Following [Neural Magicâ€™s acquisition by Red Hat in January 2025](https://www.redhat.com/en/about/press-releases/red-hat-completes-acquisition-neural-magic-fuel-optimized-generative-ai-innovation-across-hybrid-cloud), weâ€™ve shifted our focus to commercial and open-source offerings built around [vLLM (virtual large language models)](https://www.redhat.com/en/topics/ai/what-is-vllm).

As part of this transition, we ceased development and deprecated the community versions of **DeepSparse (including DeepSparse Enterprise), SparseML, SparseZoo, and Sparsify on June 2, 2025**. These tools no longer will receive updates or support.

From day one, our mission was to democratize AI through efficient, accessible tools. Weâ€™ve learned so much from your feedback, creativity, and collaborationâ€”watching these tools become vital parts of your ML journeys has meant the world to us.

Though weâ€™ve wound down the community editions, we remain committed to our original values. Now as part of Red Hat, weâ€™re excited to evolve our work around vLLM and deliver even more powerful solutions to the ML community.

_With gratitude, The Neural Magic Team (now part of Red Hat)_

---
