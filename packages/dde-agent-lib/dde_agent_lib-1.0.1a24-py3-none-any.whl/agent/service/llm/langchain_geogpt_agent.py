import asyncio
import json
import os
import re
from io import StringIO
from typing import (
    Any,
    AsyncIterator,
    Dict,
    Iterator,
    List,
    Optional, Union, Type,
)

import requests
from langchain_core.callbacks import (
    AsyncCallbackManagerForLLMRun,
    CallbackManagerForLLMRun,
)
from langchain_core.language_models import LanguageModelInput
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import GenerationChunk
from langchain_core.pydantic_v1 import Field
from langchain_core.runnables import Runnable
from pydantic import BaseModel
from ratelimit import limits

from agent.exception.custom_exception import CommonException
from agent.init_env_val import source
from agent.service.llm.custom_model import CustomModel
from agent.utils.dde_logger import dde_logger as logger

global env_source
env_source = source

class LangchainGeoGPTAgent(LLM, CustomModel):
    def __init__(self, endpoint, *, streaming=True, model="Geogpt", system="", history=None, source: str = ""):
        if history is None:
            history = []
        kwargs = {"endpoint": endpoint, " streaming": streaming, "model": model, "system": system, "history": history,
                  "source": source}
        super().__init__(**kwargs)
        self.endpoint = endpoint
        self.streaming = streaming
        self.model = model
        self.system = system
        history_format = []
        for item in history:
            if item[1] is not None:
                tmp_history = {
                    "user": item[0],
                    "bot": item[1]
                }
                history_format.append(tmp_history)
        self.history = history_format
        if source != "":
            self.source = source
        else:
            global env_source
            self.source = env_source

    init_kwargs: Dict[str, Any] = Field(default_factory=dict)
    """init kwargs for qianfan client init, such as `query_per_second` which is 
        associated with qianfan resource object to limit QPS"""

    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    """extra params for model invoke using with `do`."""

    client: Any

    streaming: Optional[bool] = True
    """Whether to stream the results or not."""

    model: str = "GeoGPT"
    """Model name. 
    `model` will be ignored if `endpoint` is set
    """
    is_ok: bool = False  # 是否正确返回reference
    endpoint: Optional[str]
    """Endpoint of the GeoGPT LLM, required if custom model used."""

    request_timeout: Optional[int] = 60
    """request timeout for chat http requests"""
    prompt: Optional[str]
    system: Optional[str]
    history: Optional[list[list[str]]]

    top_p: Optional[float] = 0.8
    temperature: Optional[float] = 0.2
    frequency_penalty: Optional[float] = 0.0
    best_of: Optional[int] = 1
    use_beam_search: Optional[bool] = False
    presence_penalty: Optional[float] = 1.0
    top_k: Optional[int] = -1
    length_penalty: Optional[float] = 1.0
    call_limits = 10
    source: str


    @property
    def _identifying_params(self) -> Dict[str, Any]:
        return {
            **{"endpoint": self.endpoint, "model": self.model},
            **super()._identifying_params,
        }

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "GeoGPT"

    @property
    def _default_params(self) -> Dict[str, Any]:
        """Get the default parameters for calling Qianfan API."""
        normal_params = {
            "system": self.system,
            "history": self.history,
            "endpoint": self.endpoint,
            "stream": self.streaming,
            "source": self.source
        }

        return {**normal_params, **self.model_kwargs}

    def _convert_prompt_msg_params(
            self,
            prompt: str,
            **kwargs: Any,
    ) -> dict:
        if "streaming" in kwargs:
            kwargs["stream"] = kwargs.pop("streaming")
        return {
            **{"prompt": prompt},
            **self._default_params,
            **kwargs,
        }

    #@limits(calls=call_limits, period=1, raise_on_limit=True)
    def _call(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[CallbackManagerForLLMRun] = None,
            **kwargs: Any,
    ) -> str:
        """Call out to an LLM models endpoint for each generation with a prompt.
        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
        Returns:
            The string generated by the model.
        """
        if self.streaming:
            return ""
        params = self._convert_prompt_msg_params(prompt, **kwargs)
        response_payload = self.http_request(**params)
        return response_payload["data"]["output"]

    async def _acall(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
            **kwargs: Any,
    ) -> str:
        if self.streaming:
            stream_iter = self._astream(prompt, stop, run_manager, **kwargs)
            res = ""
            async for msg in stream_iter:
                res += msg
            return res
        return self._call(prompt, stop, run_manager, **kwargs)

    def _stream(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[CallbackManagerForLLMRun] = None,
            **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        pass

    @limits(calls=call_limits, period=1, raise_on_limit=True)
    async def _astream(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
            **kwargs: Any,
    ) -> AsyncIterator[GenerationChunk]:
        params = self._convert_prompt_msg_params(prompt, **{**kwargs, "stream": True})
        byte_arr = 1024
        llm_stream = self.http_request(**params)
        if llm_stream.status_code != 200:
            logger.error(f"调用大模型{self.model}出错，返回为[{llm_stream}]")
            raise CommonException()
        stream_bytes = llm_stream.raw.read(byte_arr)
        # 接收reference
        reference_byte = b""
        reference_split_byte = b',"output":""}'
        max_retry_time = 10  # 最多试10次，如果没有reference结尾，就按照普通流式处理
        while max_retry_time > 0 and stream_bytes:
            reference_byte += stream_bytes
            if reference_split_byte in reference_byte:
                index = reference_byte.find(reference_split_byte)
                stream_bytes = reference_byte[index + len(reference_split_byte):]
                reference_byte = reference_byte[len('{"context":'):index]
                chunk = GenerationChunk(text=reference_byte.decode("utf-8"))
                self.is_ok = True
                yield chunk
                break
            max_retry_time -= 1
            stream_bytes = llm_stream.raw.read(byte_arr)
        if not self.is_ok:
            stream_bytes = reference_byte
        # 接收正常大模型返回
        buffer = StringIO()
        deal_index = 0
        result = ""
        tmp_str = ""
        split_str = "\"}\x00{\"output\": \""  # 流式间隔字符
        tmp_byte = b''
        pattern = "\[\[citation:(\d+)\]\]"
        pattern2 = "\[citation:(\d+)\]"
        replacement = r"<sup>[\1]</sup>"  # 前端上标标签
        # stream_bytes = tmp_byte
        is_chinese = "\\u" in str(stream_bytes)
        while stream_bytes:
            if is_chinese:
                # 收到的中文是经过转码的，如\\u5b66，但是接收到的的字节是可能是任意截取的，需要去掉不完整的部分再转码
                index = -1
                stream_bytes = tmp_byte + stream_bytes
                while True:
                    try:
                        text = stream_bytes[:index].decode('unicode_escape')
                    except:
                        index -= 1
                    else:
                        tmp_byte = stream_bytes[index:]
                        break
            else:
                text = stream_bytes.decode("utf-8")
            # chunk = GenerationChunk(text=text)
            tmp_str += text
            count_ = tmp_str.count(split_str)
            if count_ >= 1:
                tmp_list = tmp_str.split(split_str)
                selected_str = tmp_list[-2]
                if len(tmp_list[-1]) > len(tmp_list[-2]) and '"' not in tmp_list[-1]:
                    selected_str = tmp_list[-1]
                if "\x00" in selected_str:
                    selected_str = ""
                buffer.seek(0)
                buffer.write(selected_str)
                tmp_str = tmp_list[-1]
            else:
                index = tmp_str[-len(split_str):].find('\"')
                compared_index = len(tmp_str)
                if index != -1:
                    compared_index = index + len(tmp_str) - len(split_str)
                if compared_index > len(buffer.getvalue()):
                    buffer.seek(0)
                    buffer.write(tmp_str[:compared_index])
                    tmp_str = tmp_str[compared_index:]
                else:
                    stream_bytes = llm_stream.raw.read(byte_arr)
                    continue
            result = buffer.getvalue()
            # 替换字符串中的[[citation:*]]为<sup>[*]</sup>
            result = re.sub(pattern, replacement, result)
            result = re.sub(pattern2, replacement, result)
            chunk = GenerationChunk(text=result)
            yield chunk
            # deal_index, res = self.deal_str(deal_index, buffer)
            # if res != "":
            #     # 替换字符串中的[[citation:*]]为<sup>[*]</sup>
            #     res = re.sub(pattern, replacement, res)
            #     res = re.sub(pattern2, replacement, res)
            #     result += res
            #     chunk = GenerationChunk(text=result)
            #     yield chunk
            # if run_manager:
            #     await run_manager.on_llm_new_token(chunk.text)
            stream_bytes = llm_stream.raw.read(byte_arr)

    @staticmethod
    def http_request(prompt, system, history, endpoint,
                     stream, source):  # endpoint, stream, request_timeout, top_p, temperature,
        # data = {
        #     "input": prompt,
        #     "history": history,
        #     "serviceParams": {
        #         "promptTemplateName": "geogpt",
        #         "stream": stream
        #     }
        # }
        data = {
            "generateStyle": "chat",
            "input": prompt,
            "maxWindowSize": 3000,
            "history": history,
            "serviceParams": {
                "maxContentRound": 0,
                "maxOutputLength": 3000,
                "maxWindowSize": 500,
                "stream": stream,
                "system": system,
                "promptTemplateName": "geogpt_customized",
                "temperature": 0.0,
                "generateStyle": "chat"
            },
            "modelParams": {
                "best_of": 1,
                "temperature": 0.2,
                "use_beam_search": False,
                "presence_penalty": 0.0,
                "frequency_penalty": 0.0,
                "top_p": 0.8,
                "top_k": -1,
                "length_penalty": 1.0
            }
        }
        data_log = {
            "generateStyle": "chat",
            "input": prompt[:2000],
            "maxWindowSize": 3000,
            "history": history,
            "serviceParams": {
                "maxContentRound": 0,
                "maxOutputLength": 3000,
                "maxWindowSize": 500,
                "stream": stream,
                "system": system,
                "promptTemplateName": "geogpt_customized",
                "temperature": 0.0,
                "generateStyle": "chat"
            },
            "modelParams": {
                "best_of": 1,
                "temperature": 0.2,
                "use_beam_search": False,
                "presence_penalty": 0.0,
                "frequency_penalty": 0.0,
                "top_p": 0.8,
                "top_k": -1,
                "length_penalty": 1.0
            }
        }
        logger.info(f"调用大模型的source为: {source}, endpoint为：{endpoint}, 参数为：{data_log}")
        headers = {
            'Content-Type': 'application/json',
            'Accept': 'text/event-stream'
        }
        with requests.Session() as session:
            response = session.post(endpoint, json=data, headers=headers, stream=stream)
            if stream:
                return response
            else:
                return json.loads(response.content)

    def deal_str(self, deal_index: int, str_io: StringIO):
        str_io.seek(deal_index)
        target = [".", "!", "?", "。", "！", "？"]
        result = self.read_until_string(str_io, target)
        # logger.info(f"temp return: {result}")
        deal_index += len(result)
        return deal_index, result

    def with_structured_output(self, schema: Union[Dict, Type[BaseModel]], **kwargs: Any) -> Runnable[
        LanguageModelInput, Union[Dict, BaseModel]]:
        pass

    @staticmethod
    def read_until_string(buffer, target: list[str]):
        cur_position = buffer.tell()
        # 读取足够长的字符串
        content = buffer.read(1024)
        # 在读取到的内容中查找目标字符串
        last_index = -1
        for item_ in target:
            last_index = max(content.rfind(item_), last_index)
        if last_index != -1:
            # 将多读取的部分放回缓冲区
            buffer.seek(last_index + cur_position + 1)
            return content[:last_index + 1]
        else:
            return ""

    @staticmethod
    def deal_prompt(prompt: str):
        tem_prompt = prompt.split("#System#")[-1]
        prompt_list = tem_prompt.split("#Input#")
        return prompt_list[0].strip(), prompt_list[-1].strip()



