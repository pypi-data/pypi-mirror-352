import asyncio
import json
import os
from io import StringIO
from typing import (
    Any,
    AsyncIterator,
    Dict,
    Iterator,
    List,
    Optional, Union, Type,
)
import requests
from langchain_core.callbacks import (
    AsyncCallbackManagerForLLMRun,
    CallbackManagerForLLMRun,
)
from langchain_core.language_models import LanguageModelInput
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import GenerationChunk
from langchain_core.pydantic_v1 import Field
from langchain_core.runnables import Runnable
from pydantic import BaseModel
from ratelimit import limits, sleep_and_retry

from agent.init_env_val import llm_limit
from agent.service.llm.custom_model import CustomModel
from agent.utils.dde_logger import dde_logger as logger

from agent.utils.nacos_val import get_system_config_from_nacos

global call_limits
global llm_call_period
try:
    system_config = get_system_config_from_nacos()
    call_limits = system_config['limits']['call_limits']
    llm_call_period = system_config['limits']['llm_call_period']
except Exception as e:
    call_limits = llm_limit
    llm_call_period = 1


class LangchainGeoGPT(LLM, CustomModel):
    def __init__(self, endpoint, *, streaming=True, model="Geogpt", system="", history=None):
        if history is None:
            history = []
        kwargs = {"endpoint": endpoint, " streaming": streaming, "model": model, "system": system, "history": history}
        super().__init__(**kwargs)
        self.endpoint = endpoint.strip()
        self.streaming = streaming
        self.model = model
        self.system = system
        self.history = history

    init_kwargs: Dict[str, Any] = Field(default_factory=dict)
    """init kwargs for qianfan client init, such as `query_per_second` which is 
        associated with qianfan resource object to limit QPS"""

    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    """extra params for model invoke using with `do`."""

    client: Any

    streaming: Optional[bool] = True
    """Whether to stream the results or not."""

    model: str = "GeoGPT"
    """Model name. 
    `model` will be ignored if `endpoint` is set
    """

    endpoint: Optional[str]
    """Endpoint of the GeoGPT LLM, required if custom model used."""

    request_timeout: Optional[int] = 60
    """request timeout for chat http requests"""
    prompt: Optional[str]
    system: Optional[str]
    history: Optional[list[list[str]]]

    top_p: Optional[float] = 0.8
    temperature: Optional[float] = 0.2
    frequency_penalty: Optional[float] = 0.0
    best_of: Optional[int] = 1
    use_beam_search: Optional[bool] = False
    presence_penalty: Optional[float] = 1.0
    top_k: Optional[int] = -1
    length_penalty: Optional[float] = 1.0
    is_ok: bool = False  # 默认没有reference

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        return {
            **{"endpoint": self.endpoint, "model": self.model},
            **super()._identifying_params,
        }

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "GeoGPT"

    @property
    def _default_params(self) -> Dict[str, Any]:
        """Get the default parameters for calling Qianfan API."""
        normal_params = {
            "system": self.system,
            "history": self.history,
            "endpoint": self.endpoint,
            "stream": self.streaming,
        }

        return {**normal_params, **self.model_kwargs}

    def _convert_prompt_msg_params(
            self,
            prompt: str,
            **kwargs: Any,
    ) -> dict:
        if "streaming" in kwargs:
            kwargs["stream"] = kwargs.pop("streaming")
        return {
            **{"prompt": prompt},
            **self._default_params,
            **kwargs,
        }

    def _call(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[CallbackManagerForLLMRun] = None,
            **kwargs: Any,
    ) -> str:
        """Call out to an LLM models endpoint for each generation with a prompt.
        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
        Returns:
            The string generated by the model.
        """
        if self.streaming:
            return ""
        params = self._convert_prompt_msg_params(prompt, **kwargs)
        response_payload = self.http_request(**params)
        logger.debug(f"response_payload返回为[{response_payload}]")
        return response_payload["data"]["output"]

    @limits(calls=call_limits, period=llm_call_period, raise_on_limit=True)
    async def _acall(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
            **kwargs: Any,
    ) -> str:
        logger.info(
            f"current time for LangchainGeoGPT._acall limits {call_limits} in {llm_call_period}s")
        if self.streaming:
            stream_iter = self._astream(prompt, stop, run_manager, **kwargs)
            res = ""
            async for msg in stream_iter:
                res += msg
            return res
        return self._call(prompt, stop, run_manager, **kwargs)

    def _stream(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[CallbackManagerForLLMRun] = None,
            **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        pass

    @limits(calls=call_limits, period=llm_call_period, raise_on_limit=True)
    async def _astream(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
            **kwargs: Any,
    ) -> AsyncIterator[GenerationChunk]:
        logger.info(
            f"current time for LangchainGeoGPT._astream limits {call_limits} in {llm_call_period}s")
        params = self._convert_prompt_msg_params(prompt, **{**kwargs, "stream": True})
        byte_arr = 1024
        llm_stream = self.http_request(**params)
        stream_bytes = llm_stream.raw.read(byte_arr)
        buffer = StringIO()
        deal_index = 0
        result = ""
        tmp_str = ""
        split_str = "\"}\x00{\"output\": \""
        tmp_byte = b''
        is_chinese = "\\u" in str(stream_bytes)
        # print(f"is_chinese: {is_chinese}", file=f)
        while stream_bytes:
            # print(f"stream_byte: {stream_bytes}", file=f)
            if is_chinese:
                index = -1
                stream_bytes = tmp_byte + stream_bytes
                while True:
                    try:
                        text = stream_bytes[:index].decode('unicode_escape')
                    except:
                        index -= 1
                    else:
                        # print(f"text: {text}", file=f)
                        tmp_byte = stream_bytes[index:]
                        break
            else:
                text = stream_bytes.decode("utf-8")
                # print(f"text: {text}", file=f)
            chunk = GenerationChunk(text=text)
            tmp_str += text
            count_ = tmp_str.count(split_str)
            if count_ >= 1:
                tmp_list = tmp_str.split(split_str)
                selected_str = tmp_list[-2]
                if len(tmp_list[-1]) > len(tmp_list[-2]) and '"' not in tmp_list[-1]:
                    selected_str = tmp_list[-1]
                buffer.seek(0)
                buffer.write(selected_str)
                tmp_str = tmp_list[-1]
            else:
                index = tmp_str[-len(split_str):].find('\"')
                compared_index = len(tmp_str)
                if index != -1:
                    compared_index = index + len(tmp_str) - len(split_str)

                if compared_index > len(buffer.getvalue()):
                    buffer.seek(0)
                    buffer.write(tmp_str[:compared_index])
                    # print(f"tmp_str: {tmp_str}", file=f)
                    # print(f"deal_index: {deal_index}", file=f)
                    tmp_str = tmp_str[compared_index:]
                else:
                    stream_bytes = llm_stream.raw.read(byte_arr)
                    continue
            result = buffer.getvalue()
            chunk = GenerationChunk(text=result)
            yield chunk
            # deal_index, res = self.deal_str(deal_index, buffer)
            # if res != "":
            #     result += res
            #     chunk = GenerationChunk(text=result)
            #     yield chunk
            # if run_manager:
            #     await run_manager.on_llm_new_token(chunk.text)
            stream_bytes = llm_stream.raw.read(byte_arr)
        # f.flush()

    @staticmethod
    def http_request(prompt, system, history, endpoint,
                     stream):  # endpoint, stream, request_timeout, top_p, temperature,
        data = {
            "generateStyle": "chat",
            "input": prompt,
            "maxWindowSize": 3000,
            "history": history,
            "serviceParams": {
                "maxContentRound": 0,
                "maxOutputLength": 3000,
                "maxWindowSize": 500,
                "stream": stream,
                "system": system,
                "promptTemplateName": "geogpt_doc_extract",
                "generateStyle": "chat"
            },
            "modelParams": {
                "best_of": 1,
                "temperature": 0.2,
                "use_beam_search": False,
                "presence_penalty": 0.0,
                "frequency_penalty": 0.0,
                "top_p": 0.8,
                "top_k": -1,
                "length_penalty": 1.0
            }
        }
        headers = {
            'Content-Type': 'application/json',
            'Accept': 'text/event-stream'
        }
        with requests.Session() as session:
            response = session.post(endpoint, json=data, headers=headers, stream=stream)
            if stream:
                return response
            else:
                return json.loads(response.content)

    def deal_str(self, deal_index: int, str_io: StringIO):
        str_io.seek(deal_index)
        target = [".", "!", "?", "。", "！", "？"]
        result = self.read_until_string(str_io, target)
        logger.debug(f"temp return: {result}")
        deal_index += len(result)
        # print(f"result: {result}", file=f)
        # print(f"deal_index: {deal_index}", file=f)
        return deal_index, result

    def with_structured_output(self, schema: Union[Dict, Type[BaseModel]], **kwargs: Any) -> Runnable[
        LanguageModelInput, Union[Dict, BaseModel]]:
        pass

    @staticmethod
    def read_until_string(buffer, target: list[str]):
        cur_position = buffer.tell()
        # 读取足够长的字符串
        content = buffer.read(1024)
        # print(f"content: {content}", file=f)
        # 在读取到的内容中查找目标字符串
        last_index = -1
        for item_ in target:
            last_index = max(content.rfind(item_), last_index)
        # print(f"last_index: {last_index}", file=f)
        if last_index != -1:
            # 将多读取的部分放回缓冲区
            buffer.seek(last_index + cur_position + 1)
            return content[:last_index + 1]
        else:
            return ""

    @staticmethod
    def deal_prompt(prompt: str):
        tem_prompt = prompt.split("#System#")[-1]
        prompt_list = tem_prompt.split("#Input#")
        return prompt_list[0].strip(), prompt_list[-1].strip()


