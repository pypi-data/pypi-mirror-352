import abc
import asyncio
import logging
from dataclasses import dataclass
from inspect import signature
from pathlib import Path
from typing import Any, AsyncGenerator, Generator, Optional, Union, cast

from asgiref.sync import async_to_sync
from django.core.checks import Error
from django.core.files import File
from django.template import Context, Template, TemplateDoesNotExist
from django.template.loader import get_template

from .settings import llm_settings
from .types import (
    ChainReply,
    Embed,
    EmbedList,
    LLMChatModelType,
    LLMEmbeddingModelType,
    Message,
    Reply,
)

logger = logging.getLogger(__name__)


class TemplateDict(dict):
    """í…œí”Œë¦¿ ë³€ìˆ˜ ì¤‘ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ëŠ” ì›ë˜ í˜•íƒœ({key})ë¡œ ìœ ì§€í•˜ëŠ” ë”•ì…”ë„ˆë¦¬"""

    def __missing__(self, key):
        return "{" + key + "}"


@dataclass
class DescribeImageRequest:
    image: Union[str, Path, File]
    image_path: str
    system_prompt: Union[str, Template]
    user_prompt: Union[str, Template]
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    prompt_context: Optional[dict[str, Any]] = None


class BaseLLM(abc.ABC):
    EMBEDDING_DIMENSIONS = {}

    def __init__(
        self,
        model: LLMChatModelType = "gpt-4o-mini",
        embedding_model: LLMEmbeddingModelType = "text-embedding-3-small",
        temperature: float = 0.2,
        max_tokens: int = 1000,
        system_prompt: Optional[Union[str, Template]] = None,
        prompt: Optional[Union[str, Template]] = None,
        output_key: str = "text",
        initial_messages: Optional[list[Message]] = None,
        api_key: Optional[str] = None,
        tools: Optional[list] = None,
    ):
        self.model = model
        self.embedding_model = embedding_model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.system_prompt = system_prompt
        self.prompt = prompt
        self.output_key = output_key
        self.history = initial_messages or []
        self.api_key = api_key

        # ê¸°ë³¸ ë„êµ¬ ì„¤ì •
        self.default_tools = []
        if tools:
            # tools ëª¨ë“ˆì„ ë™ì  import (ìˆœí™˜ import ë°©ì§€)
            from .tools import ToolAdapter

            self.default_tools = ToolAdapter.adapt_tools(tools)

    def check(self) -> list[Error]:
        return []

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(model={self.model}, embedding_model={self.embedding_model}, temperature={self.temperature}, max_tokens={self.max_tokens})"

    def __len__(self) -> int:
        return len(self.history)

    def __or__(self, next_llm: Union["BaseLLM", "SequentialChain"]) -> "SequentialChain":
        if isinstance(next_llm, BaseLLM):
            return SequentialChain(self, next_llm)
        elif isinstance(next_llm, SequentialChain):
            next_llm.insert_first(self)
            return next_llm
        else:
            raise TypeError("next_llm must be an instance of BaseLLM or SequentialChain")

    def __ror__(self, prev_llm: Union["BaseLLM", "SequentialChain"]) -> "SequentialChain":
        if isinstance(prev_llm, BaseLLM):
            return SequentialChain(prev_llm, self)
        elif isinstance(prev_llm, SequentialChain):
            prev_llm.append(self)
            return prev_llm
        else:
            raise TypeError("prev_llm must be an instance of BaseLLM or SequentialChain")

    def clear(self):
        """Clear the chat history"""
        self.history = []

    def _process_template(self, template: Union[str, Template], context: dict[str, Any]) -> Optional[str]:
        """í…œí”Œë¦¿ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê³µí†µ ë©”ì„œë“œ"""
        # Django Template ê°ì²´ì¸ ê²½ìš°
        if hasattr(template, "render"):
            logger.debug("using template render : %s", template)
            return template.render(Context(context))

        # ë¬¸ìì—´ì¸ ê²½ìš°
        elif isinstance(template, str):
            # íŒŒì¼ ê¸°ë°˜ í…œí”Œë¦¿ ì²˜ë¦¬
            if "prompts/" in template and template.endswith((".txt", ".md", ".yaml")):
                try:
                    template_obj = get_template(template)
                    logger.debug("using template render : %s", template)
                    return template_obj.render(context)
                except TemplateDoesNotExist:
                    logger.debug("Template '%s' does not exist", template)
                    return None
            # ì¥ê³  í…œí”Œë¦¿ ë¬¸ë²•ì˜ ë¬¸ìì—´
            elif "{{" in template or "{%" in template:
                logger.debug("using string template render : %s ...", repr(template))
                return Template(template).render(Context(context))
            # ì¼ë°˜ ë¬¸ìì—´ í¬ë§·íŒ… - ì¡´ì¬í•˜ëŠ” í‚¤ë§Œ ì¹˜í™˜í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            if context:
                try:
                    return template.format_map(TemplateDict(context))
                except Exception as e:
                    logger.debug("Template formatting failed: %s", e)
                    return template
            return template

        return None

    def get_system_prompt(self, input_context: dict[str, Any], default: Any = None) -> Optional[str]:
        if not self.system_prompt:
            return default

        return self._process_template(self.system_prompt, input_context)

    def get_human_prompt(self, input: Union[str, dict[str, Any]], context: dict[str, Any]) -> str:
        if isinstance(input, (str, Template)) or hasattr(input, "render"):
            result = self._process_template(input, context)
            if result is not None:
                return result

        elif isinstance(input, dict):
            if self.prompt:
                # promptê°€ ìˆìœ¼ë©´ í…œí”Œë¦¿ ë Œë”ë§
                result = self._process_template(self.prompt, context)
                if result is not None:
                    return result
            else:
                # promptê°€ ì—†ìœ¼ë©´ dictë¥¼ ìë™ìœ¼ë¡œ í¬ë§·íŒ…
                # contextì—ì„œ 'user_message' í‚¤ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
                if "user_message" in context:
                    return str(context["user_message"])
                # ì•„ë‹ˆë©´ dictì˜ ë‚´ìš©ì„ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
                formatted_parts = []
                for key, value in context.items():
                    if key not in ["choices", "choices_formatted", "choices_optional", "original_choices"]:
                        formatted_parts.append(f"{key}: {value}")
                return "\n".join(formatted_parts) if formatted_parts else ""

        raise ValueError(f"input must be a str, Template, or dict, but got {type(input)}")

    def _update_history(
        self,
        human_message: Message,
        ai_message: Union[str, Message],
    ) -> None:
        if isinstance(ai_message, str):
            ai_message = Message(role="assistant", content=ai_message)

        self.history.extend(
            [
                human_message,
                ai_message,
            ]
        )

    def get_output_key(self) -> str:
        return self.output_key

    def _process_choice_response(
        self, text: str, choices: list[str], choices_optional: bool
    ) -> tuple[Optional[str], Optional[int], float]:
        """
        ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ choiceë¥¼ ì¶”ì¶œí•˜ê³  ê²€ì¦

        Returns:
            (choice, index, confidence) íŠœí”Œ
        """
        # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„ (OpenAI, Google ë“±)
        try:
            import json

            data = json.loads(text)
            if isinstance(data, dict) and "choice" in data:
                choice = data["choice"]
                if choice in choices:
                    return choice, choices.index(choice), data.get("confidence", 1.0)
        except (json.JSONDecodeError, KeyError, TypeError):
            pass

        # í…ìŠ¤íŠ¸ ë§¤ì¹­
        text_clean = text.strip()

        # ì •í™•í•œ ë§¤ì¹­
        if text_clean in choices:
            return text_clean, choices.index(text_clean), 1.0

        # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­
        text_lower = text_clean.lower()
        for i, choice in enumerate(choices):
            if choice.lower() == text_lower:
                return choice, i, 0.9

        # ë¶€ë¶„ ë§¤ì¹­
        for i, choice in enumerate(choices):
            if choice in text_clean or text_clean in choice:
                logger.warning("Partial match found. Response: '%s', Matched: '%s'", text_clean, choice)
                return choice, i, 0.7

        # choices_optionalì´ Trueì´ê³  "None of the above"ê°€ í¬í•¨ëœ ê²½ìš°
        if choices_optional and ("none of the above" in text_lower or "í•´ë‹¹ ì—†ìŒ" in text_clean):
            return None, None, 0.8

        # ë§¤ì¹­ ì‹¤íŒ¨
        logger.warning("No valid choice found in response: %s", text_clean)
        return None, None, 0.0

    @abc.abstractmethod
    def _make_request_params(
        self,
        input_context: dict[str, Any],
        human_message: Message,
        messages: list[Message],
        model: LLMChatModelType,
    ) -> dict:
        pass

    @abc.abstractmethod
    def _make_ask(
        self,
        input_context: dict[str, Any],
        human_message: Message,
        messages: list[Message],
        model: LLMChatModelType,
    ) -> Reply:
        """Generate a response using the specific LLM provider"""
        pass

    @abc.abstractmethod
    async def _make_ask_async(
        self,
        input_context: dict[str, Any],
        human_message: Message,
        messages: list[Message],
        model: LLMChatModelType,
    ) -> Reply:
        """Generate a response asynchronously using the specific LLM provider"""
        pass

    @abc.abstractmethod
    def _make_ask_stream(
        self,
        input_context: dict[str, Any],
        human_message: Message,
        messages: list[Message],
        model: LLMChatModelType,
    ) -> Generator[Reply, None, None]:
        """Generate a streaming response using the specific LLM provider"""
        yield Reply(text="")

    @abc.abstractmethod
    async def _make_ask_stream_async(
        self,
        input_context: dict[str, Any],
        human_message: Message,
        messages: list[Message],
        model: LLMChatModelType,
    ) -> AsyncGenerator[Reply, None]:
        """Generate a streaming response asynchronously using the specific LLM provider"""
        yield Reply(text="")

    def _ask_impl(
        self,
        input: Union[str, dict[str, str]],
        files: Optional[list[Union[str, Path, File]]] = None,
        model: Optional[LLMChatModelType] = None,
        context: Optional[dict[str, Any]] = None,
        *,
        choices: Optional[list[str]] = None,
        choices_optional: bool = False,
        is_async: bool = False,
        stream: bool = False,
        use_history: bool = True,
        raise_errors: bool = False,
        enable_cache: bool = False,
    ):
        """ë™ê¸° ë˜ëŠ” ë¹„ë™ê¸° ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ (ì¼ë°˜/ìŠ¤íŠ¸ë¦¬ë°)"""
        current_messages = [*self.history] if use_history else []
        current_model: LLMChatModelType = cast(LLMChatModelType, model or self.model)

        if isinstance(input, dict):
            input_context = input
        else:
            input_context = {}

        if context:
            input_context.update(context)

        # enable_cacheë¥¼ contextì— ì¶”ê°€
        input_context["enable_cache"] = enable_cache

        # choices ì²˜ë¦¬
        if choices:
            if len(choices) < 2:
                raise ValueError("choices must contain at least 2 items")

            # choices_optionalì´ Trueë©´ None ì˜µì…˜ ì¶”ê°€
            internal_choices = choices.copy()
            if choices_optional:
                internal_choices.append("None of the above")

            # choices ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            input_context["choices"] = internal_choices
            input_context["choices_formatted"] = "\n".join([f"{i+1}. {c}" for i, c in enumerate(internal_choices)])
            input_context["choices_optional"] = choices_optional
            input_context["original_choices"] = choices

        human_prompt = self.get_human_prompt(input, input_context)
        human_message = Message(role="user", content=human_prompt, files=files)

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
        if stream:

            async def async_stream_handler() -> AsyncGenerator[Reply, None]:
                try:
                    text_list = []
                    async for ask in self._make_ask_stream_async(
                        input_context=input_context,
                        human_message=human_message,
                        messages=current_messages,
                        model=current_model,
                    ):
                        text_list.append(ask.text)
                        yield ask

                    # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ choices ì²˜ë¦¬
                    if choices and text_list:
                        full_text = "".join(text_list)
                        choice, index, confidence = self._process_choice_response(
                            full_text, input_context["original_choices"], choices_optional
                        )
                        # ë§ˆì§€ë§‰ì— choice ì •ë³´ë¥¼ í¬í•¨í•œ Reply ì „ì†¡
                        yield Reply(text="", choice=choice, choice_index=index, confidence=confidence)

                    if use_history:
                        ai_text = "".join(text_list)
                        self._update_history(human_message=human_message, ai_message=ai_text)
                except Exception as e:
                    if raise_errors:
                        raise e
                    yield Reply(text=f"Error: {str(e)}")

            def sync_stream_handler() -> Generator[Reply, None, None]:
                try:
                    text_list = []
                    for ask in self._make_ask_stream(
                        input_context=input_context,
                        human_message=human_message,
                        messages=current_messages,
                        model=current_model,
                    ):
                        text_list.append(ask.text)
                        yield ask

                    # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ choices ì²˜ë¦¬
                    if choices and text_list:
                        full_text = "".join(text_list)
                        choice, index, confidence = self._process_choice_response(
                            full_text, input_context["original_choices"], choices_optional
                        )
                        # ë§ˆì§€ë§‰ì— choice ì •ë³´ë¥¼ í¬í•¨í•œ Reply ì „ì†¡
                        yield Reply(text="", choice=choice, choice_index=index, confidence=confidence)

                    if use_history:
                        ai_text = "".join(text_list)
                        self._update_history(human_message=human_message, ai_message=ai_text)
                except Exception as e:
                    if raise_errors:
                        raise e
                    yield Reply(text=f"Error: {str(e)}")

            return async_stream_handler() if is_async else sync_stream_handler()

        # ì¼ë°˜ ì‘ë‹µ ì²˜ë¦¬
        else:

            async def async_handler() -> Reply:
                try:
                    ask = await self._make_ask_async(
                        input_context=input_context,
                        human_message=human_message,
                        messages=current_messages,
                        model=current_model,
                    )
                except Exception as e:
                    if raise_errors:
                        raise e
                    return Reply(text=f"Error: {str(e)}")
                else:
                    # choicesê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
                    if choices:
                        choice, index, confidence = self._process_choice_response(
                            ask.text, input_context["original_choices"], choices_optional
                        )
                        ask.choice = choice
                        ask.choice_index = index
                        ask.confidence = confidence

                    if use_history:
                        self._update_history(human_message=human_message, ai_message=ask.text)
                    return ask

            def sync_handler() -> Reply:
                try:
                    ask = self._make_ask(
                        input_context=input_context,
                        human_message=human_message,
                        messages=current_messages,
                        model=current_model,
                    )
                except Exception as e:
                    if raise_errors:
                        raise e
                    return Reply(text=f"Error: {str(e)}")
                else:
                    # choicesê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
                    if choices:
                        choice, index, confidence = self._process_choice_response(
                            ask.text, input_context["original_choices"], choices_optional
                        )
                        ask.choice = choice
                        ask.choice_index = index
                        ask.confidence = confidence

                    if use_history:
                        self._update_history(human_message=human_message, ai_message=ask.text)
                    return ask

            return async_handler() if is_async else sync_handler()

    def invoke(
        self,
        input: Union[str, dict[str, str]],
        files: Optional[list[Union[str, Path, File]]] = None,
        stream: bool = False,
        raise_errors: bool = False,
    ) -> Reply:
        """langchain í˜¸í™˜ ë©”ì„œë“œ: ë™ê¸°ì ìœ¼ë¡œ LLMì— ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.ask(input=input, files=files, stream=stream, raise_errors=raise_errors)

    def stream(
        self,
        input: Union[str, dict[str, str]],
        files: Optional[list[Union[str, Path, File]]] = None,
        raise_errors: bool = False,
    ) -> Generator[Reply, None, None]:
        """langchain í˜¸í™˜ ë©”ì„œë“œ: ë™ê¸°ì ìœ¼ë¡œ LLMì— ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
        return self.ask(input=input, files=files, stream=True, raise_errors=raise_errors)

    def ask(
        self,
        input: Union[str, dict[str, Any]],
        files: Optional[list[Union[str, Path, File]]] = None,
        model: Optional[LLMChatModelType] = None,
        context: Optional[dict[str, Any]] = None,
        *,
        choices: Optional[list[str]] = None,
        choices_optional: bool = False,
        stream: bool = False,
        use_history: bool = True,
        raise_errors: bool = False,
        enable_cache: bool = False,
        tools: Optional[list] = None,
        tool_choice: str = "auto",
        max_tool_calls: int = 5,
    ) -> Union[Reply, Generator[Reply, None, None]]:
        # ê¸°ë³¸ ë„êµ¬ì™€ ask ë„êµ¬ë¥¼ í•©ì¹¨
        merged_tools = self._merge_tools(tools)

        # ë„êµ¬ê°€ ìˆìœ¼ë©´ ë„êµ¬ì™€ í•¨ê»˜ ì²˜ë¦¬
        if merged_tools:
            return self._ask_with_tools(
                input=input,
                files=files,
                model=model,
                context=context,
                tools=merged_tools,
                tool_choice=tool_choice,
                max_tool_calls=max_tool_calls,
                choices=choices,
                choices_optional=choices_optional,
                stream=stream,
                use_history=use_history,
                raise_errors=raise_errors,
                enable_cache=enable_cache,
                is_async=False,
            )
        else:
            return self._ask_impl(
                input=input,
                files=files,
                model=model,
                context=context,
                choices=choices,
                choices_optional=choices_optional,
                is_async=False,
                stream=stream,
                use_history=use_history,
                raise_errors=raise_errors,
                enable_cache=enable_cache,
            )

    async def ask_async(
        self,
        input: Union[str, dict[str, Any]],
        files: Optional[list[Union[str, Path, File]]] = None,
        model: Optional[LLMChatModelType] = None,
        context: Optional[dict[str, Any]] = None,
        *,
        choices: Optional[list[str]] = None,
        choices_optional: bool = False,
        stream: bool = False,
        raise_errors: bool = False,
        use_history: bool = True,
        enable_cache: bool = False,
        tools: Optional[list] = None,
        tool_choice: str = "auto",
        max_tool_calls: int = 5,
    ) -> Union[Reply, AsyncGenerator[Reply, None]]:
        # ê¸°ë³¸ ë„êµ¬ì™€ ask ë„êµ¬ë¥¼ í•©ì¹¨
        merged_tools = self._merge_tools(tools)

        # ë„êµ¬ê°€ ìˆìœ¼ë©´ ë„êµ¬ì™€ í•¨ê»˜ ì²˜ë¦¬
        if merged_tools:
            return_value = self._ask_with_tools(
                input=input,
                files=files,
                model=model,
                context=context,
                tools=merged_tools,
                tool_choice=tool_choice,
                max_tool_calls=max_tool_calls,
                choices=choices,
                choices_optional=choices_optional,
                stream=stream,
                use_history=use_history,
                raise_errors=raise_errors,
                enable_cache=enable_cache,
                is_async=True,
            )
        else:
            return_value = self._ask_impl(
                input=input,
                files=files,
                model=model,
                context=context,
                choices=choices,
                choices_optional=choices_optional,
                is_async=True,
                stream=stream,
                use_history=use_history,
                raise_errors=raise_errors,
                enable_cache=enable_cache,
            )

        if stream:
            return return_value
        return await return_value

    #
    # Function Calling & Tool Support
    #

    def _merge_tools(self, ask_tools: Optional[list]) -> list:
        """ê¸°ë³¸ ë„êµ¬ì™€ ask ì‹œ ì œê³µëœ ë„êµ¬ë¥¼ í•©ì¹©ë‹ˆë‹¤.

        Args:
            ask_tools: ask í˜¸ì¶œ ì‹œ ì œê³µëœ ë„êµ¬ë“¤

        Returns:
            í•©ì³ì§„ ë„êµ¬ ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µì‹œ ask_toolsê°€ ìš°ì„ )
        """
        # tools ëª¨ë“ˆì„ ë™ì  import (ìˆœí™˜ import ë°©ì§€)
        from .tools import ToolAdapter

        # 1. ê¸°ë³¸ toolsë¡œ ì‹œì‘ (nameì„ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬)
        merged = {tool.name: tool for tool in self.default_tools}

        # 2. ask tools ì¶”ê°€ (ì¤‘ë³µì‹œ ë®ì–´ì”€)
        if ask_tools:
            adapted_ask_tools = ToolAdapter.adapt_tools(ask_tools)
            for tool in adapted_ask_tools:
                merged[tool.name] = tool

        return list(merged.values())

    def _ask_with_tools(
        self,
        input: Union[str, dict[str, Any]],
        files: Optional[list[Union[str, Path, File]]] = None,
        model: Optional[LLMChatModelType] = None,
        context: Optional[dict[str, Any]] = None,
        tools: Optional[list] = None,
        tool_choice: str = "auto",
        max_tool_calls: int = 5,
        choices: Optional[list[str]] = None,
        choices_optional: bool = False,
        stream: bool = False,
        use_history: bool = True,
        raise_errors: bool = False,
        enable_cache: bool = False,
        is_async: bool = False,
    ):
        """ë„êµ¬ì™€ í•¨ê»˜ LLM í˜¸ì¶œì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            tools: ì´ë¯¸ Tool ê°ì²´ë¡œ ë³€í™˜ëœ ë„êµ¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        # tools ëª¨ë“ˆì„ ë™ì  import (ìˆœí™˜ import ë°©ì§€)
        from .tools import ToolExecutor

        # toolsëŠ” ì´ë¯¸ _merge_toolsì—ì„œ Tool ê°ì²´ë¡œ ë³€í™˜ë¨
        adapted_tools = tools

        # ë„êµ¬ ì‹¤í–‰ê¸° ì¤€ë¹„
        executor = ToolExecutor(adapted_tools)

        # Providerë³„ ë„êµ¬ ìŠ¤í‚¤ë§ˆ ë³€í™˜ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
        provider_tools = self._convert_tools_for_provider(adapted_tools)

        if is_async:
            return self._ask_with_tools_async(
                input,
                files,
                model,
                context,
                adapted_tools,
                provider_tools,
                executor,
                tool_choice,
                max_tool_calls,
                choices,
                choices_optional,
                stream,
                use_history,
                raise_errors,
                enable_cache,
            )
        else:
            return self._ask_with_tools_sync(
                input,
                files,
                model,
                context,
                adapted_tools,
                provider_tools,
                executor,
                tool_choice,
                max_tool_calls,
                choices,
                choices_optional,
                stream,
                use_history,
                raise_errors,
                enable_cache,
            )

    def _ask_with_tools_sync(
        self,
        input,
        files,
        model,
        context,
        adapted_tools,
        provider_tools,
        executor,
        tool_choice,
        max_tool_calls,
        choices,
        choices_optional,
        stream,
        use_history,
        raise_errors,
        enable_cache,
    ):
        """ë™ê¸° ë²„ì „ì˜ ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
        # Trace ì‹œì‘
        if llm_settings.trace_function_calls:
            print("ğŸ” [TRACE] Function Calling ì‹œì‘")
            print(f"   ì…ë ¥: {input}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {[tool.name for tool in adapted_tools]}")
            print(f"   ìµœëŒ€ í˜¸ì¶œ íšŸìˆ˜: {max_tool_calls}")

        # ì´ˆê¸° ë©”ì‹œì§€ ì¤€ë¹„
        current_messages = [*self.history] if use_history else []
        human_prompt = self.get_human_prompt(input, context or {})

        # ë„êµ¬ í˜¸ì¶œ ë°˜ë³µ
        for call_count in range(max_tool_calls):
            try:
                if llm_settings.trace_function_calls:
                    print(f"\nğŸ“ [TRACE] LLM í˜¸ì¶œ #{call_count + 1}")

                # LLM í˜¸ì¶œ (ë„êµ¬ í¬í•¨)
                response = self._make_ask_with_tools_sync(
                    human_prompt if call_count == 0 else None,
                    current_messages,
                    provider_tools,
                    tool_choice,
                    model,
                    files if call_count == 0 else None,
                    enable_cache,
                )

                # ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ
                tool_calls = self._extract_tool_calls_from_response(response)

                if llm_settings.trace_function_calls:
                    if tool_calls:
                        print(f"   LLMì´ ìš”ì²­í•œ ë„êµ¬ í˜¸ì¶œ: {len(tool_calls)}ê°œ")
                        for i, call in enumerate(tool_calls):
                            print(f"     {i+1}. {call['name']}({call['arguments']})")
                    else:
                        print(f"   ë„êµ¬ í˜¸ì¶œ ì—†ìŒ, ìµœì¢… ì‘ë‹µ: {response.text[:100]}...")

                # ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì™„ë£Œ
                if not tool_calls:
                    if llm_settings.trace_function_calls:
                        print(f"âœ… [TRACE] Function Calling ì™„ë£Œ (ì´ {call_count + 1}íšŒ í˜¸ì¶œ)")
                    if use_history and call_count == 0:
                        human_message = Message(role="user", content=human_prompt, files=files)
                        self._update_history(human_message, response.text)
                    return response

                # ë„êµ¬ ì‹¤í–‰
                if llm_settings.trace_function_calls:
                    print("\nğŸ› ï¸  [TRACE] ë„êµ¬ ì‹¤í–‰ ì¤‘...")

                for tool_call in tool_calls:
                    try:
                        if llm_settings.trace_function_calls:
                            # ì¸ìë¥¼ ë” ì½ê¸° ì‰½ê²Œ í¬ë§·íŒ…
                            args_str = ", ".join([f"{k}={v}" for k, v in tool_call["arguments"].items()])
                            print(f"   ì‹¤í–‰: {tool_call['name']}({args_str})")

                        result = executor.execute_tool(tool_call["name"], tool_call["arguments"])

                        if llm_settings.trace_function_calls:
                            print(f"   ê²°ê³¼: {result}")

                        # ë„êµ¬ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                        current_messages.append(Message(role="assistant", content=f"[Tool Call: {tool_call['name']}]"))
                        current_messages.append(Message(role="user", content=f"[Tool Result: {result}]"))
                    except Exception as e:
                        if llm_settings.trace_function_calls:
                            print(f"   âŒ ì˜¤ë¥˜: {str(e)}")
                        if raise_errors:
                            raise e
                        error_msg = f"Tool execution error: {str(e)}"
                        current_messages.append(Message(role="user", content=f"[Tool Error: {error_msg}]"))

                # ì²« ë²ˆì§¸ í˜¸ì¶œì´ë©´ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                if use_history and call_count == 0:
                    human_message = Message(role="user", content=human_prompt, files=files)
                    current_messages.insert(0, human_message)

            except Exception as e:
                if raise_errors:
                    raise e
                return Reply(text=f"Error in tool processing: {str(e)}")

        # ìµœëŒ€ í˜¸ì¶œ íšŸìˆ˜ì— ë„ë‹¬í•œ ê²½ìš° ìµœì¢… ì‘ë‹µ
        try:
            # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ human_messageë¡œ ì‚¬ìš©
            if current_messages:
                final_human_message = current_messages[-1]
                final_messages = current_messages[:-1]
            else:
                final_human_message = Message(role="user", content="", files=files)
                final_messages = []

            final_response = self._make_ask(
                input_context={"enable_cache": enable_cache},
                human_message=final_human_message,
                messages=final_messages,
                model=model,
            )
            return final_response
        except Exception as e:
            if raise_errors:
                raise e
            return Reply(text=f"Final response error: {str(e)}")

    async def _ask_with_tools_async(
        self,
        input,
        files,
        model,
        context,
        adapted_tools,
        provider_tools,
        executor,
        tool_choice,
        max_tool_calls,
        choices,
        choices_optional,
        stream,
        use_history,
        raise_errors,
        enable_cache,
    ):
        """ë¹„ë™ê¸° ë²„ì „ì˜ ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
        # ì´ˆê¸° ë©”ì‹œì§€ ì¤€ë¹„
        current_messages = [*self.history] if use_history else []
        human_prompt = self.get_human_prompt(input, context or {})

        # ë„êµ¬ í˜¸ì¶œ ë°˜ë³µ
        for call_count in range(max_tool_calls):
            try:
                # LLM í˜¸ì¶œ (ë„êµ¬ í¬í•¨)
                response = await self._make_ask_with_tools_async(
                    human_prompt if call_count == 0 else None,
                    current_messages,
                    provider_tools,
                    tool_choice,
                    model,
                    files if call_count == 0 else None,
                    enable_cache,
                )

                # ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ
                tool_calls = self._extract_tool_calls_from_response(response)

                # ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì™„ë£Œ
                if not tool_calls:
                    if use_history and call_count == 0:
                        human_message = Message(role="user", content=human_prompt, files=files)
                        self._update_history(human_message, response.text)
                    return response

                # ë„êµ¬ ì‹¤í–‰
                for tool_call in tool_calls:
                    try:
                        result = await executor.execute_tool_async(tool_call["name"], tool_call["arguments"])
                        # ë„êµ¬ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                        current_messages.append(Message(role="assistant", content=f"[Tool Call: {tool_call['name']}]"))
                        current_messages.append(Message(role="user", content=f"[Tool Result: {result}]"))
                    except Exception as e:
                        if raise_errors:
                            raise e
                        error_msg = f"Tool execution error: {str(e)}"
                        current_messages.append(Message(role="user", content=f"[Tool Error: {error_msg}]"))

                # ì²« ë²ˆì§¸ í˜¸ì¶œì´ë©´ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                if use_history and call_count == 0:
                    human_message = Message(role="user", content=human_prompt, files=files)
                    current_messages.insert(0, human_message)

            except Exception as e:
                if raise_errors:
                    raise e
                return Reply(text=f"Error in tool processing: {str(e)}")

        # ìµœëŒ€ í˜¸ì¶œ íšŸìˆ˜ì— ë„ë‹¬í•œ ê²½ìš° ìµœì¢… ì‘ë‹µ
        try:
            # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ human_messageë¡œ ì‚¬ìš©
            if current_messages:
                final_human_message = current_messages[-1]
                final_messages = current_messages[:-1]
            else:
                final_human_message = Message(role="user", content="", files=files)
                final_messages = []

            final_response = await self._make_ask_async(
                input_context={"enable_cache": enable_cache},
                human_message=final_human_message,
                messages=final_messages,
                model=model,
            )
            return final_response
        except Exception as e:
            if raise_errors:
                raise e
            return Reply(text=f"Final response error: {str(e)}")

    def _convert_tools_for_provider(self, tools):
        """Providerë³„ ë„êµ¬ ìŠ¤í‚¤ë§ˆ ë³€í™˜ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        # ê¸°ë³¸ì ìœ¼ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (Function Calling ë¯¸ì§€ì›)
        return []

    def _extract_tool_calls_from_response(self, response):
        """ì‘ë‹µì—ì„œ ë„êµ¬ í˜¸ì¶œ ì •ë³´ ì¶”ì¶œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        # ê¸°ë³¸ì ìœ¼ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []

    def _make_ask_with_tools_sync(self, human_prompt, messages, tools, tool_choice, model, files, enable_cache):
        """ë„êµ¬ì™€ í•¨ê»˜ ë™ê¸° LLM í˜¸ì¶œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        # ê¸°ë³¸ì ìœ¼ë¡œ ì¼ë°˜ ask í˜¸ì¶œ
        return self._make_ask(
            input_context={},
            human_message=Message(role="user", content=human_prompt or ""),
            messages=messages,
            model=model,
        )

    async def _make_ask_with_tools_async(self, human_prompt, messages, tools, tool_choice, model, files, enable_cache):
        """ë„êµ¬ì™€ í•¨ê»˜ ë¹„ë™ê¸° LLM í˜¸ì¶œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        # ê¸°ë³¸ì ìœ¼ë¡œ ì¼ë°˜ ask í˜¸ì¶œ
        return await self._make_ask_async(
            input_context={},
            human_message=Message(role="user", content=human_prompt or ""),
            messages=messages,
            model=model,
        )

    #
    # embed
    #
    def get_embed_size(self, model: Optional[LLMEmbeddingModelType] = None) -> int:
        return self.EMBEDDING_DIMENSIONS[model or self.embedding_model]

    @property
    def embed_size(self) -> int:
        return self.get_embed_size()

    @abc.abstractmethod
    def embed(
        self,
        input: Union[str, list[str]],
        model: Optional[LLMEmbeddingModelType] = None,
    ) -> Union[Embed, EmbedList]:
        pass

    @abc.abstractmethod
    async def embed_async(
        self,
        input: Union[str, list[str]],
        model: Optional[LLMEmbeddingModelType] = None,
    ) -> Union[Embed, EmbedList]:
        pass

    #
    # describe images / tables
    #

    def describe_images(
        self,
        request: Union[DescribeImageRequest, list[DescribeImageRequest]],
        max_parallel_size: int = 4,
        raise_errors: bool = False,
        enable_cache: bool = False,
    ) -> Reply:
        return async_to_sync(self.describe_images_async)(request, max_parallel_size, raise_errors, enable_cache)

    async def describe_images_async(
        self,
        request: Union[DescribeImageRequest, list[DescribeImageRequest]],
        max_parallel_size: int = 4,
        raise_errors: bool = False,
        enable_cache: bool = False,
    ) -> Union[Reply, list[Reply]]:

        # ìµœëŒ€ 4ê°œì˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´ ì„¤ì •
        semaphore = asyncio.Semaphore(max_parallel_size)

        cls = self.__class__
        sig = signature(cls.__init__)
        is_supported_max_tokens = "max_tokens" in sig.parameters  # max_tokens is not supported in ollama

        if not isinstance(request, (list, tuple)):
            request_list = [request]
        else:
            request_list = request

        async def process_single_image(
            task_request: DescribeImageRequest,
            idx: int,
            total: int,
        ) -> Reply:
            logger.info("request describe_images [%d/%d] : %s", idx + 1, total, task_request.image_path)

            if is_supported_max_tokens:
                llm = cls(
                    model=self.model,
                    temperature=self.temperature if task_request.temperature is None else task_request.temperature,
                    max_tokens=self.max_tokens if task_request.max_tokens is None else task_request.max_tokens,
                    system_prompt=task_request.system_prompt,
                )
            else:
                llm = cls(
                    model=self.model,
                    temperature=self.temperature if task_request.temperature is None else task_request.temperature,
                    system_prompt=task_request.system_prompt,
                )
                if task_request.max_tokens is not None:
                    logger.debug("max_tokens is not supported for %s LLM", self.model)

            reply = await llm.ask_async(
                input=task_request.user_prompt,
                files=[task_request.image],
                context=task_request.prompt_context,
                raise_errors=raise_errors,
                enable_cache=enable_cache,
            )
            logger.debug("image description for %s : %s", task_request.image_path, repr(reply.text))
            return reply

        async def process_with_semaphore(request_item, idx, total):
            async with semaphore:
                return await process_single_image(request_item, idx, total)

        # ì„¸ë§ˆí¬ì–´ë¥¼ í†µí•´ ë³‘ë ¬ ì²˜ë¦¬ ì œí•œ
        tasks = [process_with_semaphore(request, idx, len(request_list)) for idx, request in enumerate(request_list)]
        reply_list = await asyncio.gather(*tasks)

        assert len(request_list) == len(reply_list)

        for _request in request_list:
            if hasattr(_request.image, "seek"):
                _request.image.seek(0)

        if isinstance(request, (list, tuple)):
            return reply_list

        return reply_list[0]


class SequentialChain:
    def __init__(self, *args):
        self.llms: list[BaseLLM] = list(args)

    def insert_first(self, llm) -> "SequentialChain":
        self.llms.insert(0, llm)
        return self

    def append(self, llm) -> "SequentialChain":
        self.llms.append(llm)
        return self

    def ask(self, inputs: dict[str, Any]) -> ChainReply:
        """ì²´ì¸ì˜ ê° LLMì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì´ì „ LLMì˜ ì¶œë ¥ì´ ë‹¤ìŒ LLMì˜ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤."""

        for llm in self.llms:
            if llm.prompt is None:
                raise ValueError(f"prompt is required for LLM: {llm}")

        known_values = inputs.copy()
        reply_list = []
        for llm in self.llms:
            reply = llm.ask(known_values)
            reply_list.append(reply)

            output_key = llm.get_output_key()
            known_values[output_key] = str(reply)

        return ChainReply(
            values=known_values,
            reply_list=reply_list,
        )
