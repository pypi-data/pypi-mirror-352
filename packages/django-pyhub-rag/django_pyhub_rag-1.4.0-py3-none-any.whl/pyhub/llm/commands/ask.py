import logging
import sys
from pathlib import Path
from typing import Optional

import typer
from rich.console import Console
from rich.prompt import Prompt
from rich.table import Table

from pyhub import init
from pyhub.llm import LLM
from pyhub.llm.types import LLMChatModelEnum

console = Console()


def ask(
    ctx: typer.Context,
    query: Optional[str] = typer.Argument(None, help="ì§ˆì˜ ë‚´ìš©"),
    model: LLMChatModelEnum = typer.Option(
        LLMChatModelEnum.GPT_4O_MINI,
        "--model",
        "-m",
        help="LLM Chat ëª¨ë¸. LLM ë²¤ë”ì— ë§ê²Œ ì§€ì •í•´ì£¼ì„¸ìš”.",
    ),
    context: str = typer.Option(None, help="LLMì— ì œê³µí•  ì»¨í…ìŠ¤íŠ¸"),
    system_prompt: str = typer.Option(None, help="LLMì— ì‚¬ìš©í•  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"),
    system_prompt_path: str = typer.Option(
        "system_prompt.txt",
        help="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ëœ íŒŒì¼ ê²½ë¡œ",
    ),
    temperature: float = typer.Option(0.2, help="LLM ì‘ë‹µì˜ ì˜¨ë„ ì„¤ì • (0.0-2.0, ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì‘ë‹µ)"),
    max_tokens: int = typer.Option(1000, help="ì‘ë‹µì˜ ìµœëŒ€ í† í° ìˆ˜"),
    is_multi: bool = typer.Option(
        False,
        "--multi",
        help="ë©€í‹° í„´ ëŒ€í™”",
    ),
    output_json: bool = typer.Option(
        False,
        "--json",
        help="JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ (êµ¬ì¡°í™”ëœ ì‘ë‹µ)",
    ),
    schema_path: Optional[Path] = typer.Option(
        None,
        "--schema",
        help="JSON Schema íŒŒì¼ ê²½ë¡œ (êµ¬ì¡°í™”ëœ ì‘ë‹µ í˜•ì‹ ì •ì˜). "
        "OpenAIì™€ UpstageëŠ” ì™„ì „í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì§€ì›í•˜ë©°, "
        "Anthropic, Google, OllamaëŠ” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•œ ì œí•œì  ì§€ì›ë§Œ ì œê³µí•©ë‹ˆë‹¤.",
    ),
    output_path: Optional[Path] = typer.Option(
        None,
        "--output",
        "-o",
        help="ì‘ë‹µì„ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ",
    ),
    template_name: Optional[str] = typer.Option(
        None,
        "--template",
        "-t",
        help="í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ë¦„ (toml íŒŒì¼ì—ì„œ ë¡œë“œ)",
    ),
    show_cost: bool = typer.Option(
        False,
        "--cost",
        help="ì˜ˆìƒ ë¹„ìš© í‘œì‹œ (ì£¼ì˜: Upstage ëª¨ë¸ì€ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œ ì§€ì› ì•ˆ ë¨)",
    ),
    show_stats: bool = typer.Option(
        False,
        "--stats",
        help="í† í° ì‚¬ìš©ëŸ‰ ë° ì‘ë‹µ ì‹œê°„ í†µê³„ í‘œì‹œ",
    ),
    is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸í•œ ì²˜ë¦¬ ì •ë³´ í‘œì‹œ"),
    no_stream: bool = typer.Option(
        False,
        "--no-stream",
        help="ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™” (ê¸°ë³¸: ìŠ¤íŠ¸ë¦¬ë° í™œì„±)",
    ),
    enable_cache: bool = typer.Option(
        False,
        "--enable-cache",
        help="API ì‘ë‹µ ìºì‹œë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤",
    ),
):
    """LLMì— ì§ˆì˜í•˜ê³  ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤.

    Examples:
        # ê¸°ë³¸ ì‚¬ìš© (ìŠ¤íŠ¸ë¦¬ë°)
        pyhub.llm ask "What is Python?"

        # ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (Upstageì—ì„œ ë¹„ìš© ì •ë³´ í•„ìš” ì‹œ)
        pyhub.llm ask "Hello" --no-stream --cost

        # íŒŒì¼ë¡œ ì €ì¥
        pyhub.llm ask "Explain AI" --output response.txt

        # í…œí”Œë¦¿ ì‚¬ìš©
        pyhub.llm ask "Fix this code" --template code_review

        # JSON í˜•ì‹ ì¶œë ¥
        pyhub.llm ask "List 3 colors" --json
    """

    if query is None:
        if sys.stdin.isatty():
            # stdinì´ í„°ë¯¸ë„ì¸ ê²½ìš° (íŒŒì´í”„ë¼ì¸ì´ ì•„ë‹Œ ê²½ìš°) - help ì¶œë ¥
            console.print(ctx.get_help())
            raise typer.Exit()
        else:
            # stdinì—ì„œ ì…ë ¥ì„ ë°›ëŠ” ê²½ìš°
            console.print("[red]ì˜¤ë¥˜: ì§ˆë¬¸ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.[/red]")
            console.print('[dim]ì‚¬ìš©ë²•: pyhub.llm ask "ì§ˆë¬¸"[/dim]')
            console.print('[dim]ë˜ëŠ”: echo "ì»¨í…ìŠ¤íŠ¸" | pyhub.llm ask "ì§ˆë¬¸"[/dim]')
            raise typer.Exit(1)

    # Use stdin as context if available and no context argument was provided
    if context is None and not sys.stdin.isatty():
        context = sys.stdin.read().strip()

    # Handle system prompt options
    if system_prompt_path:
        try:
            with open(system_prompt_path, "r") as f:
                system_prompt = f.read().strip()
        except IOError:
            pass

    if context:
        system_prompt = ((system_prompt or "") + "\n\n" + f"<context>{context}</context>").strip()

    # if system_prompt:
    #     console.print(f"# System prompt\n\n{system_prompt}\n\n----\n\n", style="blue")

    if is_verbose:
        log_level = logging.DEBUG
    else:
        log_level = logging.WARNING  # Only show warnings and errors when not verbose
    init(debug=is_verbose, log_level=log_level)

    # í…œí”Œë¦¿ ë¡œë“œ
    if template_name:
        try:
            import toml

            from pyhub.config import Config

            toml_path = Config.get_default_toml_path()
            if toml_path.exists():
                with toml_path.open("r", encoding="utf-8") as f:
                    config = toml.load(f)
                    templates = config.get("prompt_templates", {}).get(template_name, {})
                    if templates:
                        system_prompt = templates.get("system", system_prompt)
                        if "user" in templates and "{query}" in templates["user"]:
                            query = templates["user"].format(query=query)
                    else:
                        console.print(f"[yellow]ê²½ê³ : í…œí”Œë¦¿ '{template_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
        except Exception as e:
            console.print(f"[yellow]í…œí”Œë¦¿ ë¡œë“œ ì˜¤ë¥˜: {e}[/yellow]")

    # JSON Schema ë¡œë“œ
    choices = None
    if schema_path and schema_path.exists():
        try:
            import json

            with schema_path.open("r", encoding="utf-8") as f:
                choices = json.load(f)
        except Exception as e:
            console.print(f"[red]JSON Schema íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}[/red]")
            raise typer.Exit(1)

    if is_verbose:
        table = Table()
        table.add_column("ì„¤ì •", style="cyan")
        table.add_column("ê°’", style="green")
        table.add_row("model", model)
        table.add_row("context", context)
        table.add_row("system prompt", system_prompt)
        table.add_row("user prompt", query)
        table.add_row("temperature", str(temperature))
        table.add_row("max_tokens", str(max_tokens))
        table.add_row("ë©€í‹° í„´ ì—¬ë¶€", "O" if is_multi else "X")
        table.add_row("JSON ì¶œë ¥", "O" if output_json else "X")
        if schema_path:
            table.add_row("JSON Schema", str(schema_path))
        console.print(table)

    llm = LLM.create(
        model=model,
        system_prompt=system_prompt,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    if is_verbose:
        console.print(f"Using llm {llm.model}")

    import time

    start_time = time.time()

    if not is_multi:
        response_text = ""
        usage = None

        if is_verbose:
            console.print(f"[dim]ë””ë²„ê·¸: ëª¨ë¸={model.value}, show_cost={show_cost}, show_stats={show_stats}[/dim]")

        if output_json or choices:
            # êµ¬ì¡°í™”ëœ ì‘ë‹µ
            response = llm.ask(query, choices=choices, enable_cache=enable_cache)
            usage = response.usage if hasattr(response, "usage") else None
            if output_json:
                import json
                from dataclasses import asdict, is_dataclass

                # dataclassë¥¼ dictë¡œ ë³€í™˜
                if is_dataclass(response):
                    response_dict = asdict(response)
                    # Usageì˜ total propertyë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€
                    if "usage" in response_dict and response_dict["usage"]:
                        response_dict["usage"]["total"] = response.usage.total
                elif hasattr(response, "model_dump"):
                    response_dict = response.model_dump()
                else:
                    response_dict = response

                # ë¹„ìš© ì •ë³´ ì¶”ê°€ (ìš”ì²­ëœ ê²½ìš°)
                if show_cost and usage:
                    try:
                        from pyhub.llm.utils.pricing import calculate_cost

                        cost = calculate_cost(model.value, usage.input, usage.output)
                        response_dict["cost"] = {
                            "input_cost": cost["input_cost"],
                            "output_cost": cost["output_cost"],
                            "total_cost": cost["total_cost"],
                            "total_cost_krw": cost["total_cost"] * 1300,
                        }
                    except Exception as e:
                        if is_verbose:
                            console.print(f"[yellow]ë¹„ìš© ê³„ì‚° ì‹¤íŒ¨: {e}[/yellow]")

                response_text = json.dumps(response_dict, ensure_ascii=False, indent=2)
                console.print(response_text)
            else:
                response_text = str(response)
                console.print(response_text)
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ
            if no_stream:
                # ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                if is_verbose:
                    console.print("[dim]ë””ë²„ê·¸: ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‚¬ìš©[/dim]")
                response = llm.ask(query, stream=False, enable_cache=enable_cache)
                response_text = response.text
                usage = response.usage
                console.print(response_text)
            else:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (ê¸°ë³¸)
                if is_verbose:
                    console.print("[dim]ë””ë²„ê·¸: ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...[/dim]")
                for chunk in llm.ask(query, stream=True, enable_cache=enable_cache):
                    if chunk.text:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶œë ¥
                        console.print(chunk.text, end="")
                        response_text += chunk.text
                    if hasattr(chunk, "usage") and chunk.usage:
                        if is_verbose:
                            console.print(
                                f"\n[dim]ë””ë²„ê·¸: Usage ë°œê²¬ - input: {chunk.usage.input}, output: {chunk.usage.output}[/dim]"
                            )
                        usage = chunk.usage
                console.print()

        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        elapsed_time = time.time() - start_time

        # íŒŒì¼ë¡œ ì €ì¥
        if output_path:
            try:
                with output_path.open("w", encoding="utf-8") as f:
                    f.write(response_text)
                console.print(f"[green]ì‘ë‹µì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}[/green]")
            except Exception as e:
                console.print(f"[red]íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}[/red]")

        # í†µê³„ í‘œì‹œ
        if show_stats and usage:
            stats_table = Table(title="í†µê³„")
            stats_table.add_column("í•­ëª©", style="cyan")
            stats_table.add_column("ê°’", style="green")
            stats_table.add_row("ì…ë ¥ í† í°", str(usage.input))
            stats_table.add_row("ì¶œë ¥ í† í°", str(usage.output))
            stats_table.add_row("ì´ í† í°", str(usage.total))
            stats_table.add_row("ì‘ë‹µ ì‹œê°„", f"{elapsed_time:.2f}ì´ˆ")
            console.print(stats_table)

        # ë¹„ìš© í‘œì‹œ
        if show_cost:
            if is_verbose:
                console.print(f"[dim]ë””ë²„ê·¸: show_cost={show_cost}, usage={usage}[/dim]")
            if usage:
                try:
                    from pyhub.llm.utils.pricing import calculate_cost

                    cost = calculate_cost(model, usage.input, usage.output)
                    cost_table = Table(title="ì˜ˆìƒ ë¹„ìš©")
                    cost_table.add_column("í•­ëª©", style="cyan")
                    cost_table.add_column("ê°’", style="green")
                    cost_table.add_row("ì…ë ¥ ë¹„ìš©", f"$ {cost['input_cost']:.6f}")
                    cost_table.add_row("ì¶œë ¥ ë¹„ìš©", f"$ {cost['output_cost']:.6f}")
                    cost_table.add_row("ì´ ë¹„ìš©", f"$ {cost['total_cost']:.6f}")
                    cost_table.add_row("ì›í™” í™˜ì‚°", f"â‚© {cost['total_cost'] * 1300:.6f}")
                    console.print(cost_table)
                except Exception as e:
                    console.print(f"[red]ë¹„ìš© ê³„ì‚° ì˜¤ë¥˜: {e}[/red]")
                    if is_verbose:
                        import traceback

                        console.print(f"[yellow]{traceback.format_exc()}[/yellow]")
            else:
                # ëª¨ë¸ë³„ ìƒì„¸í•œ ì•ˆë‚´ ë©”ì‹œì§€
                if "upstage" in model.value.lower() or "solar" in model.value.lower():
                    console.print(
                        "[yellow]ê²½ê³ : Upstage ëª¨ë¸ì€ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œ í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.[/yellow]"
                    )
                    console.print(
                        "[dim]ğŸ’¡ íŒ: ë¹„ìš© ì •ë³´ë¥¼ ë³´ë ¤ë©´ --no-stream ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì„¸ìš”.[/dim]"
                    )
                else:
                    console.print(
                        "[yellow]ê²½ê³ : í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìºì‹œëœ ì‘ë‹µì´ê±°ë‚˜ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©ëŸ‰ì´ ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤)[/yellow]"
                    )

    else:
        console.print("Human:", query)

        while query:
            console.print("AI:", end=" ")
            if no_stream:
                response = llm.ask(query, stream=False, enable_cache=enable_cache)
                console.print(response.text)
            else:
                for chunk in llm.ask(query, stream=True, enable_cache=enable_cache):
                    console.print(chunk.text, end="")
                console.print()

            query = Prompt.ask("Human")
