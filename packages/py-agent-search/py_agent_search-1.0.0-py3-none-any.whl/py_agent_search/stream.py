from langchain.callbacks.base import BaseCallbackHandler
from .log import send_log

class StreamingCallbackHandler(BaseCallbackHandler):
    """
    A callback handler for streaming tokens from a language model.
    This handler accumulates tokens as they are generated and provides
    methods to handle the start, new token, and end events of the model's response.
    It also logs metadata about the streaming process.
    Attributes:
        metadata_logger (dict): A dictionary to hold metadata for logging.
    """
    
    metadata_logger = {
        "loki_metadata": {}
    }
    
    def __init__(self):
        """
        Initializes the callback handler for token streaming.
        This handler accumulates tokens generated by a language model
        and provides methods to handle start, new token, and end events.
        """
        # List to accumulate tokens as they arrive
        self.tokens: list[str] = []
        # Flag to indicate if we are in streaming mode
        self.streaming = True

    def on_llm_start(self, serialized, prompts, **kwargs):
        """
        Called before the model starts generating.
        `serialized` contains the serialized model parameters,
        `prompts` contains the input prompts, and `kwargs` any additional parameters.
        """
        send_log(message="[Callback] on_llm_start params", 
                 metadata={
                    "serialized": serialized, 
                    "prompts": prompts, 
                    "kwargs": kwargs,
                    "streaming": self.streaming
                 })
        # Reset any previously accumulated tokens
        self.tokens = []

    def on_llm_new_token(self, token: str, **kwargs):
        """
        Called every time a new token arrives (streaming mode).
        """
        send_log(message="[Callback] on_llm_new_token params", 
                   metadata={
                    "token": token,
                    "kwargs": kwargs,
                    "streaming": self.streaming
                })
            
        # Add the token to the list
        self.tokens.append(token)
        
    def on_llm_end(self, response, **kwargs):
        """
        Viene chiamato quando la generazione Ã¨ terminata: 
        `response` contiene il testo finale completo.
        """
        send_log(message="[Callback] on_llm_end params", 
                    metadata={
                    "response": response,
                    "kwargs": kwargs,
                    "streaming": self.streaming
                })