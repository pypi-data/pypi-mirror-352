# reflex-llm
A chat client that will rebound to running local AI in a docker container when the configured LLM endpoint is not reachable.
