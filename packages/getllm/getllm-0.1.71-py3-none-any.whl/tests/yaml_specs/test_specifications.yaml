---
# YAML-based test specifications for getLLM
# This is a declarative approach to defining test cases

test_suite:
  name: "getLLM Functional Tests"
  description: "Declarative test specifications for getLLM functionality"
  version: "1.0.0"
  created_at: "2025-05-29"

setup:
  environment:
    - PYTHONPATH: "${PROJECT_ROOT}"
    - MOCK_MODE: "--mock"
  prerequisites:
    - command: "which getllm"
      expected_exit_code: 0
      error_message: "getLLM is not installed"

test_cases:
  - id: "installation-check"
    name: "Check getLLM Installation"
    description: "Verify getLLM is installed and accessible"
    steps:
      - command: "getllm --version"
        expected_exit_code: 0
        expected_output_contains: "getLLM version"

  - id: "huggingface-model-search"
    name: "Hugging Face Model Search"
    description: "Test searching for Bielik models in Hugging Face"
    steps:
      - command: "echo -e 'search-hf\nbielik\nexit' | getllm ${MOCK_MODE} -i"
        expected_exit_code: 0
        expected_output_contains: ["bielik", "Bielik"]
        error_message: "Failed to find Bielik models in Hugging Face search"

  - id: "ollama-search-fallback"
    name: "Ollama Search with Hugging Face Fallback"
    description: "Test Ollama search with fallback to Hugging Face for 'bie' query"
    steps:
      - command: "echo -e 'search-ollama\nbie\nexit' | getllm ${MOCK_MODE} -i"
        expected_exit_code: 0
        expected_output_contains: ["Searching Hugging Face GGUF models", "Found Hugging Face GGUF models"]
        error_message: "Hugging Face fallback not triggered for Ollama search"

  - id: "model-installation"
    name: "Model Installation Workflow"
    description: "Test the model installation workflow"
    steps:
      - command: "echo -e 'list\nexit' | getllm ${MOCK_MODE} -i"
        expected_exit_code: 0
        expected_output_contains: "available models"
        error_message: "Available models not listed"

  - id: "code-generation"
    name: "Direct Code Generation"
    description: "Test direct code generation capability"
    steps:
      - command: "getllm ${MOCK_MODE} 'Write a hello world program in Python'"
        expected_exit_code: 0
        expected_output_contains: "print"
        error_message: "Code generation failed to produce Python code"

  - id: "ollama-installation"
    name: "Ollama Installation Prompt"
    description: "Test that getLLM offers to install Ollama when not found"
    steps:
      - command: "echo -e 'n\nexit' | OLLAMA_PATH=/nonexistent/path getllm ${MOCK_MODE} -i"
        expected_exit_code: 0
        expected_output_contains: "install Ollama"
        error_message: "Ollama installation prompt not shown"

validations:
  model_cache_files:
    - path: "~/.getllm/models/huggingface_models.json"
      should_exist: true
      content_validation:
        json_path: "$[?(@.name.includes('bielik') || @.id.includes('bielik'))]"
        should_not_be_empty: true
        error_message: "No Bielik models found in Hugging Face cache"

    - path: "~/.getllm/models/ollama_models.json"
      should_exist: true

    - path: "~/.getllm/models/models_metadata.json"
      should_exist: true
      content_validation:
        json_path: "$.total_models"
        should_be_greater_than: 0
        error_message: "No models found in metadata"
