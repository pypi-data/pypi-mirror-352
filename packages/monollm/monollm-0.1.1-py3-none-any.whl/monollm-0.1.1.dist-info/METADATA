Metadata-Version: 2.4
Name: monollm
Version: 0.1.1
Summary: A unified framework for accessing multiple LLM providers
Project-URL: Homepage, https://github.com/cyborgoat/monollm
Project-URL: Documentation, https://cyborgoat.github.io/monollm/
Project-URL: Repository, https://github.com/cyborgoat/monollm.git
Project-URL: Issues, https://github.com/cyborgoat/monollm/issues
Project-URL: Changelog, https://github.com/cyborgoat/monollm/blob/main/CHANGELOG.md
Author-email: cyborgoat <cyborgoat@outlook.com>
Maintainer-email: cyborgoat <cyborgoat@outlook.com>
License-Expression: MIT
License-File: LICENSE
Keywords: ai,anthropic,api,async,claude,deepseek,gpt,llm,openai,qwen,streaming,unified
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Internet :: WWW/HTTP :: Dynamic Content
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Typing :: Typed
Requires-Python: >=3.12
Requires-Dist: anthropic>=0.25.0
Requires-Dist: httpx>=0.25.0
Requires-Dist: openai>=1.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: tenacity>=8.0.0
Requires-Dist: typer>=0.9.0
Provides-Extra: dev
Requires-Dist: mypy>=1.5.0; extra == 'dev'
Requires-Dist: pre-commit>=3.0.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.0.0; extra == 'dev'
Requires-Dist: pytest>=7.0.0; extra == 'dev'
Requires-Dist: ruff>=0.1.0; extra == 'dev'
Requires-Dist: types-requests>=2.31.0; extra == 'dev'
Provides-Extra: docs
Requires-Dist: linkify-it-py>=2.0.0; extra == 'docs'
Requires-Dist: mdit-py-plugins>=0.4.0; extra == 'docs'
Requires-Dist: myst-parser>=2.0.0; extra == 'docs'
Requires-Dist: sphinx-autodoc-typehints>=1.24.0; extra == 'docs'
Requires-Dist: sphinx-rtd-theme>=1.3.0; extra == 'docs'
Requires-Dist: sphinx>=7.0.0; extra == 'docs'
Provides-Extra: test
Requires-Dist: httpx>=0.25.0; extra == 'test'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'test'
Requires-Dist: pytest-cov>=4.0.0; extra == 'test'
Requires-Dist: pytest>=7.0.0; extra == 'test'
Description-Content-Type: text/markdown

# MonoLLM

[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Documentation](https://img.shields.io/badge/docs-github--pages-blue)](https://cyborgoat.github.io/unified-llm/)
[![GitHub Issues](https://img.shields.io/github/issues/cyborgoat/unified-llm)](https://github.com/cyborgoat/unified-llm/issues)

> **A powerful framework that provides a unified interface for multiple LLM providers, allowing developers to seamlessly switch between different AI models while maintaining consistent API interactions.**

## ğŸš€ Key Features

- **ğŸ”„ Unified Interface**: Access multiple LLM providers through a single, consistent API
- **ğŸŒ Proxy Support**: Configure HTTP/SOCKS5 proxies for all LLM calls
- **ğŸ“º Streaming**: Real-time streaming responses for better user experience
- **ğŸ§  Reasoning Models**: Special support for reasoning models with thinking steps
- **ğŸŒ¡ï¸ Temperature Control**: Fine-tune creativity and randomness when supported
- **ğŸ”¢ Token Management**: Control costs with maximum output token limits
- **ğŸ”§ MCP Integration**: Model Context Protocol support when available
- **ğŸ¯ OpenAI Protocol**: Prefer OpenAI-compatible APIs for consistency
- **âš™ï¸ JSON Configuration**: Easy configuration management through JSON files

## ğŸ“‹ Supported Providers

| Provider | Status | Streaming | Reasoning | MCP | OpenAI Protocol |
|----------|--------|-----------|-----------|-----|-----------------|
| **OpenAI** | âœ… Ready | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes |
| **Anthropic** | âœ… Ready | âœ… Yes | âŒ No | âœ… Yes | âŒ No |
| **Google Gemini** | ğŸš§ Planned | âœ… Yes | âŒ No | âŒ No | âŒ No |
| **Qwen (DashScope)** | âœ… Ready | âœ… Yes | âœ… Yes | âŒ No | âœ… Yes |
| **DeepSeek** | âœ… Ready | âœ… Yes | âœ… Yes | âŒ No | âœ… Yes |
| **Volcengine** | ğŸš§ Planned | âœ… Yes | âŒ No | âŒ No | âœ… Yes |

## ğŸ› ï¸ Installation

### Prerequisites

- **Python 3.13+** (required)
- **uv** (recommended) or **pip**

### Quick Install

```bash
# Clone the repository
git clone https://github.com/cyborgoat/unified-llm.git
cd unified-llm

# Install with uv (recommended)
uv sync
uv pip install -e .

# Or install with pip
pip install -e .
```

### Verify Installation

```bash
# Check CLI is working
unified-llm --help

# List available providers
unified-llm list-providers
```

## âš¡ Quick Start

### 1. Set up API Keys

```bash
# Set API keys for the providers you want to use
export DASHSCOPE_API_KEY="your-dashscope-api-key"  # For Qwen
export ANTHROPIC_API_KEY="your-anthropic-api-key"  # For Claude
export OPENAI_API_KEY="your-openai-api-key"        # For GPT models
```

### 2. Basic Python Usage

```python
import asyncio
from monollm import UnifiedLLMClient, RequestConfig


async def main():
    async with UnifiedLLMClient() as client:
        config = RequestConfig(
            model="qwq-32b",  # Qwen's reasoning model
            temperature=0.7,
            max_tokens=1000,
        )

        response = await client.generate(
            "Explain quantum computing in simple terms.",
            config
        )

        print(response.content)
        if response.usage:
            print(f"Tokens used: {response.usage.total_tokens}")


asyncio.run(main())
```

### 3. CLI Usage

```bash
# Generate text with streaming
unified-llm generate "What is artificial intelligence?" --model qwen-plus --stream

# Use reasoning model with thinking steps
unified-llm generate "Solve: 2x + 5 = 13" --model qwq-32b --thinking

# List available models
unified-llm list-models --provider qwen
```

## ğŸ“– Documentation

- **ğŸ“š [Full Documentation](https://cyborgoat.github.io/unified-llm/)** - Comprehensive guides and API reference
- **ğŸš€ [Quick Start Guide](https://cyborgoat.github.io/unified-llm/quickstart.html)** - Get up and running in minutes
- **âš™ï¸ [Configuration Guide](https://cyborgoat.github.io/unified-llm/configuration.html)** - Advanced configuration options
- **ğŸ’» [CLI Documentation](https://cyborgoat.github.io/unified-llm/cli.html)** - Command-line interface guide
- **ğŸ”§ [Examples](https://cyborgoat.github.io/unified-llm/examples.html)** - Practical usage examples

## ğŸ¯ Use Cases

### Content Generation
```python
config = RequestConfig(model="qwen-plus", temperature=0.8, max_tokens=1000)
response = await client.generate("Write a blog post about renewable energy", config)
```

### Code Assistance
```python
config = RequestConfig(model="qwq-32b", temperature=0.2)
response = await client.generate("Explain this Python function: def fibonacci(n):", config)
```

### Reasoning & Analysis
```python
config = RequestConfig(model="qwq-32b", show_thinking=True)
response = await client.generate("Analyze this data and find trends", config)
```

### Creative Writing
```python
config = RequestConfig(model="qwen-plus", temperature=1.0, max_tokens=2000)
response = await client.generate("Write a science fiction short story", config)
```

## ğŸ”§ Advanced Features

### Streaming Responses
```python
async for chunk in await client.generate_stream(prompt, config):
    if chunk.content:
        print(chunk.content, end="", flush=True)
```

### Multi-turn Conversations
```python
messages = [
    Message(role="system", content="You are a helpful assistant."),
    Message(role="user", content="Hello!"),
]
response = await client.generate(messages, config)
```

### Error Handling

```python
from monollm.core.exceptions import UnifiedLLMError, ProviderError

try:
    response = await client.generate(prompt, config)
except ProviderError as e:
    print(f"Provider error: {e}")
except UnifiedLLMError as e:
    print(f"UnifiedLLM error: {e}")
```

## ğŸŒ Proxy Support

Configure HTTP/SOCKS5 proxies:

```bash
export PROXY_ENABLED=true
export PROXY_TYPE=http
export PROXY_HOST=127.0.0.1
export PROXY_PORT=7890
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](https://cyborgoat.github.io/unified-llm/development/contributing.html) for details.

### Development Setup

```bash
# Clone and install in development mode
git clone https://github.com/cyborgoat/unified-llm.git
cd unified-llm
uv sync --dev

# Install pre-commit hooks
pre-commit install

# Run tests
pytest

# Build documentation
cd docs && make html
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Links

- **GitHub**: https://github.com/cyborgoat/unified-llm
- **Documentation**: https://cyborgoat.github.io/unified-llm/
- **Issues**: https://github.com/cyborgoat/unified-llm/issues
- **Discussions**: https://github.com/cyborgoat/unified-llm/discussions

## ğŸ™ Acknowledgments

- Thanks to all the LLM providers for their amazing APIs
- Inspired by the need for a unified interface across multiple AI providers
- Built with modern Python async/await patterns for optimal performance

## ğŸ‘¨â€ğŸ’» Author

Created and maintained by **[cyborgoat](https://github.com/cyborgoat)**

---

**Made with â¤ï¸ by cyborgoat** 