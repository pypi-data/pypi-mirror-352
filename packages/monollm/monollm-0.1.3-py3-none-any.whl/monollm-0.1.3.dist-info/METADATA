Metadata-Version: 2.4
Name: monollm
Version: 0.1.3
Summary: A unified framework for accessing multiple LLM providers with comprehensive CLI and machine interface
Project-URL: Homepage, https://github.com/cyborgoat/monollm
Project-URL: Documentation, https://cyborgoat.github.io/monollm/
Project-URL: Repository, https://github.com/cyborgoat/monollm.git
Project-URL: Issues, https://github.com/cyborgoat/monollm/issues
Project-URL: Changelog, https://github.com/cyborgoat/monollm/blob/main/CHANGELOG.md
Author-email: cyborgoat <cyborgoat@outlook.com>
Maintainer-email: cyborgoat <cyborgoat@outlook.com>
License-Expression: MIT
License-File: LICENSE
Keywords: ai,anthropic,api,async,claude,deepseek,gpt,llm,openai,qwen,streaming,unified
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Internet :: WWW/HTTP :: Dynamic Content
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Typing :: Typed
Requires-Python: >=3.12
Requires-Dist: anthropic>=0.25.0
Requires-Dist: httpx>=0.25.0
Requires-Dist: openai>=1.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: tenacity>=8.0.0
Requires-Dist: typer>=0.9.0
Provides-Extra: dev
Requires-Dist: mypy>=1.5.0; extra == 'dev'
Requires-Dist: pre-commit>=3.0.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.0.0; extra == 'dev'
Requires-Dist: pytest>=7.0.0; extra == 'dev'
Requires-Dist: ruff>=0.1.0; extra == 'dev'
Requires-Dist: types-requests>=2.31.0; extra == 'dev'
Provides-Extra: docs
Requires-Dist: linkify-it-py>=2.0.0; extra == 'docs'
Requires-Dist: mdit-py-plugins>=0.4.0; extra == 'docs'
Requires-Dist: myst-parser>=2.0.0; extra == 'docs'
Requires-Dist: sphinx-autobuild>=2021.3.14; extra == 'docs'
Requires-Dist: sphinx-autodoc-typehints>=1.24.0; extra == 'docs'
Requires-Dist: sphinx-rtd-theme>=1.3.0; extra == 'docs'
Requires-Dist: sphinx>=7.0.0; extra == 'docs'
Provides-Extra: test
Requires-Dist: httpx>=0.25.0; extra == 'test'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'test'
Requires-Dist: pytest-cov>=4.0.0; extra == 'test'
Requires-Dist: pytest>=7.0.0; extra == 'test'
Description-Content-Type: text/markdown

# MonoLLM

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Documentation](https://img.shields.io/badge/docs-github--pages-blue)](https://cyborgoat.github.io/MonoLLM/)
[![GitHub Issues](https://img.shields.io/github/issues/cyborgoat/MonoLLM)](https://github.com/cyborgoat/MonoLLM/issues)

> **A powerful framework that provides a unified interface for multiple LLM providers, allowing developers to seamlessly switch between different AI models while maintaining consistent API interactions.**

## ğŸš€ Key Features

- **ğŸ”„ Unified Interface**: Access multiple LLM providers through a single, consistent API
- **ğŸŒ Proxy Support**: Configure HTTP/SOCKS5 proxies for all LLM calls
- **ğŸ“º Streaming**: Real-time streaming responses for better user experience
- **ğŸ§  Reasoning Models**: Special support for reasoning models with thinking steps
- **ğŸŒ¡ï¸ Temperature Control**: Fine-tune creativity and randomness when supported
- **ğŸ”¢ Token Management**: Control costs with maximum output token limits
- **ğŸ”§ MCP Integration**: Model Context Protocol support when available
- **ğŸ¯ OpenAI Protocol**: Prefer OpenAI-compatible APIs for consistency
- **âš™ï¸ JSON Configuration**: Easy configuration management through JSON files

## ğŸ“‹ Supported Providers

| Provider | Status | Streaming | Reasoning | MCP | OpenAI Protocol |
|----------|--------|-----------|-----------|-----|-----------------|
| **OpenAI** | âœ… Ready | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes |
| **Anthropic** | âœ… Ready | âœ… Yes | âŒ No | âœ… Yes | âŒ No |
| **Google Gemini** | ğŸš§ Planned | âœ… Yes | âŒ No | âŒ No | âŒ No |
| **Qwen (DashScope)** | âœ… Ready | âœ… Yes | âœ… Yes | âŒ No | âœ… Yes |
| **DeepSeek** | âœ… Ready | âœ… Yes | âœ… Yes | âŒ No | âœ… Yes |
| **Volcengine** | ğŸš§ Planned | âœ… Yes | âŒ No | âŒ No | âœ… Yes |

## ğŸ› ï¸ Installation

### Prerequisites

- **Python 3.13+** (required)
- **uv** (recommended) or **pip**

### Quick Install

```bash
# Clone the repository
git clone https://github.com/cyborgoat/MonoLLM.git
cd MonoLLM

# Install with uv (recommended)
uv sync
uv pip install -e .

# Or install with pip
pip install -e .
```

### Verify Installation

```bash
# Check CLI is working
monollm --help

# List available providers
monollm list-providers
```

## âš¡ Quick Start

### 1. Set up API Keys

```bash
# Set API keys for the providers you want to use
export DASHSCOPE_API_KEY="your-dashscope-api-key"  # For Qwen
export ANTHROPIC_API_KEY="your-anthropic-api-key"  # For Claude
export OPENAI_API_KEY="your-openai-api-key"        # For GPT models
```

### 2. Basic Python Usage

```python
import asyncio
from monollm import UnifiedLLMClient, RequestConfig

async def main():
    async with UnifiedLLMClient() as client:
        config = RequestConfig(
            model="qwq-32b",  # Qwen's reasoning model
            temperature=0.7,
            max_tokens=1000,
        )
        
        response = await client.generate(
            "Explain quantum computing in simple terms.",
            config
        )
        
        print(response.content)
        if response.usage:
            print(f"Tokens used: {response.usage.total_tokens}")

asyncio.run(main())
```

### 3. CLI Usage

```bash
# Generate text with streaming
monollm generate "What is artificial intelligence?" --model qwen-plus --stream

# Use reasoning model with thinking steps
monollm generate "Solve: 2x + 5 = 13" --model qwq-32b --thinking

# List available models
monollm list-models --provider qwen
```

## ğŸ“– Documentation

- **ğŸ“š [Full Documentation](https://cyborgoat.github.io/MonoLLM/)** - Comprehensive guides and API reference
- **ğŸš€ [Quick Start Guide](https://cyborgoat.github.io/MonoLLM/quickstart.html)** - Get up and running in minutes
- **âš™ï¸ [Configuration Guide](https://cyborgoat.github.io/MonoLLM/configuration.html)** - Advanced configuration options
- **ğŸ’» [CLI Documentation](https://cyborgoat.github.io/MonoLLM/cli.html)** - Command-line interface guide
- **ğŸ¤– [Machine Interface](src/monollm/cli/README-MACHINE.md)** - JSON API for programmatic usage and Tauri sidecars
- **ğŸ”§ [Examples](https://cyborgoat.github.io/MonoLLM/examples.html)** - Practical usage examples

## ğŸ¤– Machine Interface & Tauri Integration

MonoLLM provides a powerful machine-friendly JSON API perfect for integration with external applications, automation scripts, and Tauri sidecars:

```bash
# All commands support --machine flag for JSON output
monollm list-providers --machine
monollm generate "Hello world" --model gpt-4o --machine
monollm generate-stream "Tell a story" --model qwq-32b --thinking
```

### Tauri Sidecar Example

```rust
// Rust code for Tauri app
use std::process::Command;

let output = Command::new("monollm")
    .args(&["generate", "What is AI?", "--model", "gpt-4o", "--machine"])
    .output()
    .expect("Failed to execute command");

let response: serde_json::Value = serde_json::from_slice(&output.stdout)?;
println!("AI Response: {}", response["content"]);
```

### Key Machine Interface Features

- **ğŸ”„ Structured JSON**: All responses in consistent JSON format
- **ğŸ“¡ Streaming Support**: Real-time JSON chunks for streaming responses  
- **âš™ï¸ Configuration API**: Programmatic model defaults and proxy management
- **ğŸ›¡ï¸ Error Handling**: Consistent error format with detailed context
- **ğŸ”§ Validation**: Parameter validation before API calls
- **ğŸ“Š Usage Tracking**: Token usage and performance metrics

**ğŸ“– [Complete Machine Interface Documentation](src/monollm/cli/README-MACHINE.md)**

## ğŸ¯ Use Cases

### Content Generation

```python
config = RequestConfig(model="qwen-plus", temperature=0.8, max_tokens=1000)
response = await client.generate("Write a blog post about renewable energy", config)
```

### Code Assistance

```python
config = RequestConfig(model="qwq-32b", temperature=0.2)
response = await client.generate("Explain this Python function: def fibonacci(n):", config)
```

### Reasoning & Analysis

```python
config = RequestConfig(model="qwq-32b", show_thinking=True)
response = await client.generate("Analyze this data and find trends", config)
```

### Thinking Mode for Reasoning Models

MonoLLM supports reasoning models that can show their internal thought process:

```python
# Enable thinking mode to see step-by-step reasoning
config = RequestConfig(
    model="qwq-32b",  # QwQ reasoning model
    show_thinking=True,  # Show internal reasoning
    temperature=0.7
)

response = await client.generate(
    "Solve this step by step: If a train travels 120 km in 2 hours, then 180 km in 3 hours, what is its average speed?",
    config
)

# Access the thinking process
if response.thinking:
    print("ğŸ’­ Thinking Process:")
    print(response.thinking)
    print("\n" + "="*50)

print("ğŸ¯ Final Answer:")
print(response.content)
```

**Supported Reasoning Models:**
- **QwQ-32B** (`qwq-32b`) - Stream-only reasoning model
- **QwQ-Plus** (`qwq-plus`) - Stream-only reasoning model  
- **Qwen3 Series** (`qwen3-32b`, `qwen3-8b`, etc.) - Support both modes
- **OpenAI o1** (`o1`, `o1-mini`) - Advanced reasoning models
- **DeepSeek R1** (`deepseek-reasoner`) - Reasoning model

### Creative Writing

```python
config = RequestConfig(model="qwen-plus", temperature=1.0, max_tokens=2000)
response = await client.generate("Write a science fiction short story", config)
```

## ğŸ”§ Advanced Features

### Streaming Responses

```python
async for chunk in await client.generate_stream(prompt, config):
    if chunk.content:
        print(chunk.content, end="", flush=True)
```

### Multi-turn Conversations

```python
messages = [
    Message(role="system", content="You are a helpful assistant."),
    Message(role="user", content="Hello!"),
]
response = await client.generate(messages, config)
```

### Error Handling

```python
from monollm.core.exceptions import MonoLLMError, ProviderError

try:
    response = await client.generate(prompt, config)
except ProviderError as e:
    print(f"Provider error: {e}")
except MonoLLMError as e:
    print(f"MonoLLM error: {e}")
```

## ğŸŒ Proxy Support

Configure HTTP/SOCKS5 proxies:

```bash
export PROXY_ENABLED=true
export PROXY_TYPE=http
export PROXY_HOST=127.0.0.1
export PROXY_PORT=7890
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](https://cyborgoat.github.io/MonoLLM/development/contributing.html) for details.

### Development Setup

```bash
# Clone and install in development mode
git clone https://github.com/cyborgoat/MonoLLM.git
cd MonoLLM
uv sync --dev

# Install pre-commit hooks
pre-commit install

# Run tests
pytest

# Build documentation
cd docs && make html
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Links

- **GitHub**: <https://github.com/cyborgoat/MonoLLM>
- **Documentation**: <https://cyborgoat.github.io/MonoLLM/>
- **Issues**: <https://github.com/cyborgoat/MonoLLM/issues>
- **Discussions**: <https://github.com/cyborgoat/MonoLLM/discussions>

## ğŸ™ Acknowledgments

- Thanks to all the LLM providers for their amazing APIs
- Inspired by the need for a unified interface across multiple AI providers
- Built with modern Python async/await patterns for optimal performance

## ğŸ‘¨â€ğŸ’» Author

Created and maintained by **[cyborgoat](https://github.com/cyborgoat)**

---

**Made with â¤ï¸ by cyborgoat**
