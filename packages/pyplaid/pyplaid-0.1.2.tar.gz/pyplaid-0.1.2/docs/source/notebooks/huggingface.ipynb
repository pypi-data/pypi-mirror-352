{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugginface support\n",
    "\n",
    "This Jupyter Notebook demonstrates various operations involving the huggingface bridge:\n",
    "\n",
    "1. Converting a plaid dataset to hugginface \n",
    "2. Generating a hugginface dataset with a generator\n",
    "3. Converting a hugginface dataset to plaid\n",
    "4. Saving and Loading hugginface datasets\n",
    "5. Handling plaid samples from huggingface datasets without converting the complete dataset to plaid\n",
    "\n",
    "\n",
    "**Each section is documented and explained.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "from Muscat.Bridges.CGNSBridge import MeshToCGNS\n",
    "from Muscat.Containers import MeshCreationTools as MCT\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from plaid.bridges import huggingface_bridge\n",
    "from plaid.containers.sample import Sample\n",
    "from plaid.containers.dataset import Dataset\n",
    "from plaid.problem_definition import ProblemDefinition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Sample util\n",
    "def show_sample(sample: Sample):\n",
    "    print(f\"sample = {sample}\")\n",
    "    sample.show_tree()\n",
    "    print(f\"{sample.get_scalar_names() = }\")\n",
    "    print(f\"{sample.get_field_names() = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize plaid dataset and problem_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "points = np.array([\n",
    "    [0.0, 0.0],\n",
    "    [1.0, 0.0],\n",
    "    [1.0, 1.0],\n",
    "    [0.0, 1.0],\n",
    "    [0.5, 1.5],\n",
    "])\n",
    "\n",
    "triangles = np.array([\n",
    "    [0, 1, 2],\n",
    "    [0, 2, 3],\n",
    "    [2, 4, 3],\n",
    "])\n",
    "\n",
    "\n",
    "dataset = Dataset()\n",
    "\n",
    "print(\"Creating meshes dataset...\")\n",
    "for _ in range(3):\n",
    "\n",
    "    mesh = MCT.CreateMeshOfTriangles(points, triangles)\n",
    "\n",
    "    sample = Sample()\n",
    "\n",
    "    sample.add_tree(MeshToCGNS(mesh))\n",
    "    sample.add_scalar(\"scalar\", np.random.randn())\n",
    "    sample.add_field(\"node_field\", np.random.rand(1, len(points)), location = 'Vertex')\n",
    "    sample.add_field(\"cell_field\", np.random.rand(1, len(points)), location = 'CellCenter')\n",
    "\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "infos = {\n",
    "        \"legal\": {\n",
    "            \"owner\": \"Bob\",\n",
    "            \"license\": \"my_license\"},\n",
    "        \"data_production\": {\n",
    "            \"type\": \"simulation\",\n",
    "            \"physics\": \"3D example\"}\n",
    "    }\n",
    "\n",
    "dataset.set_infos(infos)\n",
    "\n",
    "print(f\" {dataset = }\")\n",
    "\n",
    "problem = ProblemDefinition()\n",
    "problem.add_output_scalars_names([\"scalar\"])\n",
    "problem.add_output_fields_names([\"node_field\", \"cell_field\"])\n",
    "problem.add_input_meshes_names(['/Base/Zone'])\n",
    "\n",
    "problem.set_task('regression')\n",
    "problem.set_split({'train':[0,1], 'test':[2]})\n",
    "\n",
    "print(f\" {problem = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Convert plaid dataset to huggingface\n",
    "\n",
    "The description field of huggingface dataset is automatically configured to include data from the plaid dataset info and problem_definition to prevent loss of information and equivalence of format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = huggingface_bridge.plaid_dataset_to_huggingface(dataset, problem)\n",
    "print()\n",
    "print(f\"{hf_dataset = }\")\n",
    "print(f\"{hf_dataset.description = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Generate a hugginface dataset with a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    for id in range(len(dataset)):\n",
    "        yield {\n",
    "            \"sample\" : pickle.dumps(dataset[id]),\n",
    "        }\n",
    "\n",
    "hf_dataset_gen = huggingface_bridge.plaid_generator_to_huggingface(generator, infos, problem)\n",
    "print()\n",
    "print(f\"{hf_dataset_gen = }\")\n",
    "print(f\"{hf_dataset_gen.description = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Convert a hugginface dataset to plaid\n",
    "\n",
    "Plaid dataset infos and problem_defitinion are recovered from the huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2, problem_2 = huggingface_bridge.huggingface_dataset_to_plaid(hf_dataset)\n",
    "print()\n",
    "print(f\"{dataset_2 = }\")\n",
    "print(f\"{dataset_2.get_infos() = }\")\n",
    "print(f\"{problem_2 = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Save and Load hugginface datasets\n",
    "\n",
    "### From and to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "hf_dataset.save_to_disk(\"/tmp/path/to/dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "from datasets import load_from_disk\n",
    "loaded_hf_dataset = load_from_disk(\"/tmp/path/to/dir\")\n",
    "\n",
    "print()\n",
    "print(f\"{loaded_hf_dataset = }\")\n",
    "print(f\"{loaded_hf_dataset.description = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From and to the huggingface hub\n",
    "\n",
    "You need an huggingface account, with a configured access token, and to install huggingface_hub[cli].\n",
    "Pushing and loading a huggingface dataset without loss of information requires the configuration of a DatasetCard.\n",
    "\n",
    "Find below example of instruction (not executed by this notebook).\n",
    "\n",
    "### Push to the hub\n",
    "\n",
    "First login the huggingface cli:\n",
    "```bash\n",
    "    \n",
    "    huggingface-cli login\n",
    "\n",
    "```\n",
    "and enter you access token.\n",
    "\n",
    "Then, the following python instruction enable pushing a dataset to the hub:\n",
    "```python\n",
    "    \n",
    "    hf_dataset.push_to_hub(\"chanel/dataset\")\n",
    "\n",
    "    from datasets import load_dataset_builder\n",
    "\n",
    "    datasetInfo = load_dataset_builder(\"chanel/dataset\").__getstate__()['info']\n",
    "\n",
    "    from huggingface_hub import DatasetCard\n",
    "\n",
    "    card_text = create_string_for_huggingface_dataset_card(\n",
    "        description = description,\n",
    "        download_size_bytes = datasetInfo.download_size,\n",
    "        dataset_size_bytes = datasetInfo.dataset_size,\n",
    "        ...)\n",
    "    dataset_card = DatasetCard(card_text)\n",
    "    dataset_card.push_to_hub(\"chanel/dataset\")\n",
    "\n",
    "```\n",
    "\n",
    "The second upload of the dataset_card is required to ensure that load_dataset from the hub will populate\n",
    "the hf-dataset.description field, and be compatible for conversion to plaid. Wihtout a dataset_card, the description field is lost.\n",
    "\n",
    "\n",
    "### Load from hub\n",
    "\n",
    "```python\n",
    "\n",
    "    dataset = load_dataset(\"chanel/dataset\", split=\"all_samples\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Handle plaid samples from huggingface datasets without converting the complete dataset to plaid\n",
    "\n",
    "To fully exploit optimzed data handling of the huggingface datasets library, it is possible to extract information from the huggingface dataset without converting to plaid. The ``description`` atttribute includes the plaid dataset _infos attribute and plaid problem_definition attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{loaded_hf_dataset.description = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first sample of the first split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_names = list(loaded_hf_dataset.description[\"split\"].keys())\n",
    "id = loaded_hf_dataset.description[\"split\"][split_names[0]]\n",
    "hf_sample = loaded_hf_dataset[id[0]][\"sample\"]\n",
    "\n",
    "print(f\"{hf_sample = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that ``hf_sample`` is a binary object efficiently handled by huggingface datasets. It can be converted into a plaid sample using a specific constructor relying on a pydantic validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plaid_sample = Sample.model_validate(pickle.loads(hf_sample))\n",
    "\n",
    "show_sample(plaid_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
