{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNDpiAzBI0xH"
      },
      "source": [
        "# LLMSearch Google Colab Demo\n",
        "\n",
        "This notebook was tested to run the following 13B model - https://huggingface.co/TheBloke/airoboros-l2-13B-gpt4-1.4.1-GGUF\n",
        "\n",
        "In case of memory errors, tweak the config to offload some layers to CPU, or try a smaller model.\n",
        "\n",
        "## Instuctions\n",
        "\n",
        "* Upload or generate some documents (check supported format in README.md) in `sample_docs` folder.\n",
        "    * Or use a sample pdf book provided - Pro Git - https://git-scm.com/book/en/v2\n",
        "* Run the notebook.\n",
        "* Optional - tweak configuration file to point to a different model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43kUuvFzDxFr"
      },
      "source": [
        "### Prepare configuration and download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2s1HFkwBHNe",
        "outputId": "f6214a2a-2764-4a63-86da-21f06b82cb88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "# Make folder structure\n",
        "mkdir -p llm/embeddings llm/cache llm/models llm/config sample_docs\n",
        "\n",
        "# Download sample book\n",
        "wget -P sample_docs https://github.com/progit/progit2/releases/download/2.1.413/progit.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# Generate sample configuration\n",
        "\n",
        "cat << EOF > llm/config/config.yaml\n",
        "\n",
        "cache_folder: /content/llm/cache\n",
        "\n",
        "embeddings:\n",
        "  embeddings_path: /content/llm/embeddings\n",
        "  embedding_model: # Optional embedding model specification, default is e5-large-v2. Swap to a smaller model if out of CUDA memory\n",
        "    type: sentence_transformer # other supported types - \"huggingface\" and \"instruct\"\n",
        "    model_name: \"intfloat/e5-large-v2\"\n",
        "  chunk_sizes:\n",
        "    - 1024\n",
        "  document_settings:\n",
        "  - doc_path: sample_docs/\n",
        "    scan_extensions:\n",
        "      - md\n",
        "      - pdf\n",
        "    additional_parser_settings:\n",
        "      md:\n",
        "        skip_first: True\n",
        "        merge_sections: True\n",
        "        remove_images: True\n",
        "\n",
        "semantic_search:\n",
        "  search_type: similarity # mmr\n",
        "  max_char_size: 3096\n",
        "\n",
        "  reranker:\n",
        "    enabled: True\n",
        "    model: \"marco\" # for `BAAI/bge-reranker-base` or \"marco\" for cross-encoder/ms-marco-MiniLM-L-6-v2\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "\n",
        "cat << EOF > llm/config/model.yaml\n",
        "# Geberate sample model configuration for llama-cpp\n",
        "llm:\n",
        " type: llamacpp\n",
        " params:\n",
        "   model_path: /content/llm/models/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf\n",
        "   prompt_template: |\n",
        "         ### Instruction:\n",
        "         Use the following pieces of context to provide detailed answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.\n",
        "\n",
        "         ### Context:\n",
        "         ---------------\n",
        "         {context}\n",
        "         ---------------\n",
        "\n",
        "         ### Question: {question}\n",
        "         ### Response:\n",
        "   model_init_params:\n",
        "     n_ctx: 1024\n",
        "     n_batch: 512\n",
        "     n_gpu_layers: 43\n",
        "\n",
        "   model_kwargs:\n",
        "     max_tokens: 512\n",
        "     top_p: 0.1\n",
        "     top_k: 40\n",
        "     temperature: 0.2\n",
        "\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmnHPhTNEWub",
        "outputId": "5d2e5ab2-daf9-4c05-8d48-5bc68636ba0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-08-02 11:46:03--  https://huggingface.co/TheBloke/WizardLM-13B-Uncensored-GGML/resolve/main/wizardLM-13B-Uncensored.ggmlv3.q6_K.bin\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.49.2, 65.8.49.24, 65.8.49.53, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.49.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/b3/d0/b3d063894847aa127a5a297b2c1356e0f2fc6e0a03344cce92fd1b05423fbdcf/5df27eede7f7f6ca4cedc8d22dcbbbcf496e324a9711a40228041a15e27b313e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27wizardLM-13B-Uncensored.ggmlv3.q6_K.bin%3B+filename%3D%22wizardLM-13B-Uncensored.ggmlv3.q6_K.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1691235964&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MTIzNTk2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMy9kMC9iM2QwNjM4OTQ4NDdhYTEyN2E1YTI5N2IyYzEzNTZlMGYyZmM2ZTBhMDMzNDRjY2U5MmZkMWIwNTQyM2ZiZGNmLzVkZjI3ZWVkZTdmN2Y2Y2E0Y2VkYzhkMjJkY2JiYmNmNDk2ZTMyNGE5NzExYTQwMjI4MDQxYTE1ZTI3YjMxM2U%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=0asq7KV9gn5bC%7Eiq23DxHJr5fav31rpQIec09bGJUwNikf9sA098lfsAemklOnte%7EZ3Ef1HiGOa5MWmPz%7EA9Zwsh9YAfJFzjy1yFSf5UjMWvMXWYdRxjoPJNARu4rB-zJzSvRrJ6-olvEQBGbQ4j6Ky0IDcyiaR%7EqrTJaRsz2UmpOQgUHCMq4A9J8ZqlFidEV5Seltfi5vVzAe4SXlC4zAxtGM3F-xNYUYc3FTZpDoMRnfAzDEzDug4x6pXd4n-68wPl5ZGOBokl9RUw0WTJuyB0X9xiscz9z7pIWnDIQAI14TeEkf-YFfPfVQ6IQ8kCOlRtPpFWqidmAeDjGD-Uzw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-08-02 11:46:04--  https://cdn-lfs.huggingface.co/repos/b3/d0/b3d063894847aa127a5a297b2c1356e0f2fc6e0a03344cce92fd1b05423fbdcf/5df27eede7f7f6ca4cedc8d22dcbbbcf496e324a9711a40228041a15e27b313e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27wizardLM-13B-Uncensored.ggmlv3.q6_K.bin%3B+filename%3D%22wizardLM-13B-Uncensored.ggmlv3.q6_K.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1691235964&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MTIzNTk2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMy9kMC9iM2QwNjM4OTQ4NDdhYTEyN2E1YTI5N2IyYzEzNTZlMGYyZmM2ZTBhMDMzNDRjY2U5MmZkMWIwNTQyM2ZiZGNmLzVkZjI3ZWVkZTdmN2Y2Y2E0Y2VkYzhkMjJkY2JiYmNmNDk2ZTMyNGE5NzExYTQwMjI4MDQxYTE1ZTI3YjMxM2U%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=0asq7KV9gn5bC%7Eiq23DxHJr5fav31rpQIec09bGJUwNikf9sA098lfsAemklOnte%7EZ3Ef1HiGOa5MWmPz%7EA9Zwsh9YAfJFzjy1yFSf5UjMWvMXWYdRxjoPJNARu4rB-zJzSvRrJ6-olvEQBGbQ4j6Ky0IDcyiaR%7EqrTJaRsz2UmpOQgUHCMq4A9J8ZqlFidEV5Seltfi5vVzAe4SXlC4zAxtGM3F-xNYUYc3FTZpDoMRnfAzDEzDug4x6pXd4n-68wPl5ZGOBokl9RUw0WTJuyB0X9xiscz9z7pIWnDIQAI14TeEkf-YFfPfVQ6IQ8kCOlRtPpFWqidmAeDjGD-Uzw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 54.230.18.21, 54.230.18.111, 54.230.18.124, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|54.230.18.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10678859104 (9.9G) [application/octet-stream]\n",
            "Saving to: ‘wizardLM-13B-Uncensored.ggmlv3.q6_K.bin’\n",
            "\n",
            "wizardLM-13B-Uncens 100%[===================>]   9.95G   171MB/s    in 68s     \n",
            "\n",
            "2023-08-02 11:47:11 (151 MB/s) - ‘wizardLM-13B-Uncensored.ggmlv3.q6_K.bin’ saved [10678859104/10678859104]\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "# Download the model\n",
        "# Sample model - https://huggingface.co/TheBloke/WizardLM-13B-Uncensored-GGML/tree/main\n",
        "# Optionally download a smaller model to test...\n",
        "\n",
        "\n",
        "cd llm/models\n",
        "wget https://huggingface.co/TheBloke/airoboros-l2-13B-gpt4-1.4.1-GGUF/resolve/main/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0UkSdWrAByA"
      },
      "source": [
        "### Enable building with CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fESaAtai3BFH",
        "outputId": "a45e8a0a-43da-40a3-ea2c-6b093fd7d449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\n",
            "env: FORCE_CMAKE=1\n"
          ]
        }
      ],
      "source": [
        "%env CMAKE_ARGS=-DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc -DLLAMA_CUBLAS=ON\n",
        "%env FORCE_CMAKE=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zs6ZdRgO7Kxz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Install torch and torchvision\n",
        "# \n",
        "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "daEnZje03nHX",
        "outputId": "7ec01beb-d7f1-4b20-cf7d-62dfaf4ecf6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/snexus/llm-search\n",
            "  Cloning https://github.com/snexus/llm-search to /tmp/pip-req-build-6snvel4k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/snexus/llm-search /tmp/pip-req-build-6snvel4k\n",
            "  Resolved https://github.com/snexus/llm-search to commit 7207a1674f83aae1b1a7aadba5bb3cc10555ba2f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (from llmsearch==0.1.dev74+g7207a16.d20230802) (0.1.77)\n",
            "Collecting chromadb==0.3.26 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached chromadb-0.3.26-py3-none-any.whl (123 kB)\n",
            "Collecting langchain==0.0.219 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached langchain-0.0.219-py3-none-any.whl (1.2 MB)\n",
            "Collecting llama-index==0.6.9 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached llama_index-0.6.9-py3-none-any.whl (403 kB)\n",
            "Collecting tokenizers==0.13.3 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Collecting transformers==4.29.2 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "Collecting sentence-transformers==2.2.2 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
            "Collecting pypdf2==3.0.1 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "Collecting sentencepiece==0.1.99 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Requirement already satisfied: setuptools==67.7.2 in /usr/local/lib/python3.10/dist-packages (from llmsearch==0.1.dev74+g7207a16.d20230802) (67.7.2)\n",
            "Collecting loguru (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "Collecting python-dotenv (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting accelerate==0.19.0 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "Collecting protobuf==3.20.2 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from llmsearch==0.1.dev74+g7207a16.d20230802) (2.3.0)\n",
            "Collecting einops (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from llmsearch==0.1.dev74+g7207a16.d20230802) (8.1.6)\n",
            "Collecting bitsandbytes (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n",
            "Collecting auto-gptq (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached auto_gptq-0.3.2.tar.gz (63 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Discarding \u001b[4;34mhttps://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz (from llmsearch==0.1.dev74+g7207a16.d20230802) has inconsistent version: expected '0.3.2', but metadata has '0.3.2+cu118'\u001b[0m\n",
            "  Using cached auto_gptq-0.3.1.tar.gz (63 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Discarding \u001b[4;34mhttps://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz (from llmsearch==0.1.dev74+g7207a16.d20230802) has inconsistent version: expected '0.3.1', but metadata has '0.3.1+cu1180'\u001b[0m\n",
            "  Using cached auto_gptq-0.3.0-cp310-cp310-linux_x86_64.whl\n",
            "Collecting InstructorEmbedding==1.0.1 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting unstructured==0.7.8 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached unstructured-0.7.8-py3-none-any.whl (1.4 MB)\n",
            "Collecting pymupdf==1.22.5 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached PyMuPDF-1.22.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "Collecting streamlit==1.24.1 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached streamlit-1.24.1-py2.py3-none-any.whl (8.9 MB)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from llmsearch==0.1.dev74+g7207a16.d20230802) (1.16.0)\n",
            "Requirement already satisfied: sniffio==1.3.0 in /usr/local/lib/python3.10/dist-packages (from llmsearch==0.1.dev74+g7207a16.d20230802) (1.3.0)\n",
            "Collecting sqlalchemy==1.4.48 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "Collecting starlette==0.26.1 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "Requirement already satisfied: sympy==1.11.1 in /usr/local/lib/python3.10/dist-packages (from llmsearch==0.1.dev74+g7207a16.d20230802) (1.11.1)\n",
            "Requirement already satisfied: tenacity==8.2.2 in /usr/local/lib/python3.10/dist-packages (from llmsearch==0.1.dev74+g7207a16.d20230802) (8.2.2)\n",
            "Collecting threadpoolctl==3.1.0 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting tiktoken==0.3.3 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "Collecting torch==2.0.0 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "Collecting torchvision==0.15.1 (from llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "Requirement already satisfied: tqdm==4.65.0 in /usr/local/lib/python3.10/dist-packages (from llmsearch==0.1.dev74+g7207a16.d20230802) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->llmsearch==0.1.dev74+g7207a16.d20230802) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->llmsearch==0.1.dev74+g7207a16.d20230802) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->llmsearch==0.1.dev74+g7207a16.d20230802) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->llmsearch==0.1.dev74+g7207a16.d20230802) (6.0.1)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802) (1.5.3)\n",
            "Collecting requests>=2.28 (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802) (1.10.12)\n",
            "Collecting hnswlib>=0.7 (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached clickhouse_connect-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (966 kB)\n",
            "Requirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802) (0.8.1)\n",
            "Collecting fastapi>=0.85.1 (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached fastapi-0.100.1-py3-none-any.whl (65 kB)\n",
            "Collecting uvicorn[standard]>=0.18.3 (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802) (4.7.1)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.17 (from langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "Collecting openai>=0.26.4 (from llama-index==0.6.9->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "Collecting requests>=2.28 (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached requests-2.29.0-py3-none-any.whl (62 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.6.9->llmsearch==0.1.dev74+g7207a16.d20230802) (2023.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->llmsearch==0.1.dev74+g7207a16.d20230802) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->llmsearch==0.1.dev74+g7207a16.d20230802) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->llmsearch==0.1.dev74+g7207a16.d20230802) (3.8.1)\n",
            "Collecting huggingface-hub>=0.4.0 (from sentence-transformers==2.2.2->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy==1.4.48->llmsearch==0.1.dev74+g7207a16.d20230802) (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette==0.26.1->llmsearch==0.1.dev74+g7207a16.d20230802) (3.7.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (5.3.1)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/lib/python3/dist-packages (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (4.6.4)\n",
            "Requirement already satisfied: pillow<10,>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (9.4.0)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (9.0.0)\n",
            "Collecting pympler<2,>=0.9 (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (2.8.2)\n",
            "Requirement already satisfied: rich<14,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (13.4.2)\n",
            "Requirement already satisfied: toml<2 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (0.10.2)\n",
            "Collecting tzlocal<5,>=1.1 (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting validators<1,>=0.2 (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached validators-0.20.0-py3-none-any.whl\n",
            "Collecting gitpython!=3.1.19,<4,>=3 (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "Collecting pydeck<1,>=0.1.dev5 (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (6.3.1)\n",
            "Collecting watchdog (from streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy==1.11.1->llmsearch==0.1.dev74+g7207a16.d20230802) (1.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->llmsearch==0.1.dev74+g7207a16.d20230802) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802) (3.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802) (2.0.0)\n",
            "Collecting argilla (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached argilla-1.13.3-py3-none-any.whl (2.7 MB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802) (4.0.0)\n",
            "Collecting filetype (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802) (4.9.3)\n",
            "Collecting msg-parser (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802) (3.0.10)\n",
            "Collecting pdf2image (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Collecting pdfminer.six (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "Collecting pypandoc (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached pypandoc-1.11-py3-none-any.whl (20 kB)\n",
            "Collecting python-docx (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached python_docx-0.8.11-py3-none-any.whl\n",
            "Collecting python-pptx (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached python_pptx-0.6.21-py3-none-any.whl\n",
            "Collecting python-magic (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802) (3.4.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802) (0.9.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802) (2.0.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802) (0.41.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802) (16.0.6)\n",
            "Collecting datasets (from auto-gptq->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached datasets-2.14.2-py3-none-any.whl (518 kB)\n",
            "Collecting rouge (from auto-gptq->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting safetensors (from auto-gptq->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting peft (from auto-gptq->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached peft-0.4.0-py3-none-any.whl (72 kB)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python->llmsearch==0.1.dev74+g7207a16.d20230802) (5.6.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (0.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.26.1->llmsearch==0.1.dev74+g7207a16.d20230802) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.26.1->llmsearch==0.1.dev74+g7207a16.d20230802) (1.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802) (2023.7.22)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802) (1.26.16)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802) (2022.7.1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "Collecting lz4 (from clickhouse-connect>=0.5.7->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "INFO: pip is looking at multiple versions of fastapi to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting fastapi>=0.85.1 (from chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "  Using cached fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "  Using cached fastapi-0.99.0-py3-none-any.whl (58 kB)\n",
            "  Using cached fastapi-0.98.0-py3-none-any.whl (56 kB)\n",
            "  Using cached fastapi-0.97.0-py3-none-any.whl (56 kB)\n",
            "  Using cached fastapi-0.96.1-py3-none-any.whl (57 kB)\n",
            "  Using cached fastapi-0.96.0-py3-none-any.whl (57 kB)\n",
            "INFO: pip is looking at multiple versions of fastapi to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached fastapi-0.95.2-py3-none-any.whl (56 kB)\n",
            "  Using cached fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802) (23.5.26)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->llmsearch==0.1.dev74+g7207a16.d20230802) (2.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (2.14.0)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from validators<1,>=0.2->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (4.4.2)\n",
            "Collecting httpx<0.24,>=0.15 (from argilla->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "Collecting deprecated~=1.2.0 (from argilla->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.13 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802) (1.14.1)\n",
            "Collecting typer<0.8.0,>=0.6.0 (from argilla->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->auto-gptq->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "Collecting xxhash (from datasets->auto-gptq->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Collecting multiprocess (from datasets->auto-gptq->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "Collecting olefile>=0.46 (from msg-parser->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached olefile-0.46-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2->llmsearch==0.1.dev74+g7207a16.d20230802) (1.3.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802) (1.1.0)\n",
            "Collecting cryptography>=36.0.0 (from pdfminer.six->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_28_x86_64.whl (4.3 MB)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802) (1.15.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0 (from httpx<0.24,>=0.15->argilla->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3 (from httpx<0.24,>=0.15->argilla->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (0.19.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.11.0->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.219->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.26->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Collecting tzdata (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit==1.24.1->llmsearch==0.1.dev74+g7207a16.d20230802)\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured==0.7.8->llmsearch==0.1.dev74+g7207a16.d20230802) (2.21)\n",
            "Building wheels for collected packages: llmsearch\n",
            "  Building wheel for llmsearch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llmsearch: filename=llmsearch-0.1.dev74+g7207a16.d20230802-py3-none-any.whl size=32207 sha256=9943bf12b7c7df14ff027b1d318c0b408b1f825a19f7b752c09267b84375ab4e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8_cl21fr/wheels/94/da/7e/6892796caaadf08e84c36a1014bc6021762d12178afed10b67\n",
            "Successfully built llmsearch\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, rfc3986, monotonic, InstructorEmbedding, filetype, bitsandbytes, zstandard, xxhash, XlsxWriter, websockets, watchdog, validators, uvloop, tzdata, typer, threadpoolctl, sqlalchemy, smmap, rouge, requests, python-magic, python-dotenv, python-docx, pypdf2, pypandoc, pymupdf, pympler, pulsar-client, protobuf, pdf2image, overrides, olefile, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, mypy-extensions, marshmallow, lz4, loguru, humanfriendly, httptools, hnswlib, h11, einops, dill, deprecated, backoff, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, pytz-deprecation-shim, python-pptx, pydeck, posthog, openapi-schema-pydantic, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, msg-parser, langchainplus-sdk, huggingface-hub, httpcore, gitdb, cryptography, coloredlogs, clickhouse-connect, tzlocal, transformers, pdfminer.six, openai, onnxruntime, httpx, gitpython, fastapi, dataclasses-json, streamlit, langchain, datasets, chromadb, argilla, unstructured, llama-index, torch, accelerate, torchvision, peft, sentence-transformers, auto-gptq, llmsearch\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.0\n",
            "    Uninstalling typer-0.9.0:\n",
            "      Successfully uninstalled typer-0.9.0\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.2.0\n",
            "    Uninstalling threadpoolctl-3.2.0:\n",
            "      Successfully uninstalled threadpoolctl-3.2.0\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.19\n",
            "    Uninstalling SQLAlchemy-2.0.19:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.19\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 3.4.8\n",
            "    Uninstalling cryptography-3.4.8:\n",
            "      Successfully uninstalled cryptography-3.4.8\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.0.1\n",
            "    Uninstalling tzlocal-5.0.1:\n",
            "      Successfully uninstalled tzlocal-5.0.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.29.0 which is incompatible.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed InstructorEmbedding-1.0.1 XlsxWriter-3.1.2 accelerate-0.19.0 argilla-1.13.3 auto-gptq-0.3.0 backoff-2.2.1 bitsandbytes-0.41.0 chromadb-0.3.26 clickhouse-connect-0.6.8 coloredlogs-15.0.1 cryptography-41.0.3 dataclasses-json-0.5.14 datasets-2.14.2 deprecated-1.2.14 dill-0.3.7 einops-0.6.1 fastapi-0.95.1 filetype-1.2.0 gitdb-4.0.10 gitpython-3.1.32 h11-0.14.0 hnswlib-0.7.0 httpcore-0.16.3 httptools-0.6.0 httpx-0.23.3 huggingface-hub-0.16.4 humanfriendly-10.0 langchain-0.0.219 langchainplus-sdk-0.0.20 llama-index-0.6.9 llmsearch-0.1.dev74+g7207a16.d20230802 loguru-0.7.0 lz4-4.3.2 marshmallow-3.20.1 monotonic-1.6 msg-parser-1.2.0 multiprocess-0.70.15 mypy-extensions-1.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 olefile-0.46 onnxruntime-1.15.1 openai-0.27.8 openapi-schema-pydantic-1.2.4 overrides-7.3.1 pdf2image-1.16.3 pdfminer.six-20221105 peft-0.4.0 posthog-3.0.1 protobuf-3.20.2 pulsar-client-3.2.0 pydeck-0.8.1b0 pympler-1.0.1 pymupdf-1.22.5 pypandoc-1.11 pypdf2-3.0.1 python-docx-0.8.11 python-dotenv-1.0.0 python-magic-0.4.27 python-pptx-0.6.21 pytz-deprecation-shim-0.1.0.post0 requests-2.29.0 rfc3986-1.5.0 rouge-1.0.1 safetensors-0.3.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 smmap-5.0.0 sqlalchemy-1.4.48 starlette-0.26.1 streamlit-1.24.1 threadpoolctl-3.1.0 tiktoken-0.3.3 tokenizers-0.13.3 torch-2.0.0 torchvision-0.15.1 transformers-4.29.2 typer-0.7.0 typing-inspect-0.9.0 tzdata-2023.3 tzlocal-4.3.1 unstructured-0.7.8 uvicorn-0.23.2 uvloop-0.17.0 validators-0.20.0 watchdog-3.0.0 watchfiles-0.19.0 websockets-11.0.3 xxhash-3.3.0 zstandard-0.21.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# !pip install --no-cache-dir git+https://github.com/snexus/llm-search\n",
        "!pip install pyllmsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qjl2MxoA4uc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaOl4lF-4uEE",
        "outputId": "84e383d0-04ac-4c87-c33e-480fede603d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5\n",
            "2023-08-02 11:51:17.541137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[32m2023-08-02 11:51:21.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.config\u001b[0m:\u001b[36mvalidate_params\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mLoading model paramaters in configuration class LlamaModelConfig\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.cli\u001b[0m:\u001b[36mset_cache_folder\u001b[0m:\u001b[36m54\u001b[0m - \u001b[1mSetting SENTENCE_TRANSFORMERS_HOME folder: /content/llm/cache\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.cli\u001b[0m:\u001b[36mset_cache_folder\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mSetting TRANSFORMERS_CACHE folder: /content/llm/cache/transformers\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.cli\u001b[0m:\u001b[36mset_cache_folder\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mSetting HF_HOME: /content/llm/cache/hf_home\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.cli\u001b[0m:\u001b[36mset_cache_folder\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mSetting MODELS_CACHE_FOLDER: /content/llm/cache\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mScanning path for extension: md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mProcessing path using custom splitter: sample_docs/multi-repo-checkout.md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mremove_images\u001b[0m:\u001b[36m404\u001b[0m - \u001b[1mRemoving images ... \u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mremove_extra_newlines\u001b[0m:\u001b[36m417\u001b[0m - \u001b[1mRemoving extra new lines ... \u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mpostprocess_sections\u001b[0m:\u001b[36m370\u001b[0m - \u001b[1mRemoving first section ...\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mpostprocess_sections\u001b[0m:\u001b[36m374\u001b[0m - \u001b[1mMerging sections ...\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m315\u001b[0m - \u001b[1mGot 24 text chunks:\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 634\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1228\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1245\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1205\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 672\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1090\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1111\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1046\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 640\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 811\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 348\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1157\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 963\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 718\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1033\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 968\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1372\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 952\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1122\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 574\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1027\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1107\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1108\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 285\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1msample_docs/multi-repo-checkout.md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mProcessing path using custom splitter: sample_docs/subversion.md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mremove_images\u001b[0m:\u001b[36m404\u001b[0m - \u001b[1mRemoving images ... \u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mremove_extra_newlines\u001b[0m:\u001b[36m417\u001b[0m - \u001b[1mRemoving extra new lines ... \u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mpostprocess_sections\u001b[0m:\u001b[36m370\u001b[0m - \u001b[1mRemoving first section ...\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mpostprocess_sections\u001b[0m:\u001b[36m374\u001b[0m - \u001b[1mMerging sections ...\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m315\u001b[0m - \u001b[1mGot 6 text chunks:\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1007\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1146\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1149\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 938\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1024\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 994\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1msample_docs/subversion.md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mProcessing path using custom splitter: sample_docs/github.md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mremove_images\u001b[0m:\u001b[36m404\u001b[0m - \u001b[1mRemoving images ... \u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mremove_extra_newlines\u001b[0m:\u001b[36m417\u001b[0m - \u001b[1mRemoving extra new lines ... \u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mpostprocess_sections\u001b[0m:\u001b[36m370\u001b[0m - \u001b[1mRemoving first section ...\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mpostprocess_sections\u001b[0m:\u001b[36m374\u001b[0m - \u001b[1mMerging sections ...\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m315\u001b[0m - \u001b[1mGot 60 text chunks:\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1046\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1146\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 735\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1178\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1169\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1152\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 917\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 621\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1094\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1080\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1119\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1037\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1282\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1278\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 615\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 957\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 985\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1413\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1248\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1180\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1206\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1001\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 662\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 352\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1078\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1105\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1010\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 964\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 472\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1175\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 857\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1191\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 603\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 987\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 649\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1202\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1098\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1299\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1192\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1167\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 653\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1114\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1209\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 777\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1012\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1029\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 970\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 887\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 849\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 860\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1119\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1197\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1161\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1142\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1060\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 883\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1298\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 945\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 429\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 807\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1msample_docs/github.md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mProcessing path using custom splitter: sample_docs/on-premises-bitbucket.md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mremove_images\u001b[0m:\u001b[36m404\u001b[0m - \u001b[1mRemoving images ... \u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mremove_extra_newlines\u001b[0m:\u001b[36m417\u001b[0m - \u001b[1mRemoving extra new lines ... \u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mpostprocess_sections\u001b[0m:\u001b[36m370\u001b[0m - \u001b[1mRemoving first section ...\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mpostprocess_sections\u001b[0m:\u001b[36m374\u001b[0m - \u001b[1mMerging sections ...\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m315\u001b[0m - \u001b[1mGot 12 text chunks:\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1074\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 964\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1075\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 627\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 832\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 630\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 883\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 859\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 478\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1069\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1062\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1014\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1msample_docs/on-premises-bitbucket.md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mProcessing path using custom splitter: sample_docs/pipeline-options-for-git.md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mremove_images\u001b[0m:\u001b[36m404\u001b[0m - \u001b[1mRemoving images ... \u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mremove_extra_newlines\u001b[0m:\u001b[36m417\u001b[0m - \u001b[1mRemoving extra new lines ... \u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mpostprocess_sections\u001b[0m:\u001b[36m370\u001b[0m - \u001b[1mRemoving first section ...\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mpostprocess_sections\u001b[0m:\u001b[36m374\u001b[0m - \u001b[1mMerging sections ...\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m315\u001b[0m - \u001b[1mGot 28 text chunks:\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1218\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 630\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 868\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 771\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1133\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 973\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1062\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 791\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1115\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 853\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 629\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1131\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1015\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1159\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1182\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 595\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1093\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1101\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 803\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1021\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1058\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 609\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 717\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1017\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1025\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 1121\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 786\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.markdown\u001b[0m:\u001b[36mmarkdown_splitter\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1m\tChunk length: 631\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1msample_docs/pipeline-options-for-git.md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mGot 130 nodes.\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mGot 130 chunks for type: md\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mScanning path for extension: pdf\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36m_get_documents_from_custom_splitter\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mGot 0 nodes.\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:21.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.parsers.splitter\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mGot 0 chunks for type: pdf\u001b[0m\n",
            "Downloading (…)c7233/.gitattributes: 100% 1.48k/1.48k [00:00<00:00, 8.93MB/s]\n",
            "Downloading (…)_Pooling/config.json: 100% 270/270 [00:00<00:00, 1.69MB/s]\n",
            "Downloading (…)/2_Dense/config.json: 100% 116/116 [00:00<00:00, 521kB/s]\n",
            "Downloading pytorch_model.bin: 100% 3.15M/3.15M [00:00<00:00, 45.9MB/s]\n",
            "Downloading (…)9fb15c7233/README.md: 100% 66.3k/66.3k [00:00<00:00, 31.1MB/s]\n",
            "Downloading (…)b15c7233/config.json: 100% 1.53k/1.53k [00:00<00:00, 10.5MB/s]\n",
            "Downloading (…)ce_transformers.json: 100% 122/122 [00:00<00:00, 906kB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.34G/1.34G [00:15<00:00, 87.9MB/s]\n",
            "Downloading (…)nce_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 349kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 12.8MB/s]\n",
            "Downloading spiece.model: 100% 792k/792k [00:00<00:00, 94.5MB/s]\n",
            "Downloading (…)c7233/tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 41.6MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 2.41k/2.41k [00:00<00:00, 13.7MB/s]\n",
            "Downloading (…)15c7233/modules.json: 100% 461/461 [00:00<00:00, 2.84MB/s]\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "\u001b[32m2023-08-02 11:51:43.275\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllmsearch.chroma\u001b[0m:\u001b[36mcreate_index_from_documents\u001b[0m:\u001b[36m29\u001b[0m - \u001b[33m\u001b[1mDeleting the content of: /content/llm/embeddings\u001b[0m\n",
            "\u001b[32m2023-08-02 11:51:55.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.chroma\u001b[0m:\u001b[36mcreate_index_from_documents\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mPersisting the database..\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! llmsearch index create -c llm/config/config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOU3Fksc_vr9",
        "outputId": "05d0fe9f-b9ff-431c-e2dd-e403db6d9776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5\n",
            "2023-08-02 12:02:26.324549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[32m2023-08-02 12:02:38.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.config\u001b[0m:\u001b[36mvalidate_params\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mLoading model paramaters in configuration class LlamaModelConfig\u001b[0m\n",
            "\u001b[32m2023-08-02 12:02:38.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.cli\u001b[0m:\u001b[36mset_cache_folder\u001b[0m:\u001b[36m54\u001b[0m - \u001b[1mSetting SENTENCE_TRANSFORMERS_HOME folder: /content/llm/cache\u001b[0m\n",
            "\u001b[32m2023-08-02 12:02:38.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.cli\u001b[0m:\u001b[36mset_cache_folder\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mSetting TRANSFORMERS_CACHE folder: /content/llm/cache/transformers\u001b[0m\n",
            "\u001b[32m2023-08-02 12:02:38.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.cli\u001b[0m:\u001b[36mset_cache_folder\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mSetting HF_HOME: /content/llm/cache/hf_home\u001b[0m\n",
            "\u001b[32m2023-08-02 12:02:38.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.cli\u001b[0m:\u001b[36mset_cache_folder\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mSetting MODELS_CACHE_FOLDER: /content/llm/cache\u001b[0m\n",
            "\u001b[32m2023-08-02 12:02:38.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.models.llama\u001b[0m:\u001b[36mmodel\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mLoading model...\u001b[0m\n",
            "\u001b[32m2023-08-02 12:02:38.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.models.llama\u001b[0m:\u001b[36mmodel\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1mInitializing LLAmaCPP model...\u001b[0m\n",
            "\u001b[32m2023-08-02 12:02:38.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllmsearch.models.llama\u001b[0m:\u001b[36mmodel\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1m{'n_ctx': 1024, 'n_batch': 512, 'n_gpu_layers': 43}\u001b[0m\n",
            "llama.cpp: loading model from /content/llm/models/wizardLM-13B-Uncensored.ggmlv3.q6_K.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32001\n",
            "llama_model_load_internal: n_ctx      = 1024\n",
            "llama_model_load_internal: n_embd     = 5120\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 40\n",
            "llama_model_load_internal: n_head_kv  = 40\n",
            "llama_model_load_internal: n_layer    = 40\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: n_gqa      = 1\n",
            "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
            "llama_model_load_internal: n_ff       = 13824\n",
            "llama_model_load_internal: freq_base  = 10000.0\n",
            "llama_model_load_internal: freq_scale = 1\n",
            "llama_model_load_internal: ftype      = 18 (mostly Q6_K)\n",
            "llama_model_load_internal: model size = 13B\n",
            "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
            "llama_model_load_internal: using CUDA for GPU acceleration\n",
            "llama_model_load_internal: mem required  =  537.29 MB (+  800.00 MB per state)\n",
            "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 400 MB VRAM for the scratch buffer\n",
            "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
            "llama_model_load_internal: offloading non-repeating layers to GPU\n",
            "llama_model_load_internal: offloading v cache to GPU\n",
            "llama_model_load_internal: offloading k cache to GPU\n",
            "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
            "llama_model_load_internal: total VRAM used: 11256 MB\n",
            "llama_new_context_with_model: kv self size  =  800.00 MB\n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "\n",
            "ENTER QUESTION >> How to specify target branches in git?\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:StuffDocumentsChain] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:StuffDocumentsChain > 2:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"How to specify target branches in git?\",\n",
            "  \"context\": \"Metadata applicable to the next chunk of text delimited by five stars:\\n>> METADATA START\\nDocument name: github.md\\nSubsection of: [YAML](tab/yaml/)\\n>> METADATA END\\n\\n*****\\n### Protected branches\\n\\nYou can run a validation build with each commit or pull request that targets a branch, and even prevent pull requests from merging until a validation build succeeds.\\n\\nTo configure mandatory validation builds for a GitHub repository, you must be its owner, a collaborator with the Admin role, or a GitHub organization member with the Write role.\\n\\n1. First, create a pipeline for the repository and build it at least once so that its status is posted to GitHub, thereby making GitHub aware of the pipeline's name.\\n\\n2. Next, follow GitHub's documentation for [configuring protected branches](https://help.github.com/articles/configuring-protected-branches/) in the repository's settings.\\n\\n   For the status check, select the name of your pipeline in the **Status checks** list.\\n\\n   \\n\\n>[!IMPORTANT]\\n>If your pipeline doesn't show up in this list, please ensure the following:\\n>\\n>* You are using [GitHub app authentication](#github-app-authentication)\\n>* Your pipeline has run at least once in the last week\\n*****\\n\\nMetadata applicable to the next chunk of text delimited by five stars:\\n>> METADATA START\\nDocument name: github.md\\nSubsection of: [YAML](tab/yaml/)\\n>> METADATA END\\n\\n*****\\n[Classic](#tab/classic/)\\n\\nSelect the **Pull request validation** trigger and check the **Enable pull request validation** check box to enable builds on pull requests.\\n\\nYou can specify branches to include and exclude.\\nSelect a branch name from the drop-down menu and select **Include** or **Exclude** as appropriate.\\nFor included branches, a build will be triggered on each push to a pull request targeting that branch.\\n\\n---\\n\\nIf you have an open PR and you push changes to its source branch, multiple pipelines may run:\\n - The pipelines that have a PR trigger on the PR's target branch will run on the _merge commit_ (the merged code between the source and target branches of the pull request), regardless if there exist pushed commits whose messages or descriptions contain `[skip ci]` (or any of its variants).\\n*****\\n\\nMetadata applicable to the next chunk of text delimited by five stars:\\n>> METADATA START\\nDocument name: pipeline-options-for-git.md\\nSubsection of: Pipeline options for Git repositories\\n>> METADATA END\\n\\n*****\\n## Branch\\n\\nThis is the branch that you want to be the default when you manually queue this build. If you set a scheduled trigger for the build, this is the branch from which your build will get the latest sources. The default branch has no bearing when the build is triggered through continuous integration (CI). Usually you'll set this to be the same as the default branch of the repository (for example, \\\"master\\\").\\n*****\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:StuffDocumentsChain > 2:chain:LLMChain > 3:llm:CustomLlamaLangChainModel] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"### Instruction:\\nUse the following pieces of context to provide detailed answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.\\n\\n### Context:\\n---------------\\nMetadata applicable to the next chunk of text delimited by five stars:\\n>> METADATA START\\nDocument name: github.md\\nSubsection of: [YAML](tab/yaml/)\\n>> METADATA END\\n\\n*****\\n### Protected branches\\n\\nYou can run a validation build with each commit or pull request that targets a branch, and even prevent pull requests from merging until a validation build succeeds.\\n\\nTo configure mandatory validation builds for a GitHub repository, you must be its owner, a collaborator with the Admin role, or a GitHub organization member with the Write role.\\n\\n1. First, create a pipeline for the repository and build it at least once so that its status is posted to GitHub, thereby making GitHub aware of the pipeline's name.\\n\\n2. Next, follow GitHub's documentation for [configuring protected branches](https://help.github.com/articles/configuring-protected-branches/) in the repository's settings.\\n\\n   For the status check, select the name of your pipeline in the **Status checks** list.\\n\\n   \\n\\n>[!IMPORTANT]\\n>If your pipeline doesn't show up in this list, please ensure the following:\\n>\\n>* You are using [GitHub app authentication](#github-app-authentication)\\n>* Your pipeline has run at least once in the last week\\n*****\\n\\nMetadata applicable to the next chunk of text delimited by five stars:\\n>> METADATA START\\nDocument name: github.md\\nSubsection of: [YAML](tab/yaml/)\\n>> METADATA END\\n\\n*****\\n[Classic](#tab/classic/)\\n\\nSelect the **Pull request validation** trigger and check the **Enable pull request validation** check box to enable builds on pull requests.\\n\\nYou can specify branches to include and exclude.\\nSelect a branch name from the drop-down menu and select **Include** or **Exclude** as appropriate.\\nFor included branches, a build will be triggered on each push to a pull request targeting that branch.\\n\\n---\\n\\nIf you have an open PR and you push changes to its source branch, multiple pipelines may run:\\n - The pipelines that have a PR trigger on the PR's target branch will run on the _merge commit_ (the merged code between the source and target branches of the pull request), regardless if there exist pushed commits whose messages or descriptions contain `[skip ci]` (or any of its variants).\\n*****\\n\\nMetadata applicable to the next chunk of text delimited by five stars:\\n>> METADATA START\\nDocument name: pipeline-options-for-git.md\\nSubsection of: Pipeline options for Git repositories\\n>> METADATA END\\n\\n*****\\n## Branch\\n\\nThis is the branch that you want to be the default when you manually queue this build. If you set a scheduled trigger for the build, this is the branch from which your build will get the latest sources. The default branch has no bearing when the build is triggered through continuous integration (CI). Usually you'll set this to be the same as the default branch of the repository (for example, \\\"master\\\").\\n*****\\n---------------\\n\\n### Question: How to specify target branches in git?\\n### Response:\"\n",
            "  ]\n",
            "}\n",
            "To specify target branches in Git, you can use the `git branch` command. Here's an example:\n",
            "```\n",
            "$ git branch my-feature-branch\n",
            "```\n",
            "This creates a new branch called `my-feature-branch`. You can then push this branch to your remote repository using `git push`:\n",
            "```\n",
            "$ git push origin my-feature-branch\n",
            "```\n",
            "To include or exclude branches when triggering builds, you can use the `pull request` feature in GitHub. Here's an example:\n",
            "1. Open the pull request settings by clicking on the gear icon next to the pull request title.\n",
            "2. Select the `Pull request validation` trigger and check the `Enable pull request validation` box.\n",
            "3. In the `Branches to include` section, select the branch names you want to include in the build.\n",
            "4. In the `Branches to exclude` section, select the branch names you want to exclude from the build.\n",
            "5. Click on the `Save changes` button to save your settings\n",
            "llama_print_timings:        load time =  4427.94 ms\n",
            "llama_print_timings:      sample time =   338.45 ms /   222 runs   (    1.52 ms per token,   655.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =  7613.63 ms /   779 tokens (    9.77 ms per token,   102.32 tokens per second)\n",
            "llama_print_timings:        eval time = 20590.17 ms /   221 runs   (   93.17 ms per token,    10.73 tokens per second)\n",
            "llama_print_timings:       total time = 29604.52 ms\n",
            ".\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:StuffDocumentsChain > 2:chain:LLMChain > 3:llm:CustomLlamaLangChainModel] [29.61s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"To specify target branches in Git, you can use the `git branch` command. Here's an example:\\n```\\n$ git branch my-feature-branch\\n```\\nThis creates a new branch called `my-feature-branch`. You can then push this branch to your remote repository using `git push`:\\n```\\n$ git push origin my-feature-branch\\n```\\nTo include or exclude branches when triggering builds, you can use the `pull request` feature in GitHub. Here's an example:\\n1. Open the pull request settings by clicking on the gear icon next to the pull request title.\\n2. Select the `Pull request validation` trigger and check the `Enable pull request validation` box.\\n3. In the `Branches to include` section, select the branch names you want to include in the build.\\n4. In the `Branches to exclude` section, select the branch names you want to exclude from the build.\\n5. Click on the `Save changes` button to save your settings.\",\n",
            "        \"generation_info\": null\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": null,\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:StuffDocumentsChain > 2:chain:LLMChain] [29.61s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"To specify target branches in Git, you can use the `git branch` command. Here's an example:\\n```\\n$ git branch my-feature-branch\\n```\\nThis creates a new branch called `my-feature-branch`. You can then push this branch to your remote repository using `git push`:\\n```\\n$ git push origin my-feature-branch\\n```\\nTo include or exclude branches when triggering builds, you can use the `pull request` feature in GitHub. Here's an example:\\n1. Open the pull request settings by clicking on the gear icon next to the pull request title.\\n2. Select the `Pull request validation` trigger and check the `Enable pull request validation` box.\\n3. In the `Branches to include` section, select the branch names you want to include in the build.\\n4. In the `Branches to exclude` section, select the branch names you want to exclude from the build.\\n5. Click on the `Save changes` button to save your settings.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:StuffDocumentsChain] [29.61s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output_text\": \"To specify target branches in Git, you can use the `git branch` command. Here's an example:\\n```\\n$ git branch my-feature-branch\\n```\\nThis creates a new branch called `my-feature-branch`. You can then push this branch to your remote repository using `git push`:\\n```\\n$ git push origin my-feature-branch\\n```\\nTo include or exclude branches when triggering builds, you can use the `pull request` feature in GitHub. Here's an example:\\n1. Open the pull request settings by clicking on the gear icon next to the pull request title.\\n2. Select the `Pull request validation` trigger and check the `Enable pull request validation` box.\\n3. In the `Branches to include` section, select the branch names you want to include in the build.\\n4. In the `Branches to exclude` section, select the branch names you want to exclude from the build.\\n5. Click on the `Save changes` button to save your settings.\"\n",
            "}\n",
            "\n",
            "============= RESPONSE =================\n",
            "\u001b[31mTo specify target branches in Git, you can use the `git branch` command. Here's an example:\n",
            "```\n",
            "$ git branch my-feature-branch\n",
            "```\n",
            "This creates a new branch called `my-feature-branch`. You can then push this branch to your remote repository using `git push`:\n",
            "```\n",
            "$ git push origin my-feature-branch\n",
            "```\n",
            "To include or exclude branches when triggering builds, you can use the `pull request` feature in GitHub. Here's an example:\n",
            "1. Open the pull request settings by clicking on the gear icon next to the pull request title.\n",
            "2. Select the `Pull request validation` trigger and check the `Enable pull request validation` box.\n",
            "3. In the `Branches to include` section, select the branch names you want to include in the build.\n",
            "4. In the `Branches to exclude` section, select the branch names you want to exclude from the build.\n",
            "5. Click on the `Save changes` button to save your settings.\u001b[0m\n",
            "\n",
            "============= SOURCES ==================\n",
            "\u001b[34msample_docs/github.md\u001b[0m\n",
            "\u001b[36m{'heading': 'Protected%20branches'}\u001b[0m\n",
            "******************* BEING EXTRACT *****************\n",
            "Metadata applicable to the next chunk of text delimited by five stars:\n",
            ">> METADATA START\n",
            "Document name: github.md\n",
            "Subsection of: [YAML](tab/yaml/)\n",
            ">> METADATA END\n",
            "\n",
            "*****\n",
            "### Protected branches\n",
            "\n",
            "You can run a validation build with each commit or pull request that targets a branch, and even prevent pull requests from merging until a validation build succeeds.\n",
            "\n",
            "To configure mandatory validation builds for a GitHub repository, you must be its owner, a collaborator with the Admin role, or a GitHub organization member with the Write role.\n",
            "\n",
            "1. First, create a pipeline for the repository and build it at least once so that its status is posted to GitHub, thereby making GitHub aware of the pipeline's name.\n",
            "\n",
            "2. Next, follow GitHub's documentation for [configuring protected branches](https://help.github.com/articles/configuring-protected-branches/) in the repository's settings.\n",
            "\n",
            "   For the status check, select the name of your pipeline in the **Status checks** list.\n",
            "\n",
            "   \n",
            "\n",
            ">[!IMPORTANT]\n",
            ">If your pipeline doesn't show up in this list, please ensure the following:\n",
            ">\n",
            ">* You are using [GitHub app authentication](#github-app-authentication)\n",
            ">* Your pipeline has run at least once in the last week\n",
            "*****\n",
            "\n",
            "\u001b[34msample_docs/github.md\u001b[0m\n",
            "\u001b[36m{'heading': ''}\u001b[0m\n",
            "******************* BEING EXTRACT *****************\n",
            "Metadata applicable to the next chunk of text delimited by five stars:\n",
            ">> METADATA START\n",
            "Document name: github.md\n",
            "Subsection of: [YAML](tab/yaml/)\n",
            ">> METADATA END\n",
            "\n",
            "*****\n",
            "[Classic](#tab/classic/)\n",
            "\n",
            "Select the **Pull request validation** trigger and check the **Enable pull request validation** check box to enable builds on pull requests.\n",
            "\n",
            "You can specify branches to include and exclude.\n",
            "Select a branch name from the drop-down menu and select **Include** or **Exclude** as appropriate.\n",
            "For included branches, a build will be triggered on each push to a pull request targeting that branch.\n",
            "\n",
            "---\n",
            "\n",
            "If you have an open PR and you push changes to its source branch, multiple pipelines may run:\n",
            " - The pipelines that have a PR trigger on the PR's target branch will run on the _merge commit_ (the merged code between the source and target branches of the pull request), regardless if there exist pushed commits whose messages or descriptions contain `[skip ci]` (or any of its variants).\n",
            "*****\n",
            "\n",
            "\u001b[34msample_docs/pipeline-options-for-git.md\u001b[0m\n",
            "\u001b[36m{'heading': 'Branch'}\u001b[0m\n",
            "******************* BEING EXTRACT *****************\n",
            "Metadata applicable to the next chunk of text delimited by five stars:\n",
            ">> METADATA START\n",
            "Document name: pipeline-options-for-git.md\n",
            "Subsection of: Pipeline options for Git repositories\n",
            ">> METADATA END\n",
            "\n",
            "*****\n",
            "## Branch\n",
            "\n",
            "This is the branch that you want to be the default when you manually queue this build. If you set a scheduled trigger for the build, this is the branch from which your build will get the latest sources. The default branch has no bearing when the build is triggered through continuous integration (CI). Usually you'll set this to be the same as the default branch of the repository (for example, \"master\").\n",
            "*****\n",
            "\n",
            "------------------------------------------\n",
            "\n",
            "ENTER QUESTION >> "
          ]
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "llmsearch interact llm -c llm/config/config.yaml -m llm/config/model.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
