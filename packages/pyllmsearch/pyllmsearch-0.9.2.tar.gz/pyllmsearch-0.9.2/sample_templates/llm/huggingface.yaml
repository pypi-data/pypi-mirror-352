# This file contains a configuration section relevant to LLM, not the entire config

llm:
  type: huggingface
  params:
    model_name: microsoft/phi-2 # tiiuae/falcon-7b-instruct
    cache_folder: /storage/llm/cache
    prompt_template: |
          ### Instruction:
          Use the following pieces of context to answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.

          ### Context: 
          ---------------
          {context}
          ---------------

          ### Question: {question}
    load_8bit: False
    trust_remote_code: False
    device: auto
    pipeline_kwargs:
      do_sample: True
      max_new_tokens: 512
      num_return_sequences: 1
    model_kwargs:
      do_sample: True
      temperature: 0.01
